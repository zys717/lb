
@article{tian_uavs_2025,
	title = {{UAVs} {Meet} {LLMs}: {Overviews} and {Perspectives} {Toward} {Agentic} {Low}-{Altitude} {Mobility}},
	volume = {122},
	issn = {15662535},
	shorttitle = {{UAVs} {Meet} {LLMs}},
	url = {http://arxiv.org/abs/2501.02341},
	doi = {10.1016/j.inffus.2025.103158},
	abstract = {Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has introduced transformative advancements across various domains, like transportation, logistics, and agriculture. Leveraging flexible perspectives and rapid maneuverability, UAVs extend traditional systems’ perception and action capabilities, garnering widespread attention from academia and industry. However, current UAV operations primarily depend on human control, with only limited autonomy in simple scenarios, and lack the intelligence and adaptability needed for more complex environments and tasks. The emergence of large language models (LLMs) demonstrates remarkable problem-solving and generalization capabilities, offering a promising pathway for advancing UAV intelligence. This paper explores the integration of LLMs and UAVs, beginning with an overview of UAV systems’ fundamental components and functionalities, followed by an overview of the state-of-the-art LLM technology. Subsequently, it systematically highlights the multimodal data resources available for UAVs, which provide critical support for training and evaluation. Furthermore, key tasks and application scenarios where UAVs and LLMs converge are categorized and analyzed. Finally, a reference roadmap towards agentic UAVs is proposed to enable UAVs to achieve agentic intelligence through autonomous perception, memory, reasoning, and tool utilization. Related resources are available at https://github.com/Hub-Tian/UAVs Meet LLMs.},
	language = {en},
	urldate = {2025-05-19},
	journal = {Information Fusion},
	author = {Tian, Yonglin and Lin, Fei and Li, Yiduo and Zhang, Tengchao and Zhang, Qiyao and Fu, Xuan and Huang, Jun and Dai, Xingyuan and Wang, Yutong and Tian, Chunwei and Li, Bai and Lv, Yisheng and Kovács, Levente and Wang, Fei-Yue},
	month = oct,
	year = {2025},
	note = {arXiv:2501.02341 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	pages = {103158},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/VGHWJG9Q/Tian 等 - 2025 - UAVs Meet LLMs Overviews and Perspectives Toward Agentic Low-Altitude Mobility.pdf:application/pdf},
}

@incollection{li_analysis_2020,
	series = {{AIAA} {AVIATION} {Forum}},
	title = {Analysis of {Fleet} {Management} and {Infrastructure} {Constraints} in {On}-{Demand} {Urban} {Air} {Mobility} {Operations}},
	url = {https://arc.aiaa.org/doi/10.2514/6.2020-2907},
	urldate = {2025-11-13},
	booktitle = {{AIAA} {AVIATION} 2020 {FORUM}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Li, Sheng and Egorov, Maxim and Kochenderfer, Mykel J.},
	month = jun,
	year = {2020},
	doi = {10.2514/6.2020-2907},
	keywords = {Air Traffic Control, Air Transportation System, Aircraft Landing, Airspace, Energy Consumption, Gaussian Mixture Models, Poisson Distribution, Sensitivity Analysis, Urban Air Mobility, Vertical Takeoff and Landing},
	file = {已提交版本:/Users/zhangyunshi/Zotero/storage/49SXLS22/Li 等 - 2020 - Analysis of Fleet Management and Infrastructure Constraints in On-Demand Urban Air Mobility Operatio.pdf:application/pdf},
}

@misc{cao_fleet_2024,
	title = {Fleet {Size} and {Spill} for {UAM} {Operation} under {Uncertain} {Demand}},
	url = {http://arxiv.org/abs/2407.00947},
	doi = {10.48550/arXiv.2407.00947},
	abstract = {Variation and imbalance in demand poses significant challenges to Urban Air Mobility (UAM) operations, affecting strategic decisions such as fleet sizing. To study the implications of demand variation on UAM fleet operations, we propose a stochastic passenger arrival time generation model that uses realworld data to infer demand distributions, and two integer programs that compute the zero-spill fleet size and the spill-minimizing flight schedules and charging policies, respectively. Our numerical experiment on a two-vertiport network shows that spill in relatively inelastic to fleet size and that the driving factor behind spill is the imbalance in demand.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Cao, Shangqing and Jiang, Xuan and Onat, Emin Burak and Zou, Bo and Hansen, Mark and Sengupta, Raja and Chakrabarty, Anjan},
	month = jul,
	year = {2024},
	note = {arXiv:2407.00947 [eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/XX6KNTKM/Cao 等 - 2024 - Fleet Size and Spill for UAM Operation under Uncertain Demand.pdf:application/pdf},
}

@article{haba_routing_2025,
	title = {Routing and scheduling optimization for urban air mobility fleet management using quantum annealing},
	volume = {15},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-025-86843-w},
	doi = {10.1038/s41598-025-86843-w},
	language = {en},
	number = {1},
	urldate = {2025-11-13},
	journal = {Sci Rep},
	author = {Haba, Renichiro and Mano, Takuya and Ueda, Ryosuke and Ebe, Genichiro and Takeda, Kohei and Terabe, Masayoshi and Ohzeki, Masayuki},
	month = feb,
	year = {2025},
	pages = {4326},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/4JJ4JKKJ/Haba 等 - 2025 - Routing and scheduling optimization for urban air mobility fleet management using quantum annealing.pdf:application/pdf},
}

@incollection{taye_energy_2024,
	series = {{AIAA} {Aviation} {Forum} and {ASCEND} co-located {Conference} {Proceedings}},
	title = {Energy {Demand} {Analysis} for {eVTOL} {Charging} {Stations} in {Urban} {Air} {Mobility}},
	url = {https://arc.aiaa.org/doi/10.2514/6.2024-4627},
	urldate = {2025-11-13},
	booktitle = {{AIAA} {AVIATION} {FORUM} {AND} {ASCEND} 2024},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Taye, Abenezer G. and Pradeep, Priyank and Wei, Peng and Jones, James C. and Bonin, Timothy and Eberle, Derek},
	month = jul,
	year = {2024},
	doi = {10.2514/6.2024-4627},
	keywords = {Energy Consumption, Urban Air Mobility, Vertical Takeoff and Landing, Drag Coefficient, Earth, Electric Vertical Take off and Landing, Electrical Energy, Flight Planning, Optimal Control Problem, True Airspeed},
}

@article{liu_ultrafast_2021,
	title = {Ultrafast charging of energy-dense lithium-ion batteries for urban air mobility},
	volume = {7},
	issn = {25901168},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2590116821000011},
	doi = {10.1016/j.etran.2021.100103},
	abstract = {Urban air mobility (UAM) demands batteries with high energy density, long cycle life, and fast rechargeability. Here, we demonstrate an energy-dense lithium-ion battery (LiB) with ultralong cycle life under ultrafast charging. By using the asymmetric temperature modulation (ATM) method, i.e., charging at an elevated temperature and discharging around the ambient temperature, it is experimentally shown that the 209 Wh/kg LiB is charged to 88\% state of charge (SOC) in {\textasciitilde}5 min under UAM cycling while retaining 97.7\% capacity after 1,000 cycles. Moreover, an experimentally validated electrochemical-thermal (ECT) model is developed to elucidate the fast charging process and the degradation mode of UAM batteries, quantitatively capturing lithium plating during fast charging. We ﬁnd that the LiBs for UAM applications are most prone to lithium plating due to their higher initial SOC required as the reserve for safety; nevertheless, the ATM method is effective in minimizing or preventing lithium plating in the high SOC range of 30-90\%. In addition to slowing down capacity fade, the ATM method also raises the usable capacity by 10\%, which boosts the battery energy density and ensures the battery to perform full UAM cycles even at the end of life.},
	language = {en},
	urldate = {2025-11-13},
	journal = {eTransportation},
	author = {Liu, Teng and Yang, Xiao-Guang and Ge, Shanhai and Leng, Yongjun and Wang, Chao-Yang},
	month = feb,
	year = {2021},
	pages = {100103},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/GVDKEQI7/Liu 等 - 2021 - Ultrafast charging of energy-dense lithium-ion batteries for urban air mobility.pdf:application/pdf},
}

@article{yang_challenges_2021,
	title = {Challenges and key requirements of batteries for electric vertical takeoff and landing aircraft},
	volume = {5},
	issn = {25424351},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2542435121002051},
	doi = {10.1016/j.joule.2021.05.001},
	abstract = {Electric vertical takeoff and landing (eVTOL) aircraft have attracted considerable interest as a disruptive technology to transform future transportation systems. Their unique operating proﬁles and requirements present grand challenges to batteries. This work identiﬁes the primary battery requirements for eVTOL in terms of speciﬁc energy and power, fast charging, cycle life, and safety, revealing that eVTOL batteries have more stringent requirements than electric vehicle batteries in all aspects. Notably, we ﬁnd that fast charging is essential for downsizing aircraft and batteries for low cost while achieving high vehicle utilization rates to maximize revenues. We experimentally demonstrate two energy-dense Li-ion battery designs that can recharge adequate energy for 80 km eVTOL trips in 5–10 min and sustain over 2,000 fast-charge cycles, laying a foundation for eVTOL batteries.},
	language = {en},
	number = {7},
	urldate = {2025-11-13},
	journal = {Joule},
	author = {Yang, Xiao-Guang and Liu, Teng and Ge, Shanhai and Rountree, Eric and Wang, Chao-Yang},
	month = jul,
	year = {2021},
	pages = {1644--1659},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/WZD6JJGI/Yang 等 - 2021 - Challenges and key requirements of batteries for electric vertical takeoff and landing aircraft.pdf:application/pdf},
}

@article{ayyaswamy_revealing_2023,
	title = {Revealing hidden predicaments to lithium-ion battery dynamics for electric vertical take-off and landing aircraft},
	volume = {7},
	issn = {25424351},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2542435123003124},
	doi = {10.1016/j.joule.2023.07.014},
	abstract = {The future of urbanization engulfs the trident of electriﬁcation, increased accessibility, and enhanced productivity. Although electric vertical take-off and landing (eVTOL) aircrafts provide cleaner, faster, and more efﬁcient mobility solutions, they exhibit stringent phase-disparate demands on Li-ion batteries (LIBs). Through our mechanistic modeling framework, we demonstrate that eVTOL architecture, its mission constraints, and electrode design portray complex electrochemical implications in LIBs. Accrescent current densities distinctive to eVTOLs signify landing/balked phases as critical pathways to trigger thermal safety. During cold starts, we identify key limitations arising from the union of initial energy consumption and thermal convection from altitude variation. Cognizant of the mission-speciﬁc thermo-electrochemical interactions in LIBs, practical insights into the dynamic response of battery thermal management systems are discussed. The conﬂuence of eVTOL power requirements with its concomitant battery response conveys mechanistic trade-offs pertinent to a spectrum of target applications, including passenger mobility, cargo, and emergency medical services.},
	language = {en},
	number = {9},
	urldate = {2025-11-13},
	journal = {Joule},
	author = {Ayyaswamy, Abhinand and Vishnugopi, Bairav S. and Mukherjee, Partha P.},
	month = sep,
	year = {2023},
	pages = {2016--2034},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/A8HNNATP/Ayyaswamy 等 - 2023 - Revealing hidden predicaments to lithium-ion battery dynamics for electric vertical take-off and lan.pdf:application/pdf},
}

@article{he_key_2024,
	title = {Key technologies and upgrade strategies for {eVTOL} aircraft energy storage systems},
	volume = {103},
	issn = {2352152X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352152X24039884},
	doi = {10.1016/j.est.2024.114402},
	abstract = {With the increasing demand for urban air transportation, electric vertical takeoff and landing (eVTOL) aircraft have garnered significant attention as a promising new mode of urban air travel. One of the key technologies enabling the sustainability and extended range of these aircraft is their energy storage systems. This paper aims to first clarify the specific requirements of the energy storage system for eVTOL aircraft, and then explore the demand indicators and existing improvement solutions for battery technology, fast charging technology, and safety technology. Additionally, the article summarizes three commonly used strategies to enhance energy storage system performance: upgrading battery technology, applying hybrid energy technologies, and increasing energy density. Lastly, the paper provides a future outlook on the development trends of energy storage technologies in eVTOL aircraft, offering new ideas and directions for enhancing the performance of these systems.},
	language = {en},
	urldate = {2025-11-13},
	journal = {Journal of Energy Storage},
	author = {He, Jiaqi and He, Qiang and Xu, Zehua and Jia, Yangyang and Wang, Jiwen and Li, Kangshuai and Tan, Wenkai},
	month = dec,
	year = {2024},
	pages = {114402},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/YRQ8JLMJ/He 等 - 2024 - Key technologies and upgrade strategies for eVTOL aircraft energy storage systems.pdf:application/pdf},
}

@article{pak_can_2025,
	title = {Can {Urban} {Air} {Mobility} become reality? {Opportunities} and challenges of {UAM} as innovative mode of transport and {DLR} contribution to ongoing research},
	volume = {16},
	issn = {1869-5590},
	shorttitle = {Can {Urban} {Air} {Mobility} become reality?},
	url = {https://doi.org/10.1007/s13272-024-00733-x},
	doi = {10.1007/s13272-024-00733-x},
	abstract = {Urban Air Mobility (UAM) is a new air transportation system for passengers and cargo in urban environments, enabled by new technologies and integrated into multimodal transportation systems. The vision of UAM comprises the mass use in urban and suburban environments, complementing existing transportation systems and contributing to the decarbonization of the transport sector. Initial attempts to create a market for urban air transportation in the last century failed due to lack of profitability and community acceptance. Technological advances in numerous fields over the past few decades have led to a renewed interest in urban air transportation. UAM is expected to benefit users and to also have a positive impact on the economy by creating new markets and employment opportunities for manufacturing and operation of UAM vehicles and the construction of related ground infrastructure. However, there are also concerns about noise, safety and security, privacy and environmental impacts. Therefore, the UAM system needs to be designed carefully to become safe, affordable, accessible, environmentally friendly, economically viable and thus sustainable. This paper provides an overview of selected key research topics related to UAM and how the German Aerospace Center (DLR) contributed to this research in the project "HorizonUAM - Urban Air Mobility Research at the German Aerospace Center (DLR)". Selected research results on the topics of market potential and public acceptance, vehicle design (including battery degradation, onboard systems, cabin design, cabin simulation), infrastructure, operations (including U-space, safe autonomy, navigation, communication, cost modeling) and overall system modeling are briefly presented.},
	language = {en},
	number = {3},
	urldate = {2025-11-13},
	journal = {CEAS Aeronaut J},
	author = {Pak, Henry and Asmer, Lukas and Kokus, Petra and Schuchardt, Bianca I. and End, Albert and Meller, Frank and Schweiger, Karolin and Torens, Christoph and Barzantny, Carolina and Becker, Dennis and Ernst, Johannes Maria and Jäger, Florian and Laudien, Tim and Naeem, Nabih and Papenfuß, Anne and Pertz, Jan and Prakasha, Prajwal Shiva and Ratei, Patrick and Reimer, Fabian and Sieb, Patrick and Zhu, Chen and Abdellaoui, Rabeb and Becker, Richard-Gregor and Bertram, Oliver and Devta, Aditya and Gerz, Thomas and Jaksche, Roman and König, Andreas and Lenz, Helge and Metz, Isabel C. and Naser, Fares and Schalk, Lukas and Schier-Morgenthal, Sebastian and Stolz, Maria and Swaid, Majed and Volkert, Andreas and Wendt, Kristin},
	month = jul,
	year = {2025},
	keywords = {Air taxi, Market development, Social acceptance, System-of-systems, Urban air mobility, Vertidrome},
	pages = {665--695},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/SDBPJIUT/Pak 等 - 2025 - Can Urban Air Mobility become reality Opportunities and challenges of UAM as innovative mode of tra.pdf:application/pdf},
}

@article{sengupta_urban_2025,
	title = {Urban {Air} {Mobility} {Research} {Challenges} and {Opportunities}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2573-5144},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-control-022823-031353},
	doi = {10.1146/annurev-control-022823-031353},
	abstract = {This article reviews the literature on urban air mobility (UAM), examining both the research challenges it presents and the transformative opportunities that make these challenges worth addressing. While UAM has historical precedents, the current iteration is born of novel aircraft technology, primarily electric vertical takeoff and landing (eVTOL) and electric short takeoff and landing (eSTOL) aircraft. These advances raise new questions in aerodynamics, control, and integration with urban infrastructure. We explore several key research areas, including aircraft design, vertiport development, network planning, and air traffic management. We also address the scalability challenges in air traffic management for high-density UAM operations and the potential of autonomous and remotely piloted systems. If new aircraft are to birth a new urbanism, they will do so by integrating aircraft engineering and computational intelligence in control, systems, robotics, and human factors.},
	language = {en},
	number = {1},
	urldate = {2025-11-13},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Sengupta, Raja and Bulusu, Vishwanath and Mballo, Chams Eddine and Onat, Emin Burak and Cao, Shangqing (Albert)},
	month = may,
	year = {2025},
	pages = {407--431},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/Z6J3IRH9/Sengupta 等 - 2025 - Urban Air Mobility Research Challenges and Opportunities.pdf:application/pdf},
}

@inproceedings{abdalla_machine_2020,
	title = {Machine {Learning}-{Assisted} {UAV} {Operations} with the {UTM}: {Requirements}, {Challenges}, and {Solutions}},
	shorttitle = {Machine {Learning}-{Assisted} {UAV} {Operations} with the {UTM}},
	url = {https://ieeexplore.ieee.org/document/9348605},
	doi = {10.1109/VTC2020-Fall49728.2020.9348605},
	abstract = {Unmanned aerial vehicles (UAVs) are emerging in commercial spaces and will support many applications, such as smart agriculture, dynamic network deployment, network coverage extension, surveillance and security. The unmanned aircraft system (UAS) traffic management (UTM) provides a framework for safe UAV operation by integrating UAV controllers and central data bases through a communications network. This paper discusses the challenges and opportunities for machine learning (ML) for effectively providing critical UTM services. We introduce the four pillars of UTM—operation planning, situational awareness, failure detection and recovery, and remote identification—and discuss the main services, specific opportunities for ML and the ongoing research. We conclude that the multi-faceted operating environment and operational parameters will benefit from collected data and data-driven algorithms, as well as online learning to support new UAV operation situations.},
	urldate = {2025-11-13},
	booktitle = {2020 {IEEE} 92nd {Vehicular} {Technology} {Conference} ({VTC2020}-{Fall})},
	author = {Abdalla, Aly Sabri and Marojevic, Vuk},
	month = nov,
	year = {2020},
	note = {ISSN: 2577-2465},
	keywords = {UAV, Unmanned aerial vehicles, Vehicle dynamics, UAS, Surveillance, Security, UTM, ML, Regulation, Space vehicles, Vehicular and wireless technologies},
	pages = {1--5},
	file = {已提交版本:/Users/zhangyunshi/Zotero/storage/ESMJ5TKK/Abdalla和Marojevic - 2020 - Machine Learning-Assisted UAV Operations with the UTM Requirements, Challenges, and Solutions.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/3Q29RDXL/9348605.html:text/html},
}

@article{ravich_-demand_nodate,
	title = {On-{Demand} {Aviation}: {Governance} {Challenges} of {Urban} {Air} {Mobility} (“{UAM}”)},
	abstract = {The first generation that has never known a world without smartphones and social media may be close to making the world forget about traditional cars. Investment is pouring into urban air mobility (“UAM”)—the local, on-demand movement of people and goods by air using a range of piloted and semi- and fully autonomous electric aircraft that take off and land vertically. In fact, the innovation of aerial ridesharing at scale—a technology that is still very much associated with the 1960s cartoon series “The Jetsons”—may be at market as soon as 2025, according to some estimates.},
	language = {en},
	author = {Ravich, Timothy M},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/XDUXHKDT/Ravich - On-Demand Aviation Governance Challenges of Urban Air Mobility (“UAM”).pdf:application/pdf},
}

@article{thornton_novel_nodate,
	title = {A {Novel} {Framework} for {Testing} {Causal} {Reasoning} in {LLMs}: {Design}, {Data} {Collection}, and {Evaluation}},
	abstract = {In this paper, we address significant gaps and challenges in benchmarking the causal reasoning capabilities of large language models (LLMs). We propose a comprehensive and robust evaluation framework for multilingual causal reasoning and present results from our initial assessment. We critically analyze existing benchmarking datasets, highlighting their limitations in originality, complexity, and linguistic diversity, and we illustrate that current evaluations fall short in truly assessing multilingual causal reasoning abilities. We outline the design of our dataset, which involved human experts crafting long and complex prompts, which were translated into Spanish, Japanese, Korean, Turkish, and Standard Arabic. We evaluate model accuracy and consistency across these languages from over 20 models from 10 different developers. Our findings reveal that LLMs exhibit inconsistent causal reasoning with complex, novel prompts in English and underperformance in languages such as Turkish and Arabic. We argue that the evaluation underscores an urgent need for multilingual reasoning assessments, as model performance appears to decline with increased linguistic complexity and decreased availability of resources. We advocate for the development of additional multilingual causal reasoning training data to enhance the fine-tuning of models for improved causal reasoning capabilities.},
	language = {en},
	author = {Thornton, Abigail E and Karageorgos, Konstantinos K and Greenberg, Melissa E and Alvarez, Daisy and Grace, Mikaela R and Callahan-Kanik, Taletha M and Alva, Rafael S Villalobos and Migone, F},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/4RPS5ZM5/Thornton 等 - A Novel Framework for Testing Causal Reasoning in LLMs Design, Data Collection, and Evaluation.pdf:application/pdf},
}

@article{aditya_review_2024,
	title = {A review on air traffic flow management optimization: trends, challenges, and future directions},
	volume = {5},
	issn = {2662-9984},
	shorttitle = {A review on air traffic flow management optimization},
	url = {https://link.springer.com/10.1007/s43621-024-00781-7},
	doi = {10.1007/s43621-024-00781-7},
	abstract = {Air Traffic Flow Management (ATFM) is the backbone of modern aviation and ensures that aircraft move safely and efficiently through increasingly congested skies. As global air travel grows, managing air traffic has become more pressing than ever. This review assesses ten years of the ATFM literature, the period between 2014 and 2024, and discusses 162 studies published in peer-reviewed journals. Employing VOSViewer and Biblioshiny, this review analyzes the history of ATFM research. It explores the trends and gaps in research, which suggest there is room for improvement for more sound approaches. While optimization techniques have significantly improved efficiency and eased bottlenecks, the future lies in real-time solutions that can handle unpredictable events, from weather disruptions to technical failures. The review identified key areas for optimizing ATFM, categorized by primary focus: delay minimization, airspace congestion, and scheduling. It suggests ways in which more dynamic ATFM systems are possible in the growing global aviation network. By synthesizing the current research landscape, this review addresses the progress made. It offers a roadmap for future innovations that will enhance the safety, efficiency, and sustainability of air traffic management.},
	language = {en},
	number = {1},
	urldate = {2025-11-13},
	journal = {Discov Sustain},
	author = {Aditya, Verma and Aswin, Dande Sureshkumar and Dhaneesh, Somasundaram Vanitha and Chakravarthy, Sakthivelan and Kumar, Bhukya Shanmuk and Venkadavarahan, Marimuthu},
	month = dec,
	year = {2024},
	pages = {519},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/K3TJ3CHD/Aditya 等 - 2024 - A review on air traffic flow management optimization trends, challenges, and future directions.pdf:application/pdf},
}

@article{yang_adversarial_2025,
	title = {Adversarial prompt and fine-tuning attacks threaten medical large language models},
	volume = {16},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-025-64062-1},
	doi = {10.1038/s41467-025-64062-1},
	abstract = {Abstract
            The integration of Large Language Models (LLMs) into healthcare applications offers promising advancements in medical diagnostics, treatment recommendations, and patient care. However, the susceptibility of LLMs to adversarial attacks poses a significant threat, potentially leading to harmful outcomes in delicate medical contexts. This study investigates the vulnerability of LLMs to two types of adversarial attacks–prompt injections with malicious instructions and fine-tuning with poisoned samples–across three medical tasks: disease prevention, diagnosis, and treatment. Utilizing real-world patient data, we demonstrate that both open-source and proprietary LLMs are vulnerable to malicious manipulation across multiple tasks. We discover that while integrating poisoned data does not markedly degrade overall model performance on medical benchmarks, it can lead to noticeable shifts in fine-tuned model weights, suggesting a potential pathway for detecting and countering model attacks. This research highlights the urgent need for robust security measures and the development of defensive mechanisms to safeguard LLMs in medical applications, to ensure their safe and effective deployment in healthcare settings.},
	language = {en},
	number = {1},
	urldate = {2025-11-13},
	journal = {Nat Commun},
	author = {Yang, Yifan and Jin, Qiao and Huang, Furong and Lu, Zhiyong},
	month = oct,
	year = {2025},
	pages = {9011},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/SXNMUWK9/Yang 等 - 2025 - Adversarial prompt and fine-tuning attacks threaten medical large language models.pdf:application/pdf},
}

@article{xu_llm_2024,
	title = {{AN} {LLM} {CAN} {FOOL} {ITSELF}: {A} {PROMPT}-{BASED} {ADVERSARIAL} {ATTACK}},
	abstract = {The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness. This paper proposes an efficient tool to audit the LLM’s adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions. Our source code is available at https://github.com/GodXuxilie/PromptAttack.},
	language = {en},
	author = {Xu, Xilie and Kong, Keyi and Liu, Ning and Cui, Lizhen and Zhang, Jingfeng and Kankanhalli, Mohan},
	year = {2024},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/ESE4ADZT/Xu 等 - 2024 - AN LLM CAN FOOL ITSELF A PROMPT-BASED ADVERSARIAL ATTACK.pdf:application/pdf},
}

@misc{herrera-poyatos_overview_2025,
	title = {An overview of model uncertainty and variability in {LLM}-based sentiment analysis. {Challenges}, mitigation strategies and the role of explainability},
	url = {http://arxiv.org/abs/2504.04462},
	doi = {10.48550/arXiv.2504.04462},
	abstract = {Large Language Models (LLMs) have significantly advanced sentiment analysis, yet their inherent uncertainty and variability pose critical challenges to achieving reliable and consistent outcomes. This paper systematically explores the Model Variability Problem (MVP) in LLM-based sentiment analysis, characterized by inconsistent sentiment classification, polarization, and uncertainty arising from stochastic inference mechanisms, prompt sensitivity, and biases in training data. We analyze the core causes of MVP, presenting illustrative examples and a case study to highlight its impact. In addition, we investigate key challenges and mitigation strategies, paying particular attention to the role of temperature as a driver of output randomness and emphasizing the crucial role of explainability in improving transparency and user trust. By providing a structured perspective on stability, reproducibility, and trustworthiness, this study helps develop more reliable, explainable, and robust sentiment analysis models, facilitating their deployment in high-stakes domains such as finance, healthcare, and policymaking, among others.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Herrera-Poyatos, David and Peláez-González, Carlos and Zuheros, Cristina and Herrera-Poyatos, Andrés and Tejedor, Virilo and Herrera, Francisco and Montes, Rosana},
	month = apr,
	year = {2025},
	note = {arXiv:2504.04462 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/34B7KPN8/Herrera-Poyatos 等 - 2025 - An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitiga.pdf:application/pdf},
}

@article{saez_automation_2020,
	title = {Automation for {Separation} with {Continuous} {Descent} {Operations}: {Dynamic} {Aircraft} {Arrival} {Routes}},
	volume = {28},
	issn = {2380-9450},
	shorttitle = {Automation for {Separation} with {Continuous} {Descent} {Operations}},
	url = {https://arc.aiaa.org/doi/10.2514/1.D0176},
	doi = {10.2514/1.D0176},
	language = {en},
	number = {4},
	urldate = {2025-11-13},
	journal = {Journal of Air Transportation},
	author = {Sáez, Raúl and Prats, Xavier and Polishchuk, Tatiana and Polishchuk, Valentin and Schmidt, Christiane},
	month = oct,
	year = {2020},
	pages = {144--154},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/HY38CL5W/Sáez 等 - 2020 - Automation for Separation with Continuous Descent Operations Dynamic Aircraft Arrival Routes.pdf:application/pdf},
}

@misc{kiciman_causal_2024,
	title = {Causal {Reasoning} and {Large} {Language} {Models}: {Opening} a {New} {Frontier} for {Causality}},
	shorttitle = {Causal {Reasoning} and {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.00050},
	doi = {10.48550/arXiv.2305.00050},
	abstract = {The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a “behavorial” study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97\%, 13 points gain), counterfactual reasoning task (92\%, 20 points gain) and event causality (86\% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Kıcıman, Emre and Ness, Robert and Sharma, Amit and Tan, Chenhao},
	month = aug,
	year = {2024},
	note = {arXiv:2305.00050 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Human-Computer Interaction, Computer Science - Computers and Society, Statistics - Methodology},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/26UMEDV7/Kıcıman 等 - 2024 - Causal Reasoning and Large Language Models Opening a New Frontier for Causality.pdf:application/pdf},
}

@misc{kumar_certifying_2025,
	title = {Certifying {LLM} {Safety} against {Adversarial} {Prompting}},
	url = {http://arxiv.org/abs/2309.02705},
	doi = {10.48550/arXiv.2309.02705},
	abstract = {Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. It labels the input prompt as harmful if any of the subsequences or the prompt itself is detected as harmful by the filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu},
	month = feb,
	year = {2025},
	note = {arXiv:2309.02705 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/UCFYW6L9/Kumar 等 - 2025 - Certifying LLM Safety against Adversarial Prompting.pdf:application/pdf},
}

@misc{mousavi_garbage_2025,
	title = {Garbage {In}, {Reasoning} {Out}? {Why} {Benchmark} {Scores} are {Unreliable} and {What} to {Do} {About} {It}},
	shorttitle = {Garbage {In}, {Reasoning} {Out}?},
	url = {http://arxiv.org/abs/2506.23864},
	doi = {10.48550/arXiv.2506.23864},
	abstract = {We conduct a systematic audit of three widely used reasoning benchmarks, SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark items and evaluation methodology. Using five LLMs (GPT-3, 3.5, 4, o1, and LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic issues in benchmark design (e.g., duplicated items, ambiguous wording, and implausible answers), as well as scoring procedures that prioritize output form over reasoning process. Through systematic human annotation and reevaluation on cleaned benchmark subsets, we find that model scores often improve not due to due to erratic surface wording variations and not to improved reasoning. Infact, further analyses show that model performance is highly sensitive to minor input variations such as context availability and phrasing, revealing that high scores may reflect alignment with formatspecific cues rather than consistent inference based on the input. These findings challenge the validity of current benchmark-based claims about reasoning in LLMs, and highlight the need for evaluation protocols that assess reasoning as a process of drawing inference from available information, rather than as static output selection. We release audited data and evaluation tools to support more interpretable and diagnostic assessments of model reasoning1.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Mousavi, Seyed Mahed and Cecchinato, Edoardo and Hornikova, Lucia and Riccardi, Giuseppe},
	month = jul,
	year = {2025},
	note = {arXiv:2506.23864 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/38HFF8LP/Mousavi 等 - 2025 - Garbage In, Reasoning Out Why Benchmark Scores are Unreliable and What to Do About It.pdf:application/pdf},
}

@article{liu_large_nodate,
	title = {Large {Language} {Models} and {Causal} {Inference} in {Collaboration}: {A} {Comprehensive} {Survey}},
	abstract = {Causal inference has demonstrated significant potential to enhance Natural Language Processing (NLP) models in areas such as predictive accuracy, fairness, robustness, and explainability by capturing causal relationships among variables. The rise of generative Large Language Models (LLMs) has greatly impacted various language processing tasks. This survey focuses on research that evaluates or improves LLMs from a causal view in the following areas: reasoning capacity, fairness and safety issues, explainability, and handling multimodality. Meanwhile, LLMs can assist in causal inference tasks, such as causal relationship discovery and causal effect estimation, by leveraging their generation ability and knowledge learned during pre-training. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and robust artificial intelligence systems.},
	language = {en},
	author = {Liu, Xiaoyu and Xu, Paiheng and Wu, Junda and Yuan, Jiaxin and Yang, Yifan and Zhou, Yuhang and Liu, Fuxiao and Guan, Tianrui and Wang, Haoliang and Yu, Tong and McAuley, Julian and Ai, Wei and Huang, Furong},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/GQAZ2BU6/Liu 等 - Large Language Models and Causal Inference in Collaboration A Comprehensive Survey.pdf:application/pdf},
}

@inproceedings{joshi_llms_2024,
	address = {Miami, Florida, USA},
	title = {{LLMs} {Are} {Prone} to {Fallacies} in {Causal} {Inference}},
	url = {https://aclanthology.org/2024.emnlp-main.590},
	doi = {10.18653/v1/2024.emnlp-main.590},
	abstract = {Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks. However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. Thus, this work investigates: Can LLMs infer causal relations from other relational data in text? To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations. We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality.},
	language = {en},
	urldate = {2025-11-13},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Nitish and Saparov, Abulhair and Wang, Yixin and He, He},
	year = {2024},
	pages = {10553--10569},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/7FR5ADRZ/Joshi 等 - 2024 - LLMs Are Prone to Fallacies in Causal Inference.pdf:application/pdf},
}

@article{cong_manner_2024,
	title = {Manner implicatures in large language models},
	volume = {14},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-80571-3},
	doi = {10.1038/s41598-024-80571-3},
	abstract = {In human speakers’ daily conversations, what we do not say matters. We not only compute the literal semantics but also go beyond and draw inferences from what we could have said but chose not to. How well is this pragmatic reasoning process represented in pre-trained large language models (LLM)? In this study, we attempt to address this question through the lens of manner implicature, a pragmatic inference triggered by a violation of the Grice manner maxim. Manner implicature is a central member of the class of context-sensitive phenomena. The current work investigates to what extent pre-trained LLMs are able to identify and tease apart different shades of meaning in manner implicature. We constructed three metrics to explain LLMs’ behavior, including LLMs-surprisals, embedding vectors’ similarities, and natural language prompting. Results showed no striking evidence that LLMs have explainable representations of meaning. First, the LLMs-surprisal findings suggest that some LLMs showed above chance accuracy in capturing different dimensions of meaning, and they were able to differentiate neutral relations from entailment or implications, but they did not show consistent and robust sensitivities to more nuanced comparisons, such as entailment versus implications and equivalence versus entailment. Second, the similarity findings suggest that the perceived advantage of contextual over static embeddings was minimal, and contextual LLMs did not notably outperform static GloVe embeddings. LLMs and GloVe showed no significant difference, though distinctions between entailment and implication were slightly more observable in LLMs. Third, the prompting findings suggest no further supportive evidence indicating LLM’s competence in fully representing different shades of meaning. Overall, our study suggests that current dominant pre-training paradigms do not seem to lead to significant competence in manner implicature within our models. Our investigation sheds light on the design of datasets and benchmark metrics driven by formal and distributional linguistic theories.},
	language = {en},
	number = {1},
	urldate = {2025-11-13},
	journal = {Sci Rep},
	author = {Cong, Yan},
	month = nov,
	year = {2024},
	pages = {29113},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/49QCWURY/Cong - 2024 - Manner implicatures in large language models.pdf:application/pdf},
}

@misc{kirchhof_position_2025,
	title = {Position: {Uncertainty} {Quantification} {Needs} {Reassessment} for {Large}-language {Model} {Agents}},
	shorttitle = {Position},
	url = {http://arxiv.org/abs/2505.22655},
	doi = {10.48550/arXiv.2505.22655},
	abstract = {Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Kirchhof, Michael and Kasneci, Gjergji and Kasneci, Enkelejda},
	month = may,
	year = {2025},
	note = {arXiv:2505.22655 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/FTDQXZIJ/Kirchhof 等 - 2025 - Position Uncertainty Quantification Needs Reassessment for Large-language Model Agents.pdf:application/pdf},
}

@inproceedings{pedinotti_pragmatic_2022,
	address = {Seattle, USA},
	title = {Pragmatic and {Logical} {Inferences} in {NLI} {Systems}: {The} {Case} of {Conjunction} {Buttressing}},
	shorttitle = {Pragmatic and {Logical} {Inferences} in {NLI} {Systems}},
	url = {https://aclanthology.org/2022.unimplicit-1.2},
	doi = {10.18653/v1/2022.unimplicit-1.2},
	abstract = {An intelligent system is expected to perform reasonable inferences, accounting for both the literal meaning of a word and the meanings a word can acquire in different contexts. A specific kind of inference concerns the connective and, which in some cases gives rise to a temporal succession or causal interpretation in contrast with the logic, commutative one (Levinson, 2000). In this work, we investigate the phenomenon by creating a new dataset for evaluating the interpretation of and by NLI systems, which we use to test three Transformer-based models. Our results show that all systems generalize patterns that are consistent with both the logical and the pragmatic interpretation, perform inferences that are inconsistent with each other, and show clear divergences with both theoretical accounts and humans’ behavior.},
	language = {en},
	urldate = {2025-11-13},
	booktitle = {Proceedings of the {Second} {Workshop} on {Understanding} {Implicit} and {Underspecified} {Language}},
	publisher = {Association for Computational Linguistics},
	author = {Pedinotti, Paolo and Chersoni, Emmanuele and Santus, Enrico and Lenci, Alessandro},
	year = {2022},
	pages = {8--16},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/YASM7RF3/Pedinotti 等 - 2022 - Pragmatic and Logical Inferences in NLI Systems The Case of Conjunction Buttressing.pdf:application/pdf},
}

@article{li_security_2025,
	title = {Security concerns for {Large} {Language} {Models}: {A} survey},
	volume = {95},
	issn = {22142126},
	shorttitle = {Security concerns for {Large} {Language} {Models}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2214212625003217},
	doi = {10.1016/j.jisa.2025.104284},
	abstract = {Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: inference-time attacks via prompt manipulation; training-time attacks; misuse by malicious actors; and the inherent risks in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze existing defense mechanisms and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.},
	language = {en},
	urldate = {2025-11-13},
	journal = {Journal of Information Security and Applications},
	author = {Li, Miles Q. and Fung, Benjamin C.M.},
	month = dec,
	year = {2025},
	pages = {104284},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/M6QTPVYU/Li和Fung - 2025 - Security concerns for Large Language Models A survey.pdf:application/pdf},
}

@article{gao_spuq_nodate,
	title = {{SPUQ}: {Perturbation}-{Based} {Uncertainty} {Quantification} for {Large} {Language} {Models}},
	abstract = {In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50\% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs1.},
	language = {en},
	author = {Gao, Xiang and Zhang, Jiaxin and Mouatadid, Lalla and Das, Kamalika},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/P5MH9934/Gao 等 - SPUQ Perturbation-Based Uncertainty Quantification for Large Language Models.pdf:application/pdf},
}

@misc{nasr_attacker_2025,
	title = {The {Attacker} {Moves} {Second}: {Stronger} {Adaptive} {Attacks} {Bypass} {Defenses} {Against} {Llm} {Jailbreaks} and {Prompt} {Injections}},
	shorttitle = {The {Attacker} {Moves} {Second}},
	url = {http://arxiv.org/abs/2510.09023},
	doi = {10.48550/arXiv.2510.09023},
	abstract = {How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. We argue that this evaluation process is flawed.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Nasr, Milad and Carlini, Nicholas and Sitawarin, Chawin and Schulhoff, Sander V. and Hayes, Jamie and Ilie, Michael and Pluto, Juliette and Song, Shuang and Chaudhari, Harsh and Shumailov, Ilia and Thakurta, Abhradeep and Xiao, Kai Yuanqing and Terzis, Andreas and Tramèr, Florian},
	month = oct,
	year = {2025},
	note = {arXiv:2510.09023 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/5M6IIGJD/Nasr 等 - 2025 - The Attacker Moves Second Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prom.pdf:application/pdf},
}

@misc{tripathi_confidence_2025,
	title = {The {Confidence} {Paradox}: {Can} {LLM} {Know} {When} {It}'s {Wrong}},
	shorttitle = {The {Confidence} {Paradox}},
	url = {http://arxiv.org/abs/2506.23464},
	doi = {10.48550/arXiv.2506.23464},
	abstract = {Document Visual Question Answering (DocVQA) models often produce overconfident or ethically misaligned responses, especially under uncertainty. Existing models like LayoutLMv3, UDOP, and DONUT focus on accuracy but lack ethical calibration. We propose HonestVQA, a model-agnostic, self-supervised framework that aligns model confidence with correctness using weighted loss and contrastive learning. We introduce two new metrics—Honesty1 Score (H-Score) and Ethical Confidence Index (ECI)—to evaluate ethical alignment. HonestVQA improves accuracy and F1 by up to 4.3\% across SpDocVQA, InfographicsVQA, and SROIE datasets, while reducing overconfidence. It also generalizes well across domains, achieving 78.9\% accuracy and 76.1\% F1-score.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Tripathi, Sahil and Nafis, Md Tabrez and Hussain, Imran and Gao, Jiechao},
	month = oct,
	year = {2025},
	note = {arXiv:2506.23464 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/VKF7AN3P/Tripathi 等 - 2025 - The Confidence Paradox Can LLM Know When It's Wrong.pdf:application/pdf},
}

@misc{bakman_uncertainty_2025,
	title = {Uncertainty as {Feature} {Gaps}: {Epistemic} {Uncertainty} {Quantification} of {LLMs} in {Contextual} {Question}-{Answering}},
	shorttitle = {Uncertainty as {Feature} {Gaps}},
	url = {http://arxiv.org/abs/2510.02671},
	doi = {10.48550/arXiv.2510.02671},
	abstract = {Uncertainty Quantification (UQ) research has primarily focused on closed-book factual question answering (QA), while contextual QA remains unexplored, despite its importance in real-world applications. In this work, we focus on UQ for the contextual QA task and propose a theoretically grounded approach to quantify epistemic uncertainty. We begin by introducing a task-agnostic, token-level uncertainty measure defined as the cross-entropy between the predictive distribution of the given model and the unknown true distribution. By decomposing this measure, we isolate the epistemic component and approximate the true distribution by a perfectly prompted, idealized model. We then derive an upper bound for epistemic uncertainty and show that it can be interpreted as semantic feature gaps in the given model’s hidden representations relative to the ideal model. We further apply this generic framework to the contextual QA task and hypothesize that three features approximate this gap: context-reliance (using the provided context rather than parametric knowledge), context comprehension (extracting relevant information from context), and honesty (avoiding intentional lies). Using a top-down interpretability approach, we extract these features by using only a small number of labeled samples and ensemble them to form a robust uncertainty score. Experiments on multiple QA benchmarks in both in-distribution and out-of-distribution settings show that our method substantially outperforms state-of-the-art unsupervised (sampling-free and sampling-based) and supervised UQ methods, achieving up to a 13-point PRR improvement while incurring a negligible inference overhead.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Bakman, Yavuz and Kang, Sungmin and Huang, Zhiqi and Yaldiz, Duygu Nur and Belém, Catarina G. and Zhu, Chenyang and Kumar, Anoop and Samuel, Alfy and Avestimehr, Salman and Liu, Daben and Karimireddy, Sai Praneeth},
	month = oct,
	year = {2025},
	note = {arXiv:2510.02671 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/JD9CBU7T/Bakman 等 - 2025 - Uncertainty as Feature Gaps Epistemic Uncertainty Quantification of LLMs in Contextual Question-Ans.pdf:application/pdf},
}

@misc{liu_uncertainty_2025,
	title = {Uncertainty {Quantification} and {Confidence} {Calibration} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Uncertainty {Quantification} and {Confidence} {Calibration} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2503.15850},
	doi = {10.48550/arXiv.2503.15850},
	abstract = {Uncertainty quantification (UQ) enhances the reliability of Large Language Models (LLMs) by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions, including input, reasoning, parameter, and prediction uncertainty. We evaluate existing techniques, summarize existing benchmarks and metrics for UQ, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},
	month = jun,
	year = {2025},
	note = {arXiv:2503.15850 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/K3YAS2AJ/Liu 等 - 2025 - Uncertainty Quantification and Confidence Calibration in Large Language Models A Survey.pdf:application/pdf},
}

@misc{kang_uncertainty_2025,
	title = {Uncertainty {Quantification} for {Hallucination} {Detection} in {Large} {Language} {Models}: {Foundations}, {Methodology}, and {Future} {Directions}},
	shorttitle = {Uncertainty {Quantification} for {Hallucination} {Detection} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2510.12040},
	doi = {10.48550/arXiv.2510.12040},
	abstract = {The rapid advancement of large language models (LLMs) has transformed the landscape of natural language processing, enabling breakthroughs across a wide range of areas including question answering, machine translation, and text summarization. Yet, their deployment in real-world applications has raised concerns over reliability and trustworthiness, as LLMs remain prone to hallucinations that produce plausible but factually incorrect outputs. Uncertainty quantification (UQ) has emerged as a central research direction to address this issue, offering principled measures for assessing the trustworthiness of model generations. We begin by introducing the foundations of UQ, from its formal definition to the traditional distinction between epistemic and aleatoric uncertainty, and then highlight how these concepts have been adapted to the context of LLMs. Building on this, we examine the role of UQ in hallucination detection, where quantifying uncertainty provides a mechanism for identifying unreliable generations and improving reliability. We systematically categorize a wide spectrum of existing methods along multiple dimensions and present empirical results for several representative approaches. Finally, we discuss current limitations and outline promising future research directions, providing a clearer picture of the current landscape of LLM UQ for hallucination detection.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Kang, Sungmin and Bakman, Yavuz Faruk and Yaldiz, Duygu Nur and Buyukates, Baturalp and Avestimehr, Salman},
	month = oct,
	year = {2025},
	note = {arXiv:2510.12040 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/Y6GGAX23/Kang 等 - 2025 - Uncertainty Quantification for Hallucination Detection in Large Language Models Foundations, Method.pdf:application/pdf},
}

@inproceedings{ling_uncertainty_2024,
	address = {Mexico City, Mexico},
	title = {Uncertainty {Quantification} for {In}-{Context} {Learning} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.naacl-long.184},
	doi = {10.18653/v1/2024.naacl-long.184},
	abstract = {In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM’s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM’s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model’s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ\_ICL.},
	language = {en},
	urldate = {2025-11-13},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ling, Chen and Zhao, Xujiang and Zhang, Xuchao and Cheng, Wei and Liu, Yanchi and Sun, Yiyou and Oishi, Mika and Osaki, Takao and Matsuda, Katsushi and Ji, Jie and Bai, Guangji and Zhao, Liang and Chen, Haifeng},
	year = {2024},
	pages = {3357--3370},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/VMAFM52N/Ling 等 - 2024 - Uncertainty Quantification for In-Context Learning of Large Language Models.pdf:application/pdf},
}

@misc{gandhi_understanding_2023,
	title = {Understanding {Social} {Reasoning} in {Language} {Models} with {Language} {Models}},
	url = {http://arxiv.org/abs/2306.15448},
	doi = {10.48550/arXiv.2306.15448},
	abstract = {As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Gandhi, Kanishk and Fränken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D.},
	month = dec,
	year = {2023},
	note = {arXiv:2306.15448 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/7N8MLWJJ/Gandhi 等 - 2023 - Understanding Social Reasoning in Language Models with Language Models.pdf:application/pdf},
}

@article{hamissi_survey_2024,
	title = {A {Survey} on the {Unmanned} {Aircraft} {System} {Traffic} {Management}},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3617992},
	doi = {10.1145/3617992},
	abstract = {The Unmanned Aircraft System Traffic Management (UTM) system is a set of services offering an automated management of the airspace and thus providing safe and secure Unmanned Aerial Vehicle (UAV) flights in both controlled and uncontrolled airspace. Controlled airspace refers to the portion of the airspace that is under the authority of Air Traffic Control (ATC) and where separation services are offered, while uncontrolled airspace refers to the portion of airspace where aircraft are not regulated by ATC. This article is a comprehensive survey of the existing UTMs development efforts with a focus on the different UTMs architectures, the provided services, the used communication technologies and the decision-making process within UTMs. We firstly review the different UTM architecture and propose a novel UTM taxonomy based on high-level qualitative criteria. Secondly, we detail the services provided by UTMs with an emphasis on the used technologies in the identification, the surveillance, the monitoring, and the deconfliction services. Effective decision-making is crucial, particularly in emergency scenarios such as Air-to-Ground (A2G) communication loss, battery or motor malfunction, or encountering aerial obstacles, among other potential hazards. Despite its significance, the UTM decision-making process is not enough considered in the literature and especially in UTM surveys. We analyze and compare in this article both the centralized and decentralized UTM decision-making. Centralized decision-making is not conducted in real-time and primarily relies on Air-to-Ground (A2G) communication. In the decentralized case, the decision-making process primarily relies on communication and collaboration among UAVs with varying degrees of autonomy. We show in this paper that centralized decision-making may encounter issues with packet loss and imperfect data, which can negatively impact the quality of decision-making. We also highlight that the decentralized decision-making may also face challenges related to security and scalability, which can hinder its effectiveness. Finally, evaluating the performance of UTMs on a real environment raises several challenges and the simulation is a cost-effective alternative. Hence, we provide a summary of the existing UTMs simulators and discuss their main features.},
	language = {en},
	number = {3},
	urldate = {2025-11-13},
	journal = {ACM Comput. Surv.},
	author = {Hamissi, Asma and Dhraief, Amine},
	month = mar,
	year = {2024},
	pages = {1--37},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/6HA624PN/Hamissi和Dhraief - 2024 - A Survey on the Unmanned Aircraft System Traffic Management.pdf:application/pdf},
}

@misc{andriushchenko_agentharm_2025,
	title = {{AgentHarm}: {A} {Benchmark} for {Measuring} {Harmfulness} of {LLM} {Agents}},
	shorttitle = {{AgentHarm}},
	url = {http://arxiv.org/abs/2410.09024},
	doi = {10.48550/arXiv.2410.09024},
	abstract = {The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents—which use external tools and can execute multi-stage tasks—may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ ai-safety-institute/AgentHarm.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Andriushchenko, Maksym and Souly, Alexandra and Dziemian, Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin and Hendrycks, Dan and Zou, Andy and Kolter, Zico and Fredrikson, Matt and Winsor, Eric and Wynne, Jerome and Gal, Yarin and Davies, Xander},
	month = apr,
	year = {2025},
	note = {arXiv:2410.09024 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/NN37U45F/Andriushchenko 等 - 2025 - AgentHarm A Benchmark for Measuring Harmfulness of LLM Agents.pdf:application/pdf},
}

@inproceedings{parrish_bbq_2022,
	address = {Dublin, Ireland},
	title = {{BBQ}: {A} hand-built bias benchmark for question answering},
	shorttitle = {{BBQ}},
	url = {https://aclanthology.org/2022.findings-acl.165},
	doi = {10.18653/v1/2022.findings-acl.165},
	abstract = {It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluates model responses at two levels: (i) given an under-informative context, we test how strongly responses refect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice. We fnd that models often rely on stereotypes when the context is under-informative, meaning the model’s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conficts, with this difference widening to over 5 points on examples targeting gender for most models tested.},
	language = {en},
	urldate = {2025-11-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel},
	year = {2022},
	pages = {2086--2105},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/TQUC25WJ/Parrish 等 - 2022 - BBQ A hand-built bias benchmark for question answering.pdf:application/pdf},
}

@article{cantini_benchmarking_2025,
	title = {Benchmarking {Adversarial} {Robustness} to {Bias} {Elicitation} in {Large} {Language} {Models}: {Scalable} {Automated} {Assessment} with {LLM}-as-a-{Judge}},
	volume = {114},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Benchmarking {Adversarial} {Robustness} to {Bias} {Elicitation} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2504.07887},
	doi = {10.1007/s10994-025-06862-6},
	abstract = {The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities. To facilitate systematic benchmarking, we release a curated dataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying DeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is uneven, with age, disability, and intersectional biases among the most prominent. Some small models outperform larger ones in safety, suggesting that training and architecture may matter more than scale. However, no model is fully robust to adversarial elicitation, with jailbreak attacks using low-resource languages or refusal suppression proving effective across model families. We also find that successive LLM generations exhibit slight safety gains, while models fine-tuned for the medical domain tend to be less safe than their general-purpose counterparts.},
	language = {en},
	number = {11},
	urldate = {2025-11-13},
	journal = {Mach Learn},
	author = {Cantini, Riccardo and Orsino, Alessio and Ruggiero, Massimo and Talia, Domenico},
	month = nov,
	year = {2025},
	note = {arXiv:2504.07887 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {249},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/FX7I98AJ/Cantini 等 - 2025 - Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models Scalable Automated.pdf:application/pdf},
}

@misc{srivastava_beyond_2023,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {See Section 7 for detailed author contributions.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and Melo, Gerard de and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and Hoeve, Maartje ter and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan A. and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2023},
	note = {arXiv:2206.04615 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/MPP8WBUN/Srivastava 等 - 2023 - Beyond the Imitation Game Quantifying and extrapolating the capabilities of language models.pdf:application/pdf},
}

@inproceedings{zhang_clamber_2024,
	address = {Bangkok, Thailand},
	title = {{CLAMBER}: {A} {Benchmark} of {Identifying} and {Clarifying} {Ambiguous} {Information} {Needs} in {Large} {Language} {Models}},
	shorttitle = {{CLAMBER}},
	url = {https://aclanthology.org/2024.acl-long.578},
	doi = {10.18653/v1/2024.acl-long.578},
	abstract = {Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ∼ 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-theshelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at https://github.com/SCUNLP/CLAMBER.},
	language = {en},
	urldate = {2025-11-13},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Tong and Qin, Peixin and Deng, Yang and Huang, Chen and Lei, Wenqiang and Liu, Junhong and Jin, Dingnan and Liang, Hongru and Chua, Tat-Seng},
	year = {2024},
	pages = {10746--10766},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/I7IM8DEB/Zhang 等 - 2024 - CLAMBER A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Mod.pdf:application/pdf},
}

@inproceedings{holliday_conditional_2024,
	address = {Miami, Florida, USA},
	title = {Conditional and {Modal} {Reasoning} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.emnlp-main.222},
	doi = {10.18653/v1/2024.emnlp-main.222},
	abstract = {The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., ‘If Ann has a queen, then Bob has a jack’) and epistemic modals (e.g., ‘Ann might have an ace’, ‘Bob must have a king’). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibilities. Assessing LLMs on these inferences is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. All the LLMs we tested make some basic mistakes with conditionals or modals, though zero-shot chain-of-thought prompting helps them make fewer mistakes. Even the best performing LLMs make basic errors in modal reasoning, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals, and give answers about complex conditional inferences that do not match reported human judgments. These results highlight gaps in basic logical reasoning in today’s LLMs.},
	language = {en},
	urldate = {2025-11-13},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Holliday, Wesley H. and Mandelkern, Matthew and Zhang, Cedegao E.},
	year = {2024},
	pages = {3800--3821},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/MLL2AZL3/Holliday 等 - 2024 - Conditional and Modal Reasoning in Large Language Models.pdf:application/pdf},
}

@article{su_conflictbank_nodate,
	title = {{CONFLICTBANK}: {A} {Benchmark} for {Evaluating} {Knowledge} {Conﬂicts} in {Large} {Language} {Models}},
	abstract = {Large language models (LLMs) have achieved impressive advancements across numerous disciplines, yet the critical issue of knowledge conﬂicts, a major source of hallucinations, has rarely been studied. While a few research explored the conﬂicts between the inherent knowledge of LLMs and the retrieved contextual knowledge, a comprehensive assessment of knowledge conﬂict in LLMs is still missing. Motivated by this research gap, we ﬁrstly propose CONFLICTBANK, the largest benchmark with 7.45M claim-evidence pairs and 553k QA pairs, addressing conﬂicts from misinformation, temporal discrepancies, and semantic divergences. Using CONFLICTBANK, we conduct the thorough and controlled experiments for a comprehensive understanding of LLM behavior in knowledge conﬂicts, focusing on three key aspects: (i) conﬂicts encountered in retrieved knowledge, (ii) conﬂicts within the models’ encoded knowledge, and (iii) the interplay between these conﬂict forms. Our investigation delves into four model families and twelve LLM instances and provides insights into conﬂict types, model sizes, and the impact at different stages. We believe that knowledge conﬂicts represent a critical bottleneck to achieving trustworthy artiﬁcial intelligence and hope our work will offer valuable guidance for future model training and development. Resources are available at https://github.com/zhaochen0110/conflictbank.},
	language = {en},
	author = {Su, Zhaochen and Zhang, Jun and Qu, Xiaoye and Zhu, Tong and Li, Yanshu and Sun, Jiashuo and Li, Juntao and Zhang, Min and Cheng, Yu},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/5WIG8XLV/Su 等 - CONFLICTBANK A Benchmark for Evaluating Knowledge Conﬂicts in Large Language Models.pdf:application/pdf},
}

@misc{bai_constitutional_2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle = {Constitutional {AI}},
	url = {http://arxiv.org/abs/2212.08073},
	doi = {10.48550/arXiv.2212.08073},
	abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through selfimprovement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as ‘Constitutional AI’. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then ﬁnetune the original model on revised responses. In the RL phase, we sample from the ﬁnetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use ‘RL from AI Feedback’ (RLAIF). As a result we are able to train a harmless but nonevasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/LGK7PMY5/Bai 等 - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf:application/pdf},
}

@misc{wang_decodingtrust_2024,
	title = {{DecodingTrust}: {A} {Comprehensive} {Assessment} of {Trustworthiness} in {GPT} {Models}},
	shorttitle = {{DecodingTrust}},
	url = {http://arxiv.org/abs/2306.11698},
	doi = {10.48550/arXiv.2306.11698},
	abstract = {Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance – where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives – including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/; our dataset can be previewed at https: //huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of this work is at https://openreview.net/pdf?id=kaHpo8OZw2.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and Truong, Sang T. and Arora, Simran and Mazeika, Mantas and Hendrycks, Dan and Lin, Zinan and Cheng, Yu and Koyejo, Sanmi and Song, Dawn and Li, Bo},
	month = feb,
	year = {2024},
	note = {arXiv:2306.11698 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/PGWFL586/Wang 等 - 2024 - DecodingTrust A Comprehensive Assessment of Trustworthiness in GPT Models.pdf:application/pdf},
}

@misc{yamin_failure_2025,
	title = {Failure {Modes} of {LLMs} for {Causal} {Reasoning} on {Narratives}},
	url = {http://arxiv.org/abs/2410.23884},
	doi = {10.48550/arXiv.2410.23884},
	abstract = {The ability to robustly identify causal relationships is essential for autonomous decision-making and adaptation to novel scenarios. However, accurately inferring causal structure requires integrating both world knowledge and abstract logical reasoning. In this work, we investigate the interaction between these two capabilities through the representative task of causal reasoning over narratives. Through controlled synthetic, semi-synthetic and real-world experiments, we find that stateof-the-art large language models (LLMs) often rely on superficial heuristics—for example, inferring causality from event order or recalling memorized world knowledge without attending to context. Furthermore, we show that simple reformulations of the task can elicit more robust reasoning behavior. Our evaluation spans a range of causal structures, from linear chains to complex graphs involving colliders and forks. These findings uncover systematic patterns in how LLMs perform causal reasoning and lay the groundwork for developing methods that better align LLM behavior with principled causal inference.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Yamin, Khurram and Gupta, Shantanu and Ghosal, Gaurav R. and Lipton, Zachary C. and Wilder, Bryan},
	month = jun,
	year = {2025},
	note = {arXiv:2410.23884 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/GXYVD5AP/Yamin 等 - 2025 - Failure Modes of LLMs for Causal Reasoning on Narratives.pdf:application/pdf},
}

@misc{mirzadeh_gsm-symbolic_2025,
	title = {{GSM}-{Symbolic}: {Understanding} the {Limitations} of {Mathematical} {Reasoning} in {Large} {Language} {Models}},
	shorttitle = {{GSM}-{Symbolic}},
	url = {http://arxiv.org/abs/2410.05229},
	doi = {10.48550/arXiv.2410.05229},
	abstract = {Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65\%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
	month = aug,
	year = {2025},
	note = {arXiv:2410.05229 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/D8XNHDEM/Mirzadeh 等 - 2025 - GSM-Symbolic Understanding the Limitations of Mathematical Reasoning in Large Language Models.pdf:application/pdf},
}

@misc{liang_holistic_2023,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what’s missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible (87.5\% of the time), ensuring that metrics beyond accuracy don’t fall to the wayside, and that trade-offs across models and metrics are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to more deeply analyze specific aspects (e.g. knowledge, reasoning, memorization/copyright, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, including 21 scenarios that were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on a set of core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings concerning the interplay between different scenarios, metrics, and models. For full transparency, we release all raw model prompts and completions publicly1 for further analysis, as well as a general modular toolkit for easily adding new scenarios, models, metrics, and prompting strategies.2 We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = oct,
	year = {2023},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/KGWWJ53Q/Liang 等 - 2023 - Holistic Evaluation of Language Models.pdf:application/pdf},
}

@misc{phan_humanitys_2025,
	title = {Humanity's {Last} {Exam}},
	url = {http://arxiv.org/abs/2501.14249},
	doi = {10.48550/arXiv.2501.14249},
	abstract = {Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90{\textbackslash}\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Zhang, Chen Bo Calvin and Shaaban, Mohamed and Ling, John and Shi, Sean and Choi, Michael and Agrawal, Anish and Chopra, Arnav and Khoja, Adam and Kim, Ryan and Ren, Richard and Hausenloy, Jason and Zhang, Oliver and Mazeika, Mantas and Dodonov, Dmitry and Nguyen, Tung and Lee, Jaeho and Anderson, Daron and Doroshenko, Mikhail and Stokes, Alun Cennyth and Mahmood, Mobeen and Pokutnyi, Oleksandr and Iskra, Oleg and Wang, Jessica P. and Levin, John-Clark and Kazakov, Mstyslav and Feng, Fiona and Feng, Steven Y. and Zhao, Haoran and Yu, Michael and Gangal, Varun and Zou, Chelsea and Wang, Zihan and Popov, Serguei and Gerbicz, Robert and Galgon, Geoff and Schmitt, Johannes and Yeadon, Will and Lee, Yongki and Sauers, Scott and Sanchez, Alvaro and Giska, Fabian and Roth, Marc and Riis, Søren and Utpala, Saiteja and Burns, Noah and Goshu, Gashaw M. and Naiya, Mohinder Maheshbhai and Agu, Chidozie and Giboney, Zachary and Cheatom, Antrell and Fournier-Facio, Francesco and Crowson, Sarah-Jane and Finke, Lennart and Cheng, Zerui and Zampese, Jennifer and Hoerr, Ryan G. and Nandor, Mark and Park, Hyunwoo and Gehrunger, Tim and Cai, Jiaqi and McCarty, Ben and Garretson, Alexis C. and Taylor, Edwin and Sileo, Damien and Ren, Qiuyu and Qazi, Usman and Li, Lianghui and Nam, Jungbae and Wydallis, John B. and Arkhipov, Pavel and Shi, Jack Wei Lun and Bacho, Aras and Willcocks, Chris G. and Cao, Hangrui and Motwani, Sumeet and Santos, Emily de Oliveira and Veith, Johannes and Vendrow, Edward and Cojoc, Doru and Zenitani, Kengo and Robinson, Joshua and Tang, Longke and Li, Yuqi and Vendrow, Joshua and Fraga, Natanael Wildner and Kuchkin, Vladyslav and Maksimov, Andrey Pupasov and Marion, Pierre and Efremov, Denis and Lynch, Jayson and Liang, Kaiqu and Mikov, Aleksandar and Gritsevskiy, Andrew and Guillod, Julien and Demir, Gözdenur and Martinez, Dakotah and Pageler, Ben and Zhou, Kevin and Soori, Saeed and Press, Ori and Tang, Henry and Rissone, Paolo and Green, Sean R. and Brüssel, Lina and Twayana, Moon and Dieuleveut, Aymeric and Imperial, Joseph Marvin and Prabhu, Ameya and Yang, Jinzhou and Crispino, Nick and Rao, Arun and Zvonkine, Dimitri and Loiseau, Gabriel and Kalinin, Mikhail and Lukas, Marco and Manolescu, Ciprian and Stambaugh, Nate and Mishra, Subrata and Hogg, Tad and Bosio, Carlo and Coppola, Brian P. and Salazar, Julian and Jin, Jaehyeok and Sayous, Rafael and Ivanov, Stefan and Schwaller, Philippe and Senthilkuma, Shaipranesh and Bran, Andres M. and Algaba, Andres and Houte, Kelsey Van den and Sypt, Lynn Van Der and Verbeken, Brecht and Noever, David and Kopylov, Alexei and Myklebust, Benjamin and Li, Bikun and Schut, Lisa and Zheltonozhskii, Evgenii and Yuan, Qiaochu and Lim, Derek and Stanley, Richard and Yang, Tong and Maar, John and Wykowski, Julian and Oller, Martí and Sahu, Anmol and Ardito, Cesare Giulio and Hu, Yuzheng and Kamdoum, Ariel Ghislain Kemogne and Jin, Alvin and Vilchis, Tobias Garcia and Zu, Yuexuan and Lackner, Martin and Koppel, James and Sun, Gongbo and Antonenko, Daniil S. and Chern, Steffi and Zhao, Bingchen and Arsene, Pierrot and Cavanagh, Joseph M. and Li, Daofeng and Shen, Jiawei and Crisostomi, Donato and Zhang, Wenjin and Dehghan, Ali and Ivanov, Sergey and Perrella, David and Kaparov, Nurdin and Zang, Allen and Sucholutsky, Ilia and Kharlamova, Arina and Orel, Daniil and Poritski, Vladislav and Ben-David, Shalev and Berger, Zachary and Whitfill, Parker and Foster, Michael and Munro, Daniel and Ho, Linh and Sivarajan, Shankar and Hava, Dan Bar and Kuchkin, Aleksey and Holmes, David and Rodriguez-Romero, Alexandra and Sommerhage, Frank and Zhang, Anji and Moat, Richard and Schneider, Keith and Kazibwe, Zakayo and Clarke, Don and Kim, Dae Hyun and Dias, Felipe Meneguitti and Fish, Sara and Elser, Veit and Kreiman, Tobias and Vilchis, Victor Efren Guadarrama and Klose, Immo and Anantheswaran, Ujjwala and Zweiger, Adam and Rawal, Kaivalya and Li, Jeffery and Nguyen, Jeremy and Daans, Nicolas and Heidinger, Haline and Radionov, Maksim and Rozhoň, Václav and Ginis, Vincent and Stump, Christian and Cohen, Niv and Poświata, Rafał and Tkadlec, Josef and Goldfarb, Alan and Wang, Chenguang and Padlewski, Piotr and Barzowski, Stanislaw and Montgomery, Kyle and Stendall, Ryan and Tucker-Foltz, Jamie and Stade, Jack and Rogers, T. Ryan and Goertzen, Tom and Grabb, Declan and Shukla, Abhishek and Givré, Alan and Ambay, John Arnold and Sen, Archan and Aziz, Muhammad Fayez and Inlow, Mark H. and He, Hao and Zhang, Ling and Kaddar, Younesse and Ängquist, Ivar and Chen, Yanxu and Wang, Harrison K. and Ramakrishnan, Kalyan and Thornley, Elliott and Terpin, Antonio and Schoelkopf, Hailey and Zheng, Eric and Carmi, Avishy and Brown, Ethan D. L. and Zhu, Kelin and Bartolo, Max and Wheeler, Richard and Stehberger, Martin and Bradshaw, Peter and Heimonen, J. P. and Sridhar, Kaustubh and Akov, Ido and Sandlin, Jennifer and Makarychev, Yury and Tam, Joanna and Hoang, Hieu and Cunningham, David M. and Goryachev, Vladimir and Patramanis, Demosthenes and Krause, Michael and Redenti, Andrew and Aldous, David and Lai, Jesyin and Coleman, Shannon and Xu, Jiangnan and Lee, Sangwon and Magoulas, Ilias and Zhao, Sandy and Tang, Ning and Cohen, Michael K. and Paradise, Orr and Kirchner, Jan Hendrik and Ovchynnikov, Maksym and Matos, Jason O. and Shenoy, Adithya and Wang, Michael and Nie, Yuzhou and Sztyber-Betley, Anna and Faraboschi, Paolo and Riblet, Robin and Crozier, Jonathan and Halasyamani, Shiv and Verma, Shreyas and Joshi, Prashant and Meril, Eli and Ma, Ziqiao and Andréoletti, Jérémy and Singhal, Raghav and Platnick, Jacob and Nevirkovets, Volodymyr and Basler, Luke and Ivanov, Alexander and Khoury, Seri and Gustafsson, Nils and Piccardo, Marco and Mostaghimi, Hamid and Chen, Qijia and Singh, Virendra and Khánh, Tran Quoc and Rosu, Paul and Szlyk, Hannah and Brown, Zachary and Narayan, Himanshu and Menezes, Aline and Roberts, Jonathan and Alley, William and Sun, Kunyang and Patel, Arkil and Lamparth, Max and Reuel, Anka and Xin, Linwei and Xu, Hanmeng and Loader, Jacob and Martin, Freddie and Wang, Zixuan and Achilleos, Andrea and Preu, Thomas and Korbak, Tomek and Bosio, Ida and Kazemi, Fereshteh and Chen, Ziye and Bálint, Biró and Lo, Eve J. Y. and Wang, Jiaqi and Nunes, Maria Inês S. and Milbauer, Jeremiah and Bari, M. Saiful and Wang, Zihao and Ansarinejad, Behzad and Sun, Yewen and Durand, Stephane and Elgnainy, Hossam and Douville, Guillaume and Tordera, Daniel and Balabanian, George and Wolff, Hew and Kvistad, Lynna and Milliron, Hsiaoyun and Sakor, Ahmad and Eron, Murat and O, Andrew Favre D. and Shah, Shailesh and Zhou, Xiaoxiang and Kamalov, Firuz and Abdoli, Sherwin and Santens, Tim and Barkan, Shaul and Tee, Allison and Zhang, Robin and Tomasiello, Alessandro and Luca, G. Bruno De and Looi, Shi-Zhuo and Le, Vinh-Kha and Kolt, Noam and Pan, Jiayi and Rodman, Emma and Drori, Jacob and Fossum, Carl J. and Muennighoff, Niklas and Jagota, Milind and Pradeep, Ronak and Fan, Honglu and Eicher, Jonathan and Chen, Michael and Thaman, Kushal and Merrill, William and Firsching, Moritz and Harris, Carter and Ciobâcă, Stefan and Gross, Jason and Pandey, Rohan and Gusev, Ilya and Jones, Adam and Agnihotri, Shashank and Zhelnov, Pavel and Mofayezi, Mohammadreza and Piperski, Alexander and Zhang, David K. and Dobarskyi, Kostiantyn and Leventov, Roman and Soroko, Ignat and Duersch, Joshua and Taamazyan, Vage and Ho, Andrew and Ma, Wenjie and Held, William and Xian, Ruicheng and Zebaze, Armel Randy and Mohamed, Mohanad and Leser, Julian Noah and Yuan, Michelle X. and Yacar, Laila and Lengler, Johannes and Olszewska, Katarzyna and Fratta, Claudio Di and Oliveira, Edson and Jackson, Joseph W. and Zou, Andy and Chidambaram, Muthu and Manik, Timothy and Haffenden, Hector and Stander, Dashiell and Dasouqi, Ali and Shen, Alexander and Golshani, Bita and Stap, David and Kretov, Egor and Uzhou, Mikalai and Zhidkovskaya, Alina Borisovna and Winter, Nick and Rodriguez, Miguel Orbegozo and Lauff, Robert and Wehr, Dustin and Tang, Colin and Hossain, Zaki and Phillips, Shaun and Samuele, Fortuna and Ekström, Fredrik and Hammon, Angela and Patel, Oam and Farhidi, Faraz and Medley, George and Mohammadzadeh, Forough and Peñaflor, Madellene and Kassahun, Haile and Friedrich, Alena and Perez, Rayner Hernandez and Pyda, Daniel and Sakal, Taom and Dhamane, Omkar and Mirabadi, Ali Khajegili and Hallman, Eric and Okutsu, Kenchi and Battaglia, Mike and Maghsoudimehrabani, Mohammad and Amit, Alon and Hulbert, Dave and Pereira, Roberto and Weber, Simon and Handoko and Peristyy, Anton and Malina, Stephen and Mehkary, Mustafa and Aly, Rami and Reidegeld, Frank and Dick, Anna-Katharina and Friday, Cary and Singh, Mukhwinder and Shapourian, Hassan and Kim, Wanyoung and Costa, Mariana and Gurdogan, Hubeyb and Kumar, Harsh and Ceconello, Chiara and Zhuang, Chao and Park, Haon and Carroll, Micah and Tawfeek, Andrew R. and Steinerberger, Stefan and Aggarwal, Daattavya and Kirchhof, Michael and Dai, Linjie and Kim, Evan and Ferret, Johan and Shah, Jainam and Wang, Yuzhou and Yan, Minghao and Burdzy, Krzysztof and Zhang, Lixin and Franca, Antonio and Pham, Diana T. and Loh, Kang Yong and Robinson, Joshua and Jackson, Abram and Giordano, Paolo and Petersen, Philipp and Cosma, Adrian and Colino, Jesus and White, Colin and Votava, Jacob and Vinnikov, Vladimir and Delaney, Ethan and Spelda, Petr and Stritecky, Vit and Shahid, Syed M. and Mourrat, Jean-Christophe and Vetoshkin, Lavr and Sponselee, Koen and Bacho, Renas and Yong, Zheng-Xin and Rosa, Florencia de la and Cho, Nathan and Li, Xiuyu and Malod, Guillaume and Weller, Orion and Albani, Guglielmo and Lang, Leon and Laurendeau, Julien and Kazakov, Dmitry and Adesanya, Fatimah and Portier, Julien and Hollom, Lawrence and Souza, Victor and Zhou, Yuchen Anna and Degorre, Julien and Yalın, Yiğit and Obikoya, Gbenga Daniel and Rai and Bigi, Filippo and Boscá, M. C. and Shumar, Oleg and Bacho, Kaniuar and Recchia, Gabriel and Popescu, Mara and Shulga, Nikita and Tanwie, Ngefor Mildred and Lux, Thomas C. H. and Rank, Ben and Ni, Colin and Brooks, Matthew and Yakimchyk, Alesia and Huanxu and Liu and Cavalleri, Stefano and Häggström, Olle and Verkama, Emil and Newbould, Joshua and Gundlach, Hans and Brito-Santana, Leonor and Amaro, Brian and Vajipey, Vivek and Grover, Rynaa and Wang, Ting and Kratish, Yosi and Li, Wen-Ding and Gopi, Sivakanth and Caciolai, Andrea and Witt, Christian Schroeder de and Hernández-Cámara, Pablo and Rodolà, Emanuele and Robins, Jules and Williamson, Dominic and Cheng, Vincent and Raynor, Brad and Qi, Hao and Segev, Ben and Fan, Jingxuan and Martinson, Sarah and Wang, Erik Y. and Hausknecht, Kaylie and Brenner, Michael P. and Mao, Mao and Demian, Christoph and Kassani, Peyman and Zhang, Xinyu and Avagian, David and Scipio, Eshawn Jessica and Ragoler, Alon and Tan, Justin and Sims, Blake and Plecnik, Rebeka and Kirtland, Aaron and Bodur, Omer Faruk and Shinde, D. P. and Labrador, Yan Carlos Leyva and Adoul, Zahra and Zekry, Mohamed and Karakoc, Ali and Santos, Tania C. B. and Shamseldeen, Samir and Karim, Loukmane and Liakhovitskaia, Anna and Resman, Nate and Farina, Nicholas and Gonzalez, Juan Carlos and Maayan, Gabe and Anderson, Earth and Pena, Rodrigo De Oliveira and Kelley, Elizabeth and Mariji, Hodjat and Pouriamanesh, Rasoul and Wu, Wentao and Finocchio, Ross and Alarab, Ismail and Cole, Joshua and Ferreira, Danyelle and Johnson, Bryan and Safdari, Mohammad and Dai, Liangti and Arthornthurasuk, Siriphan and McAlister, Isaac C. and Moyano, Alejandro José and Pronin, Alexey and Fan, Jing and Ramirez-Trinidad, Angel and Malysheva, Yana and Pottmaier, Daphiny and Taheri, Omid and Stepanic, Stanley and Perry, Samuel and Askew, Luke and Rodríguez, Raúl Adrián Huerta and Minissi, Ali M. R. and Lorena, Ricardo and Iyer, Krishnamurthy and Fasiludeen, Arshad Anil and Clark, Ronald and Ducey, Josh and Piza, Matheus and Somrak, Maja and Vergo, Eric and Qin, Juehang and Borbás, Benjámin and Chu, Eric and Lindsey, Jack and Jallon, Antoine and McInnis, I. M. J. and Chen, Evan and Semler, Avi and Gloor, Luk and Shah, Tej and Carauleanu, Marc and Lauer, Pascal and Huy, Tran Đuc and Shahrtash, Hossein and Duc, Emilien and Lewark, Lukas and Brown, Assaf and Albanie, Samuel and Weber, Brian and Vaz, Warren S. and Clavier, Pierre and Fan, Yiyang and Silva, Gabriel Poesia Reis e and Long and Lian and Abramovitch, Marcus and Jiang, Xi and Mendoza, Sandra and Islam, Murat and Gonzalez, Juan and Mavroudis, Vasilios and Xu, Justin and Kumar, Pawan and Goswami, Laxman Prasad and Bugas, Daniel and Heydari, Nasser and Jeanplong, Ferenc and Jansen, Thorben and Pinto, Antonella and Apronti, Archimedes and Galal, Abdallah and Ze-An, Ng and Singh, Ankit and Jiang, Tong and Xavier, Joan of Arc and Agarwal, Kanu Priya and Berkani, Mohammed and Zhang, Gang and Du, Zhehang and Junior, Benedito Alves de Oliveira and Malishev, Dmitry and Remy, Nicolas and Hartman, Taylor D. and Tarver, Tim and Mensah, Stephen and Loume, Gautier Abou and Morak, Wiktor and Habibi, Farzad and Hoback, Sarah and Cai, Will and Gimenez, Javier and Montecillo, Roselynn Grace and Łucki, Jakub and Campbell, Russell and Sharma, Asankhaya and Meer, Khalida and Gul, Shreen and Gonzalez, Daniel Espinosa and Alapont, Xavier and Hoover, Alex and Chhablani, Gunjan and Vargus, Freddie and Agarwal, Arunim and Jiang, Yibo and Patil, Deepakkumar and Outevsky, David and Scaria, Kevin Joseph and Maheshwari, Rajat and Dendane, Abdelkader and Shukla, Priti and Cartwright, Ashley and Bogdanov, Sergei and Mündler, Niels and Möller, Sören and Arnaboldi, Luca and Thaman, Kunvar and Siddiqi, Muhammad Rehan and Saxena, Prajvi and Gupta, Himanshu and Fruhauff, Tony and Sherman, Glen and Vincze, Mátyás and Usawasutsakorn, Siranut and Ler, Dylan and Radhakrishnan, Anil and Enyekwe, Innocent and Salauddin, Sk Md and Muzhen, Jiang and Maksapetyan, Aleksandr and Rossbach, Vivien and Harjadi, Chris and Bahaloohoreh, Mohsen and Sparrow, Claire and Sidhu, Jasdeep and Ali, Sam and Bian, Song and Lai, John and Singer, Eric and Uro, Justine Leon and Bateman, Greg and Sayed, Mohamed and Menshawy, Ahmed and Duclosel, Darling and Bezzi, Dario and Jain, Yashaswini and Aaron, Ashley and Tiryakioglu, Murat and Siddh, Sheeshram and Krenek, Keith and Shah, Imad Ali and Jin, Jun and Creighton, Scott and Peskoff, Denis and EL-Wasif, Zienab and P, Ragavendran and Richmond, Michael and McGowan, Joseph and Patwardhan, Tejal and Sun, Hao-Yu and Sun, Ting and Zubić, Nikola and Sala, Samuele and Ebert, Stephen and Kaddour, Jean and Schottdorf, Manuel and Wang, Dianzhuo and Petruzella, Gerol and Meiburg, Alex and Medved, Tilen and ElSheikh, Ali and Hebbar, S. Ashwin and Vaquero, Lorenzo and Yang, Xianjun and Poulos, Jason and Zouhar, Vilém and Bogdanik, Sergey and Zhang, Mingfang and Sanz-Ros, Jorge and Anugraha, David and Dai, Yinwei and Nhu, Anh N. and Wang, Xue and Demircali, Ali Anil and Jia, Zhibai and Zhou, Yuyin and Wu, Juncheng and He, Mike and Chandok, Nitin and Sinha, Aarush and Luo, Gaoxiang and Le, Long and Noyé, Mickaël and Perełkiewicz, Michał and Pantidis, Ioannis and Qi, Tianbo and Purohit, Soham Sachin and Parcalabescu, Letitia and Nguyen, Thai-Hoa and Winata, Genta Indra and Ponti, Edoardo M. and Li, Hanchen and Dhole, Kaustubh and Park, Jongee and Abbondanza, Dario and Wang, Yuanli and Nayak, Anupam and Caetano, Diogo M. and Wong, Antonio A. W. L. and Rio-Chanona, Maria del and Kondor, Dániel and Francois, Pieter and Chalstrey, Ed and Zsambok, Jakob and Hoyer, Dan and Reddish, Jenny and Hauser, Jakob and Rodrigo-Ginés, Francisco-Javier and Datta, Suchandra and Shepherd, Maxwell and Kamphuis, Thom and Zhang, Qizheng and Kim, Hyunjun and Sun, Ruiji and Yao, Jianzhu and Dernoncourt, Franck and Krishna, Satyapriya and Rismanchian, Sina and Pu, Bonan and Pinto, Francesco and Wang, Yingheng and Shridhar, Kumar and Overholt, Kalon J. and Briia, Glib and Nguyen, Hieu and David and Bartomeu, Soler and Pang, Tony CY and Wecker, Adam and Xiong, Yifan and Li, Fanfei and Huber, Lukas S. and Jaeger, Joshua and Maddalena, Romano De and Lù, Xing Han and Zhang, Yuhui and Beger, Claas and Kon, Patrick Tser Jern and Li, Sean and Sanker, Vivek and Yin, Ming and Liang, Yihao and Zhang, Xinlu and Agrawal, Ankit and Yifei, Li S. and Zhang, Zechen and Cai, Mu and Sonmez, Yasin and Cozianu, Costin and Li, Changhao and Slen, Alex and Yu, Shoubin and Park, Hyun Kyu and Sarti, Gabriele and Briański, Marcin and Stolfo, Alessandro and Nguyen, Truong An and Zhang, Mike and Perlitz, Yotam and Hernandez-Orallo, Jose and Li, Runjia and Shabani, Amin and Juefei-Xu, Felix and Dhingra, Shikhar and Zohar, Orr and Nguyen, My Chiffon and Pondaven, Alexander and Yilmaz, Abdurrahim and Zhao, Xuandong and Jin, Chuanyang and Jiang, Muyan and Todoran, Stefan and Han, Xinyao and Kreuer, Jules and Rabern, Brian and Plassart, Anna and Maggetti, Martino and Yap, Luther and Geirhos, Robert and Kean, Jonathon and Wang, Dingsu and Mollaei, Sina and Sun, Chenkai and Yin, Yifan and Wang, Shiqi and Li, Rui and Chang, Yaowen and Wei, Anjiang and Bizeul, Alice and Wang, Xiaohan and Arrais, Alexandre Oliveira and Mukherjee, Kushin and Chamorro-Padial, Jorge and Liu, Jiachen and Qu, Xingyu and Guan, Junyi and Bouyamourn, Adam and Wu, Shuyu and Plomecka, Martyna and Chen, Junda and Tang, Mengze and Deng, Jiaqi and Subramanian, Shreyas and Xi, Haocheng and Chen, Haoxuan and Zhang, Weizhi and Ren, Yinuo and Tu, Haoqin and Kim, Sejong and Chen, Yushun and Marjanović, Sara Vera and Ha, Junwoo and Luczyna, Grzegorz and Ma, Jeff J. and Shen, Zewen and Song, Dawn and Zhang, Cedegao E. and Wang, Zhun and Gendron, Gaël and Xiao, Yunze and Smucker, Leo and Weng, Erica and Lee, Kwok Hao and Ye, Zhe and Ermon, Stefano and Lopez-Miguel, Ignacio D. and Knights, Theo and Gitter, Anthony and Park, Namkyu and Wei, Boyi and Chen, Hongzheng and Pai, Kunal and Elkhanany, Ahmed and Lin, Han and Siedler, Philipp D. and Fang, Jichao and Mishra, Ritwik and Zsolnai-Fehér, Károly and Jiang, Xilin and Khan, Shadab and Yuan, Jun and Jain, Rishab Kumar and Lin, Xi and Peterson, Mike and Wang, Zhe and Malusare, Aditya and Tang, Maosen and Gupta, Isha and Fosin, Ivan and Kang, Timothy and Dworakowska, Barbara and Matsumoto, Kazuki and Zheng, Guangyao and Sewuster, Gerben and Villanueva, Jorge Pretel and Rannev, Ivan and Chernyavsky, Igor and Chen, Jiale and Banik, Deepayan and Racz, Ben and Dong, Wenchao and Wang, Jianxin and Bashmal, Laila and Gonçalves, Duarte V. and Hu, Wei and Bar, Kaushik and Bohdal, Ondrej and Patlan, Atharv Singh and Dhuliawala, Shehzaad and Geirhos, Caroline and Wist, Julien and Kansal, Yuval and Chen, Bingsen and Tire, Kutay and Yücel, Atak Talay and Christof, Brandon and Singla, Veerupaksh and Song, Zijian and Chen, Sanxing and Ge, Jiaxin and Ponkshe, Kaustubh and Park, Isaac and Shi, Tianneng and Ma, Martin Q. and Mak, Joshua and Lai, Sherwin and Moulin, Antoine and Cheng, Zhuo and Zhu, Zhanda and Zhang, Ziyi and Patil, Vaidehi and Jha, Ketan and Men, Qiutong and Wu, Jiaxuan and Zhang, Tianchi and Vieira, Bruno Hebling and Aji, Alham Fikri and Chung, Jae-Won and Mahfoud, Mohammed and Hoang, Ha Thi and Sperzel, Marc and Hao, Wei and Meding, Kristof and Xu, Sihan and Kostakos, Vassilis and Manini, Davide and Liu, Yueying and Toukmaji, Christopher and Paek, Jay and Yu, Eunmi and Demircali, Arif Engin and Sun, Zhiyi and Dewerpe, Ivan and Qin, Hongsen and Pflugfelder, Roman and Bailey, James and Morris, Johnathan and Heilala, Ville and Rosset, Sybille and Yu, Zishun and Chen, Peter E. and Yeo, Woongyeong and Jain, Eeshaan and Yang, Ryan and Chigurupati, Sreekar and Chernyavsky, Julia and Reddy, Sai Prajwal and Venugopalan, Subhashini and Batra, Hunar and Park, Core Francisco and Tran, Hieu and Maximiano, Guilherme and Zhang, Genghan and Liang, Yizhuo and Shiyu, Hu and Xu, Rongwu and Pan, Rui and Suresh, Siddharth and Liu, Ziqi and Gulati, Samaksh and Zhang, Songyang and Turchin, Peter and Bartlett, Christopher W. and Scotese, Christopher R. and Cao, Phuong M. and Wu, Ben and Karwowski, Jacek and Scaramuzza, Davide and Nattanmai, Aakaash and McKellips, Gordon and Cheraku, Anish and Suhail, Asim and Luo, Ethan and Deng, Marvin and Luo, Jason and Zhang, Ashley and Jindel, Kavin and Paek, Jay and Halevy, Kasper and Baranov, Allen and Liu, Michael and Avadhanam, Advaith and Zhang, David and Cheng, Vincent and Ma, Brad and Fu, Evan and Do, Liam and Lass, Joshua and Yang, Hubert and Sunkari, Surya and Bharath, Vishruth and Ai, Violet and Leung, James and Agrawal, Rishit and Zhou, Alan and Chen, Kevin and Kalpathi, Tejas and Xu, Ziqi and Wang, Gavin and Xiao, Tyler and Maung, Erik and Lee, Sam and Yang, Ryan and Yue, Roy and Zhao, Ben and Yoon, Julia and Sun, Sunny and Singh, Aryan and Luo, Ethan and Peng, Clark and Osbey, Tyler and Wang, Taozhi and Echeazu, Daryl and Yang, Hubert and Wu, Timothy and Patel, Spandan and Kulkarni, Vidhi and Sundarapandiyan, Vijaykaarti and Zhang, Ashley and Le, Andrew and Nasim, Zafir and Yalam, Srikar and Kasamsetty, Ritesh and Samal, Soham and Yang, Hubert and Sun, David and Shah, Nihar and Saha, Abhijeet and Zhang, Alex and Nguyen, Leon and Nagumalli, Laasya and Wang, Kaixin and Zhou, Alan and Wu, Aidan and Luo, Jason and Telluri, Anwith and Yue, Summer and Wang, Alexandr and Hendrycks, Dan},
	month = sep,
	year = {2025},
	note = {arXiv:2501.14249 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/TZMZUBGF/Phan 等 - 2025 - Humanity's Last Exam.pdf:application/pdf},
}

@misc{chao_jailbreakbench_2024,
	title = {{JailbreakBench}: {An} {Open} {Robustness} {Benchmark} for {Jailbreaking} {Large} {Language} {Models}},
	shorttitle = {{JailbreakBench}},
	url = {http://arxiv.org/abs/2404.01318},
	doi = {10.48550/arXiv.2404.01318},
	abstract = {Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors—both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024)—which align with OpenAI’s usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Chao, Patrick and Debenedetti, Edoardo and Robey, Alexander and Andriushchenko, Maksym and Croce, Francesco and Sehwag, Vikash and Dobriban, Edgar and Flammarion, Nicolas and Pappas, George J. and Tramer, Florian and Hassani, Hamed and Wong, Eric},
	month = nov,
	year = {2024},
	note = {arXiv:2404.01318 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/PXVT4HDR/Chao 等 - 2024 - JailbreakBench An Open Robustness Benchmark for Jailbreaking Large Language Models.pdf:application/pdf},
}

@article{choi_language_nodate,
	title = {Language {Models} {Identify} {Ambiguities} and {Exploit} {Loopholes}},
	language = {en},
	author = {Choi, Jio and Bansal, Mohit and Stengel-Eskin, Elias},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/9I3MH5PN/Choi 等 - Language Models Identify Ambiguities and Exploit Loopholes.pdf:application/pdf},
}

@misc{li_livesecbench_2025,
	title = {{LiveSecBench}: {A} {Dynamic} and {Culturally}-{Relevant} {AI} {Safety} {Benchmark} for {LLMs} in {Chinese} {Context}},
	shorttitle = {{LiveSecBench}},
	url = {http://arxiv.org/abs/2511.02366},
	doi = {10.48550/arXiv.2511.02366},
	abstract = {In this work, we propose LiveSecBench, a dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through a dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing a landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Li, Yudong and Yang, Zhongliang and Chen, Kejiang and Wang, Wenxuan and Zhang, Tianxin and Wan, Sifang and Wang, Kecheng and Li, Haitian and Wang, Xu and Cheng, Lefan and Yang, Youdan and Chen, Baocheng and Liu, Ziyu and Sun, Yufei and Wu, Liyan and Wen, Wenya and Gu, Xingchi and Yang, Peiru},
	month = nov,
	year = {2025},
	note = {arXiv:2511.02366 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/QSX9F6L4/Li 等 - 2025 - LiveSecBench A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context.pdf:application/pdf},
}

@misc{ganguli_red_2022,
	title = {Red {Teaming} {Language} {Models} to {Reduce} {Harms}: {Methods}, {Scaling} {Behaviors}, and {Lessons} {Learned}},
	shorttitle = {Red {Teaming} {Language} {Models} to {Reduce} {Harms}},
	url = {http://arxiv.org/abs/2209.07858},
	doi = {10.48550/arXiv.2209.07858},
	abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We ﬁnd that the RLHF models are increasingly difﬁcult to red team as they scale, and we ﬁnd a ﬂat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and ﬁnd a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. Warning: this paper contains examples that may be offensive or upsetting.},
	language = {en},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and El-Showk, Sheer and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and Tran-Johnson, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
	month = nov,
	year = {2022},
	note = {arXiv:2209.07858 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/XCSEVCYW/Ganguli 等 - 2022 - Red Teaming Language Models to Reduce Harms Methods, Scaling Behaviors, and Lessons Learned.pdf:application/pdf},
}

@article{hou_wikicontradict_nodate,
	title = {{WikiContradict}: {A} {Benchmark} for {Evaluating} {LLMs} on {Real}-{World} {Knowledge} {Conflicts} from {Wikipedia}},
	abstract = {Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 highquality, human-annotated instances designed to assess the performance of LLMs in providing a complete perspective on conflicts from the retrieved documents, rather than choosing one answer over another, when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict at https://ibm.biz/wikicontradict.},
	language = {en},
	author = {Hou, Yufang and Pascale, Alessandra and Carnerero-Cano, Javier and Tchrakian, Tigran and Marinescu, Radu and Daly, Elizabeth and Padhi, Inkit and Sattigeri, Prasanna},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/9RJSIEQJ/Hou 等 - WikiContradict A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia.pdf:application/pdf},
}

@misc{bee_improving_2025,
	title = {Improving {Large} {Language} {Models}’ {Handling} of {Contradictions}: {Fostering} {Epistemic} {Humility}},
	shorttitle = {Improving {Large} {Language} {Models}’ {Handling} of {Contradictions}},
	url = {https://medium.com/@mbonsign/improving-large-language-models-handling-of-contradictions-fostering-epistemic-humility-629fca6abcf0},
	abstract = {Abstract},
	language = {en},
	urldate = {2025-11-13},
	journal = {Medium},
	author = {Bee, Micheal},
	month = may,
	year = {2025},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/AI6Z5R4A/improving-large-language-models-handling-of-contradictions-fostering-epistemic-humility-629fca6.html:text/html},
}

@misc{piatti_cooperate_2024,
	title = {Cooperate or {Collapse}: {Emergence} of {Sustainable} {Cooperation} in a {Society} of {LLM} {Agents}},
	shorttitle = {Cooperate or {Collapse}},
	url = {http://arxiv.org/abs/2404.16698},
	doi = {10.48550/arXiv.2404.16698},
	abstract = {As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54\%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage "Universalization"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Piatti, Giorgio and Jin, Zhijing and Kleiman-Weiner, Max and Schölkopf, Bernhard and Sachan, Mrinmaya and Mihalcea, Rada},
	month = dec,
	year = {2024},
	note = {arXiv:2404.16698 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/6VL2MCH7/Piatti 等 - 2024 - Cooperate or Collapse Emergence of Sustainable Cooperation in a Society of LLM Agents.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/F6FLIZQF/2404.html:text/html},
}

@misc{backmann_when_2025,
	title = {When {Ethics} and {Payoffs} {Diverge}: {LLM} {Agents} in {Morally} {Charged} {Social} {Dilemmas}},
	shorttitle = {When {Ethics} and {Payoffs} {Diverge}},
	url = {http://arxiv.org/abs/2505.19212},
	doi = {10.48550/arXiv.2505.19212},
	abstract = {Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner's dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent's "self-interest" may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim.},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Backmann, Steffen and Piedrahita, David Guzman and Tewolde, Emanuel and Mihalcea, Rada and Schölkopf, Bernhard and Jin, Zhijing},
	month = may,
	year = {2025},
	note = {arXiv:2505.19212 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/PEZEBLXN/Backmann 等 - 2025 - When Ethics and Payoffs Diverge LLM Agents in Morally Charged Social Dilemmas.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/8FGMDJ2S/2505.html:text/html},
}

@inproceedings{prabhakar_deciphering_2024,
	address = {Miami, Florida, USA},
	title = {Deciphering the {Factors} {Influencing} the {Efficacy} of {Chain}-of-{Thought}: {Probability}, {Memorization}, and {Noisy} {Reasoning}},
	shorttitle = {Deciphering the {Factors} {Influencing} the {Efficacy} of {Chain}-of-{Thought}},
	url = {https://aclanthology.org/2024.findings-emnlp.212/},
	doi = {10.18653/v1/2024.findings-emnlp.212},
	abstract = {Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step reasoning capabilities of Large Language Models (LLMs). However, debates persist about whether LLMs exhibit *abstract generalization* or rely on *shallow heuristics* when given CoT prompts. To understand the factors influencing CoT reasoning we provide a detailed case study of the symbolic reasoning task of decoding shift ciphers, where letters are shifted forward some number of steps in the alphabet. We analyze the pattern of results produced by three LLMs—GPT-4, Claude 3, and Llama 3.1—performing this task using CoT prompting. By focusing on a single relatively simple task, we are able to identify three factors that systematically affect CoT performance: the probability of the task's expected output (probability), what the model has implicitly learned during pre-training (memorization), and the number of intermediate operations involved in reasoning (noisy reasoning). We show that these factors can drastically influence task accuracy across all three LLMs; e.g., when tested with GPT-4, varying the output's probability of occurrence shifts accuracy from 26\% to 70\%. Overall, we conclude that CoT prompting performance reflects both memorization and a probabilistic version of genuine reasoning.},
	urldate = {2025-11-17},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Prabhakar, Akshara and Griffiths, Thomas L. and McCoy, R. Thomas},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {3710--3724},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/YJPW7W55/Prabhakar 等 - 2024 - Deciphering the Factors Influencing the Efficacy of Chain-of-Thought Probability, Memorization, and.pdf:application/pdf},
}

@misc{jain_livecodebench_2024,
	title = {{LiveCodeBench}: {Holistic} and {Contamination} {Free} {Evaluation} of {Large} {Language} {Models} for {Code}},
	shorttitle = {{LiveCodeBench}},
	url = {http://arxiv.org/abs/2403.07974},
	doi = {10.48550/arXiv.2403.07974},
	abstract = {Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model},
	language = {en-US},
	urldate = {2025-11-21},
	publisher = {arXiv},
	author = {Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
	month = jun,
	year = {2024},
	note = {arXiv:2403.07974 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/J9TMBCJ5/Jain 等 - 2024 - LiveCodeBench Holistic and Contamination Free Evaluation of Large Language Models for Code.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/8YHKG3PM/2403.html:text/html},
}

@misc{mialon_gaia_2023,
	title = {{GAIA}: a benchmark for {General} {AI} {Assistants}},
	shorttitle = {{GAIA}},
	url = {http://arxiv.org/abs/2311.12983},
	doi = {10.48550/arXiv.2311.12983},
	abstract = {We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92{\textbackslash}\% vs. 15{\textbackslash}\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.},
	urldate = {2025-11-21},
	publisher = {arXiv},
	author = {Mialon, Grégoire and Fourrier, Clémentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12983 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/WQ84IRW7/Mialon 等 - 2023 - GAIA a benchmark for General AI Assistants.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/AMEPMH5H/2311.html:text/html},
}

@misc{zhang_humaneval-v_2025,
	title = {{HumanEval}-{V}: {Benchmarking} {High}-{Level} {Visual} {Reasoning} with {Complex} {Diagrams} in {Coding} {Tasks}},
	shorttitle = {{HumanEval}-{V}},
	url = {http://arxiv.org/abs/2410.12381},
	doi = {10.48550/arXiv.2410.12381},
	abstract = {Understanding and reasoning over diagrams is a fundamental aspect of human intelligence. While Large Multimodal Models (LMMs) have demonstrated impressive capabilities across various tasks, existing benchmarks lack comprehensive evaluation of their diagram interpretation and reasoning abilities, particularly in coding contexts. We present HumanEval-V, a rigorous benchmark of human-annotated coding tasks that spans six task types and evaluates diverse visual reasoning capabilities. Each task features carefully crafted diagrams paired with function signatures and test cases, employing novel code generation tasks to thoroughly assess models' diagram comprehension. Through extensive experiments with 22 LMMs, we find that even top-performing models achieve modest success rates, with Claude 3.5 Sonnet reaching only 36.8\% pass@1, highlighting substantial room for improvement. Our analysis reveals that current LMMs struggle with spatial transformations, topological relationships, and dynamic patterns that humans find intuitive. These findings provide valuable insights for advancing LMMs' visual reasoning abilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.},
	urldate = {2025-11-21},
	publisher = {arXiv},
	author = {Zhang, Fengji and Wu, Linquan and Bai, Huiyu and Lin, Guancheng and Li, Xiao and Yu, Xiao and Wang, Yue and Chen, Bei and Keung, Jacky},
	month = feb,
	year = {2025},
	note = {arXiv:2410.12381 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/9XISIDEY/Zhang 等 - 2025 - HumanEval-V Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/SRZ3QBI5/2410.html:text/html},
}

@misc{jimenez_swe-bench_2024,
	title = {{SWE}-bench: {Can} {Language} {Models} {Resolve} {Real}-{World} {GitHub} {Issues}?},
	shorttitle = {{SWE}-bench},
	url = {http://arxiv.org/abs/2310.06770},
	doi = {10.48550/arXiv.2310.06770},
	abstract = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of \$2,294\$ software engineering problems drawn from real GitHub issues and corresponding pull requests across \$12\$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere \$1.96\$\% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.},
	urldate = {2025-11-21},
	publisher = {arXiv},
	author = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
	month = nov,
	year = {2024},
	note = {arXiv:2310.06770 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/5977DR6Z/Jimenez 等 - 2024 - SWE-bench Can Language Models Resolve Real-World GitHub Issues.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/2Y6X83I5/2310.html:text/html},
}

@misc{derouiche_agentic_2025,
	title = {Agentic {AI} {Frameworks}: {Architectures}, {Protocols}, and {Design} {Challenges}},
	shorttitle = {Agentic {AI} {Frameworks}},
	url = {http://arxiv.org/abs/2508.10146},
	doi = {10.48550/arXiv.2508.10146},
	abstract = {The emergence of Large Language Models (LLMs) has ushered in a transformative paradigm in artificial intelligence, Agentic AI, where intelligent agents exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent coordination. This paper provides a systematic review and comparative analysis of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural principles, communication mechanisms, memory management, safety guardrails, and alignment with service-oriented computing paradigms. Furthermore, we identify key limitations, emerging trends, and open challenges in the field. To address the issue of agent communication, we conduct an in-depth analysis of protocols such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network Protocol (ANP), and Agora. Our findings not only establish a foundational taxonomy for Agentic AI systems but also propose future research directions to enhance scalability, robustness, and interoperability. This work serves as a comprehensive reference for researchers and practitioners working to advance the next generation of autonomous AI systems.},
	urldate = {2025-11-21},
	publisher = {arXiv},
	author = {Derouiche, Hana and Brahmi, Zaki and Mazeni, Haithem},
	month = aug,
	year = {2025},
	note = {arXiv:2508.10146 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/IRIC3AYM/Derouiche 等 - 2025 - Agentic AI Frameworks Architectures, Protocols, and Design Challenges.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/BFR34856/2508.html:text/html},
}

@inproceedings{khatiri_simulation-based_2024,
	title = {Simulation-{Based} {Testing} of {Unmanned} {Aerial} {Vehicles} with {Aerialist}},
	url = {https://ieeexplore.ieee.org/document/10554828},
	doi = {10.1145/3639478.3640031},
	abstract = {Simulation-based testing is crucial for ensuring the safety and reliability of unmanned aerial vehicles (UAVs), especially as they become more autonomous and get increasingly used in commercial scenarios. The complexity and automated nature of UAVs requires sophisticated simulation environments for effectively testing their safety requirements. The primary challenges in setting up these environments pose significant barriers to the practical, widespread adoption of UAVs. We address this issue by introducing Aerialist (unmanned AERIAL vehicle teST bench), a novel UAV test bench, built on top of PX4 firmware, that facilitates or automates all the necessary steps of definition, generation, execution, and analysis of system-level UAV test cases in simulation environments. Moreover, it also supports parallel and scalable execution and analysis of test cases on Kubernetes clusters. This makes Aerialist a unique platform for research and development of test generation approaches for UAVs. To evaluate Aerialist's support for UAV developers in defining, generating, and executing UAV test cases, we implemented a search-based approach for generating realistic simulation-based test cases using real-world UAV flight logs. We confirmed its effectiveness in improving the realism and representativeness of simulation-based UAV tests.},
	urldate = {2025-11-21},
	booktitle = {2024 {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Khatiri, Sajad and Panichella, Sebastiano and Tonella, Paolo},
	month = apr,
	year = {2024},
	note = {ISSN: 2574-1934},
	keywords = {Autonomous aerial vehicles, Safety, Unmanned Aerial Vehicles, Simulation, Reliability, Testing, Complexity theory, Test pattern generators, Research and development, Test Generation},
	pages = {134--138},
	file = {全文:/Users/zhangyunshi/Zotero/storage/ISLXYQJ9/Khatiri 等 - 2024 - Simulation-Based Testing of Unmanned Aerial Vehicles with Aerialist.pdf:application/pdf},
}

@article{khatiri_sbft_2024,
	title = {{SBFT} {Tool} {Competition} 2024 - {CPS}-{UAV} {Test} {Case} {Generation} {Track}},
	abstract = {While simulation-based testing is critical for ensuring the safety of autonomous Unmanned Aerial Vehicles (UAVs), it has not been adequately researched yet. The UAV Testing Competition organized by the Search-Based and Fuzz Testing (SBFT) workshop is an initiative designed to inspire and encourage the software testing Community to direct their attention toward UAVs as a rapidly emerging and crucial domain. It provides a simple software platform and case study to facilitate their onboarding in the UAV domain and help them develop their first test generation tools for UAVs.},
	language = {en},
	author = {Khatiri, Sajad and Saurabh, Prasun and Zimmermann, Timothy and Munasinghe, Charith and Birchler, Christian and Panichella, Sebastiano},
	year = {2024},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/9HWC33PQ/Khatiri 等 - 2024 - SBFT Tool Competition 2024 - CPS-UAV Test Case Generation Track.pdf:application/pdf},
}

@article{qiu_quantifying_2025,
	title = {Quantifying the reasoning abilities of {LLMs} on clinical cases},
	volume = {16},
	copyright = {2025 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-025-64769-1},
	doi = {10.1038/s41467-025-64769-1},
	abstract = {Recent advances in reasoning-enhanced large language models (LLMs) show promise, yet their application in professional medicine, especially the evaluation of their reasoning process, remains underexplored. We present MedR-Bench, a benchmark of 1453 structured patient cases with reference reasoning derived from clinical case reports, spanning 13 body systems and 10 specialties across common and rare diseases. Our evaluation framework covers three stages of care: examination recommendation, diagnostic decision-making, and treatment planning. To assess reasoning quality, we develop the Reasoning Evaluator, an automated scorer of written reasoning along efficiency, factual accuracy, and completeness. We evaluate seven state-of-the-art reasoning LLMs. Here we show that current models exceed 85\% accuracy on simple diagnostic tasks when sufficient examination results are available, but performance drops on examination recommendation and treatment planning. Reasoning is generally factual, yet critical steps are often missing. Open-source models are closing the gap with proprietary systems, highlighting potential for more accessible, equitable clinical AI.},
	language = {en},
	number = {1},
	urldate = {2025-11-23},
	journal = {Nat Commun},
	author = {Qiu, Pengcheng and Wu, Chaoyi and Liu, Shuyu and Fan, Yanjie and Zhao, Weike and Chen, Zhuoxia and Gu, Hongfei and Peng, Chuanjin and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
	month = nov,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Health care},
	pages = {9799},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/TP5S2HSD/Qiu 等 - 2025 - Quantifying the reasoning abilities of LLMs on clinical cases.pdf:application/pdf},
}

@misc{chang_survey_2023,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.03109},
	doi = {10.48550/arXiv.2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = dec,
	year = {2023},
	note = {arXiv:2307.03109 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/AD2Q5XIQ/Chang 等 - 2023 - A Survey on Evaluation of Large Language Models.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/JD82SEJD/2307.html:text/html},
}

@article{du_objective_2025,
	title = {Objective {Metrics} for {Evaluating} {Large} {Language} {Models} {Using} {External} {Data} {Sources}},
	url = {http://arxiv.org/abs/2508.08277},
	doi = {10.5281/zenodo.15870300},
	abstract = {Evaluating the performance of Large Language Models (LLMs) is a critical yet challenging task, particularly when aiming to avoid subjective assessments. This paper proposes a framework for leveraging subjective metrics derived from the class textual materials across different semesters to assess LLM outputs across various tasks. By utilizing well-defined benchmarks, factual datasets, and structured evaluation pipelines, the approach ensures consistent, reproducible, and bias-minimized measurements. The framework emphasizes automation and transparency in scoring, reducing reliance on human interpretation while ensuring alignment with real-world applications. This method addresses the limitations of subjective evaluation methods, providing a scalable solution for performance assessment in educational, scientific, and other high-stakes domains.},
	urldate = {2025-11-23},
	author = {Du, Haoze and Li, Richard and Gehringer, Edward},
	month = jul,
	year = {2025},
	note = {arXiv:2508.08277 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/P4NFAG87/Du 等 - 2025 - Objective Metrics for Evaluating Large Language Models Using External Data Sources.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/AFQSJLCN/2508.html:text/html},
}

@misc{kumar_robustness_2025,
	title = {Robustness in {Large} {Language} {Models}: {A} {Survey} of {Mitigation} {Strategies} and {Evaluation} {Metrics}},
	shorttitle = {Robustness in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2505.18658},
	doi = {10.48550/arXiv.2505.18658},
	abstract = {Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Kumar, Pankaj and Mishra, Subhankar},
	month = nov,
	year = {2025},
	note = {arXiv:2505.18658 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/NV6Z9WNB/Kumar和Mishra - 2025 - Robustness in Large Language Models A Survey of Mitigation Strategies and Evaluation Metrics.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/CA3IMK6V/2505.html:text/html},
}

@misc{tomani_uncertainty-based_2024,
	title = {Uncertainty-{Based} {Abstention} in {LLMs} {Improves} {Safety} and {Reduces} {Hallucinations}},
	url = {http://arxiv.org/abs/2404.10960},
	doi = {10.48550/arXiv.2404.10960},
	abstract = {A major barrier towards the practical deployment of large language models (LLMs) is their lack of reliability. Three situations where this is particularly apparent are correctness, hallucinations when given unanswerable questions, and safety. In all three cases, models should ideally abstain from responding, much like humans, whose ability to understand uncertainty makes us refrain from answering questions we don't know. Inspired by analogous approaches in classification, this study explores the feasibility and efficacy of abstaining while uncertain in the context of LLMs within the domain of question-answering. We investigate two kinds of uncertainties, statistical uncertainty metrics and a distinct verbalized measure, termed as In-Dialogue Uncertainty (InDU). Using these uncertainty measures combined with models with and without Reinforcement Learning with Human Feedback (RLHF), we show that in all three situations, abstention based on the right kind of uncertainty measure can boost the reliability of LLMs. By sacrificing only a few highly uncertain samples we can improve correctness by 2\% to 8\%, avoid 50\% hallucinations via correctly identifying unanswerable questions and increase safety by 70\% up to 99\% with almost no additional computational overhead.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Tomani, Christian and Chaudhuri, Kamalika and Evtimov, Ivan and Cremers, Daniel and Ibrahim, Mark},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10960 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/2N4KZ3YA/Tomani 等 - 2024 - Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/T6Q53GZT/2404.html:text/html},
}

@misc{long_multi-expert_2024,
	title = {Multi-expert {Prompting} {Improves} {Reliability}, {Safety}, and {Usefulness} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2411.00492},
	doi = {10.48550/arXiv.2411.00492},
	abstract = {We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69\% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Long, Do Xuan and Yen, Duong Ngoc and Luu, Anh Tuan and Kawaguchi, Kenji and Kan, Min-Yen and Chen, Nancy F.},
	month = nov,
	year = {2024},
	note = {arXiv:2411.00492 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/PPKXIP6J/Long 等 - 2024 - Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/QKP2PEMQ/2411.html:text/html},
}

@misc{gokhale_logicguard_2025,
	title = {{LogicGuard}: {Improving} {Embodied} {LLM} agents through {Temporal} {Logic} based {Critics}},
	shorttitle = {{LogicGuard}},
	url = {http://arxiv.org/abs/2507.03293},
	doi = {10.48550/arXiv.2507.03293},
	abstract = {Large language models (LLMs) have shown promise in zero-shot and single step reasoning and decision making problems, but in long horizon sequential planning tasks, their errors compound, often leading to unreliable or inefficient behavior. We introduce LogicGuard, a modular actor-critic architecture in which an LLM actor is guided by a trajectory level LLM critic that communicates through Linear Temporal Logic (LTL). Our setup combines the reasoning strengths of language models with the guarantees of formal logic. The actor selects high-level actions from natural language observations, while the critic analyzes full trajectories and proposes new LTL constraints that shield the actor from future unsafe or inefficient behavior. LogicGuard supports both fixed safety rules and adaptive, learned constraints, and is model-agnostic: any LLM-based planner can serve as the actor, with LogicGuard acting as a logic-generating wrapper. We formalize planning as graph traversal under symbolic constraints, allowing LogicGuard to analyze failed or suboptimal trajectories and generate new temporal logic rules that improve future behavior. To demonstrate generality, we evaluate LogicGuard across two distinct settings: short-horizon general tasks and long-horizon specialist tasks. On the Behavior benchmark of 100 household tasks, LogicGuard increases task completion rates by 25\% over a baseline InnerMonologue planner. On the Minecraft diamond-mining task, which is long-horizon and requires multiple interdependent subgoals, LogicGuard improves both efficiency and safety compared to SayCan and InnerMonologue. These results show that enabling LLMs to supervise each other through temporal logic yields more reliable, efficient and safe decision-making for both embodied agents.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Gokhale, Anand and Srivastava, Vaibhav and Bullo, Francesco},
	month = sep,
	year = {2025},
	note = {arXiv:2507.03293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/88KMIKY4/Gokhale 等 - 2025 - LogicGuard Improving Embodied LLM agents through Temporal Logic based Critics.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/FJKJ2FZF/2507.html:text/html},
}

@misc{cai_llm-land_2025,
	title = {{LLM}-{Land}: {Large} {Language} {Models} for {Context}-{Aware} {Drone} {Landing}},
	shorttitle = {{LLM}-{Land}},
	url = {http://arxiv.org/abs/2505.06399},
	doi = {10.48550/arXiv.2505.06399},
	abstract = {Autonomous landing is essential for drones deployed in emergency deliveries, post-disaster response, and other large-scale missions. By enabling self-docking on charging platforms, it facilitates continuous operation and significantly extends mission endurance. However, traditional approaches often fall short in dynamic, unstructured environments due to limited semantic awareness and reliance on fixed, context-insensitive safety margins. To address these limitations, we propose a hybrid framework that integrates large language model (LLMs) with model predictive control (MPC). Our approach begins with a vision-language encoder (VLE) (e.g., BLIP), which transforms real-time images into concise textual scene descriptions. These descriptions are processed by a lightweight LLM (e.g., Qwen 2.5 1.5B or LLaMA 3.2 1B) equipped with retrieval-augmented generation (RAG) to classify scene elements and infer context-aware safety buffers, such as 3 meters for pedestrians and 5 meters for vehicles. The resulting semantic flags and unsafe regions are then fed into an MPC module, enabling real-time trajectory replanning that avoids collisions while maintaining high landing precision. We validate our framework in the ROS-Gazebo simulator, where it consistently outperforms conventional vision-based MPC baselines. Our results show a significant reduction in near-miss incidents with dynamic obstacles, while preserving accurate landings in cluttered environments.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Cai, Siwei and Wu, Yuwei and Zhou, Lifeng},
	month = may,
	year = {2025},
	note = {arXiv:2505.06399 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/XLMGSR3R/Cai 等 - 2025 - LLM-Land Large Language Models for Context-Aware Drone Landing.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/4Q6VP4N8/2505.html:text/html},
}

@misc{li_retrieval_2025,
	title = {Retrieval {Augmented} {Generation}-{Enhanced} {Distributed} {LLM} {Agents} for {Generalizable} {Traffic} {Signal} {Control} with {Emergency} {Vehicles}},
	url = {http://arxiv.org/abs/2510.26242},
	doi = {10.48550/arXiv.2510.26242},
	abstract = {With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00\%, queue length by 62.31\%, and emergency vehicle waiting time by 83.16\%, outperforming other state-of-the-art methods.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Li, Xinhang and Guo, Qing and Chen, Junyu and Guo, Zheng and Xu, Shengzhe and Li, Lei and Zhang, Lin},
	month = oct,
	year = {2025},
	note = {arXiv:2510.26242 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/DSVVH5JX/Li 等 - 2025 - Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Cont.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/DX4MAYEI/2510.html:text/html},
}

@inproceedings{ahuja_megaverse_2024,
	address = {Mexico City, Mexico},
	title = {{MEGAVERSE}: {Benchmarking} {Large} {Language} {Models} {Across} {Languages}, {Modalities}, {Models} and {Tasks}},
	shorttitle = {{MEGAVERSE}},
	url = {https://aclanthology.org/2024.naacl-long.143/},
	doi = {10.18653/v1/2024.naacl-long.143},
	abstract = {There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.},
	urldate = {2025-11-23},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ahuja, Sanchit and Aggarwal, Divyanshu and Gumma, Varun and Watts, Ishaan and Sathe, Ashutosh and Ochieng, Millicent and Hada, Rishav and Jain, Prachi and Ahmed, Mohamed and Bali, Kalika and Sitaram, Sunayana},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {2598--2637},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/7C3F5K5R/Ahuja 等 - 2024 - MEGAVERSE Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks.pdf:application/pdf},
}

@misc{minaee_large_2025,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = mar,
	year = {2025},
	note = {arXiv:2402.06196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/8WBZRRS3/Minaee 等 - 2025 - Large Language Models A Survey.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/XUW6ICPJ/2402.html:text/html},
}

@inproceedings{sakai_toward_2024,
	address = {Miami, Florida, US},
	title = {Toward the {Evaluation} of {Large} {Language} {Models} {Considering} {Score} {Variance} across {Instruction} {Templates}},
	url = {https://aclanthology.org/2024.blackboxnlp-1.31/},
	doi = {10.18653/v1/2024.blackboxnlp-1.31},
	abstract = {The natural language understanding (NLU) performance of large language models (LLMs) has been evaluated across various tasks and datasets. The existing evaluation methods, however, do not take into account the variance in scores due to differences in prompts, which leads to unfair evaluation and comparison of NLU performance. Moreover, evaluation designed for specific prompts is inappropriate for instruction tuning, which aims to perform well with any prompt. It is therefore necessary to find a way to measure NLU performance in a fair manner, considering score variance between different instruction templates. In this study, we provide English and Japanese cross-lingual datasets for evaluating the NLU performance of LLMs, which include multiple instruction templates for fair evaluation of each task, along with regular expressions to constrain the output format. Furthermore, we propose the Sharpe score as an evaluation metric that takes into account the variance in scores between templates. Comprehensive analysis of English and Japanese LLMs reveals that the high variance among templates has a significant impact on the fair evaluation of LLMs.},
	urldate = {2025-11-23},
	booktitle = {Proceedings of the 7th {BlackboxNLP} {Workshop}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Sakai, Yusuke and Nohejl, Adam and Hang, Jiangnan and Kamigaito, Hidetaka and Watanabe, Taro},
	editor = {Belinkov, Yonatan and Kim, Najoung and Jumelet, Jaap and Mohebbi, Hosein and Mueller, Aaron and Chen, Hanjie},
	month = nov,
	year = {2024},
	pages = {499--529},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/FIHGAPRS/Sakai 等 - 2024 - Toward the Evaluation of Large Language Models Considering Score Variance across Instruction Templat.pdf:application/pdf},
}

@article{feng_benchmarking_2025,
	title = {Benchmarking large and small {MLLMs}},
	volume = {36},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-025-01762-0},
	doi = {10.1007/s00138-025-01762-0},
	abstract = {Large multimodal language models (MLLMs) such as GPT-4V and GPT-4o have achieved remarkable advancements in understanding and generating multimodal content, showcasing superior quality and capabilities across diverse tasks. However, their deployment faces significant challenges, including slow inference, high computational cost, and impracticality for on-device applications. In contrast, the emergence of small MLLMs, exemplified by the LLava-series models and Phi-3-Vision, offers promising alternatives with faster inference, reduced deployment costs, and the ability to handle domain-specific scenarios. Despite their growing presence, the capability boundaries between large and small MLLMs remain underexplored. In this work, we conduct a systematic and comprehensive evaluation to benchmark both small and large MLLMs, spanning general capabilities such as object recognition, temporal reasoning, and multimodal comprehension, as well as real-world applications in domains like industry and automotive. Our evaluation reveals that small MLLMs can achieve comparable performance to large models in specific scenarios but lag significantly in complex tasks requiring deeper reasoning or nuanced understanding. Furthermore, we identify common failure cases in both small and large MLLMs, highlighting domains where even state-of-the-art models struggle. We hope our findings will guide the research community in pushing the quality boundaries of MLLMs, advancing their usability and effectiveness across diverse applications.},
	language = {en},
	number = {6},
	urldate = {2025-11-23},
	journal = {Machine Vision and Applications},
	author = {Feng, Xuelu and Li, Yunsheng and Chen, Dongdong and Gao, Mei and Liu, Mengchen and Yuan, Junsong and Qiao, Chunming},
	month = oct,
	year = {2025},
	keywords = {Application, Evaluation, General capabilities, Multi-modal large language model},
	pages = {137},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/B9UFBVMA/Feng 等 - 2025 - Benchmarking large and small MLLMs.pdf:application/pdf},
}

@article{li_palmbench_2025,
	title = {{PALMBENCH}: {A} {COMPREHENSIVE} {BENCHMARK} {OF} {COMPRESSED} {LARGE} {LANGUAGE} {MODELS} {ON} {MOBILE} {PLATFORMS}},
	language = {en},
	author = {Li, Yilong and Liu, Jingyu and Zhang, Hao and Narayanan, M Badri and Sharma, Utkarsh and Zhang, Shuai and Zeng, Yijing and Raghuram, Jayaram and Banerjee, Suman},
	year = {2025},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/RFA2B76W/Li 等 - 2025 - PALMBENCH A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS.pdf:application/pdf},
}

@misc{zhang_safetybench_2024,
	title = {{SafetyBench}: {Evaluating} the {Safety} of {Large} {Language} {Models}},
	shorttitle = {{SafetyBench}},
	url = {http://arxiv.org/abs/2309.07045},
	doi = {10.48550/arXiv.2309.07045},
	abstract = {With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at {\textbackslash}url\{https://github.com/thu-coai/SafetyBench\}\{https://github.com/thu-coai/SafetyBench\}. Submission entrance and leaderboard are available at {\textbackslash}url\{https://llmbench.ai/safety\}\{https://llmbench.ai/safety\}.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie},
	month = jun,
	year = {2024},
	note = {arXiv:2309.07045 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/WEV7XS4N/Zhang 等 - 2024 - SafetyBench Evaluating the Safety of Large Language Models.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/84VSD6WG/2309.html:text/html},
}

@misc{wang_novel_2025,
	title = {A {Novel} {Evaluation} {Benchmark} for {Medical} {LLMs}: {Illuminating} {Safety} and {Effectiveness} in {Clinical} {Domains}},
	shorttitle = {A {Novel} {Evaluation} {Benchmark} for {Medical} {LLMs}},
	url = {http://arxiv.org/abs/2507.23486},
	doi = {10.48550/arXiv.2507.23486},
	abstract = {Large language models (LLMs) hold promise in clinical decision support but face major challenges in safety evaluation and effectiveness validation. We developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a multidimensional framework built on clinical expert consensus, encompassing 30 criteria covering critical areas like critical illness recognition, guideline adherence, and medication safety, with weighted consequence measures. Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q\&A items aligned with these criteria, spanning 26 clinical departments to simulate real-world scenarios. Benchmark testing of six LLMs revealed moderate overall performance (average total score 57.2\%, safety 54.7\%, effectiveness 62.3\%), with a significant 13.3\% performance drop in high-risk scenarios (p {\textless} 0.0001). Domain-specific medical LLMs showed consistent performance advantages over general-purpose models, with relatively higher top scores in safety (0.912) and effectiveness (0.861). The findings of this study not only provide a standardized metric for evaluating the clinical application of medical LLMs, facilitating comparative analyses, risk exposure identification, and improvement directions across different scenarios, but also hold the potential to promote safer and more effective deployment of large language models in healthcare environments.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Wang, Shirui and Tang, Zhihui and Yang, Huaxia and Gong, Qiuhong and Gu, Tiantian and Ma, Hongyang and Wang, Yongxin and Sun, Wubin and Lian, Zeliang and Mao, Kehang and Jiang, Yinan and Huang, Zhicheng and Ma, Lingyun and Shen, Wenjie and Ji, Yajie and Tan, Yunhui and Wang, Chunbo and Gao, Yunlu and Ye, Qianling and Lin, Rui and Chen, Mingyu and Niu, Lijuan and Wang, Zhihao and Yu, Peng and Lang, Mengran and Liu, Yue and Zhang, Huimin and Shen, Haitao and Chen, Long and Zhao, Qiguang and Liu, Si-Xuan and Zhou, Lina and Gao, Hua and Ye, Dongqiang and Meng, Lingmin and Yu, Youtao and Liang, Naixin and Wu, Jianxiong},
	month = aug,
	year = {2025},
	note = {arXiv:2507.23486 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/R4ILZZ7F/Wang 等 - 2025 - A Novel Evaluation Benchmark for Medical LLMs Illuminating Safety and Effectiveness in Clinical Dom.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/AE5UDBE6/2507.html:text/html},
}

@misc{bedi_medhelm_2025,
	title = {{MedHELM}: {Holistic} {Evaluation} of {Large} {Language} {Models} for {Medical} {Tasks}},
	shorttitle = {{MedHELM}},
	url = {http://arxiv.org/abs/2505.23802},
	doi = {10.48550/arXiv.2505.23802},
	abstract = {While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66\% win-rate; o3-mini: 64\% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40\% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication \& Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration \& Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Bedi, Suhana and Cui, Hejie and Fuentes, Miguel and Unell, Alyssa and Wornow, Michael and Banda, Juan M. and Kotecha, Nikesh and Keyes, Timothy and Mai, Yifan and Oez, Mert and Qiu, Hao and Jain, Shrey and Schettini, Leonardo and Kashyap, Mehr and Fries, Jason Alan and Swaminathan, Akshay and Chung, Philip and Nateghi, Fateme and Aali, Asad and Nayak, Ashwin and Vedak, Shivam and Jain, Sneha S. and Patel, Birju and Fayanju, Oluseyi and Shah, Shreya and Goh, Ethan and Yao, Dong-han and Soetikno, Brian and Reis, Eduardo and Gatidis, Sergios and Divi, Vasu and Capasso, Robson and Saralkar, Rachna and Chiang, Chia-Chun and Jindal, Jenelle and Pham, Tho and Ghoddusi, Faraz and Lin, Steven and Chiou, Albert S. and Hong, Christy and Roy, Mohana and Gensheimer, Michael F. and Patel, Hinesh and Schulman, Kevin and Dash, Dev and Char, Danton and Downing, Lance and Grolleau, Francois and Black, Kameron and Mieso, Bethel and Zahedivash, Aydin and Yim, Wen-wai and Sharma, Harshita and Lee, Tony and Kirsch, Hannah and Lee, Jennifer and Ambers, Nerissa and Lugtu, Carlene and Sharma, Aditya and Mawji, Bilal and Alekseyev, Alex and Zhou, Vicky and Kakkar, Vikas and Helzer, Jarrod and Revri, Anurang and Bannett, Yair and Daneshjou, Roxana and Chen, Jonathan and Alsentzer, Emily and Morse, Keith and Ravi, Nirmal and Aghaeepour, Nima and Kennedy, Vanessa and Chaudhari, Akshay and Wang, Thomas and Koyejo, Sanmi and Lungren, Matthew P. and Horvitz, Eric and Liang, Percy and Pfeffer, Mike and Shah, Nigam H.},
	month = jun,
	year = {2025},
	note = {arXiv:2505.23802 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/6KI87859/Bedi 等 - 2025 - MedHELM Holistic Evaluation of Large Language Models for Medical Tasks.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/C9DVBERQ/2505.html:text/html},
}

@inproceedings{ouyang_climedbench_2024,
	address = {Miami, Florida, USA},
	title = {{CliMedBench}: {A} {Large}-{Scale} {Chinese} {Benchmark} for {Evaluating} {Medical} {Large} {Language} {Models} in {Clinical} {Scenarios}},
	shorttitle = {{CliMedBench}},
	url = {https://aclanthology.org/2024.emnlp-main.480/},
	doi = {10.18653/v1/2024.emnlp-main.480},
	abstract = {With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.},
	urldate = {2025-11-23},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ouyang, Zetian and Qiu, Yishuai and Wang, Linlin and De Melo, Gerard and Zhang, Ya and Wang, Yanfeng and He, Liang},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {8428--8438},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/B7QB3392/Ouyang 等 - 2024 - CliMedBench A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinica.pdf:application/pdf},
}

@inproceedings{tagliabue_real_2024,
	title = {{REAL}: {Resilience} and {Adaptation} using {Large} {Language} {Models} on {Autonomous} {Aerial} {Robots}},
	shorttitle = {{REAL}},
	url = {https://ieeexplore.ieee.org/document/10885890/},
	doi = {10.1109/CDC56724.2024.10885890},
	abstract = {Large Language Models (LLMs) pre-trained on internet-scale datasets have shown impressive capabilities in code understanding, synthesis and in processing extended sequences of symbols, often presented in natural language. This work aims to explore new opportunities in long-term reasoning, natural language comprehension, and the available prior knowledge of LLMs for increased resilience and adaptation in autonomous mobile robots. We introduce REAL, an approach for REsilience and Adaptation using LLMs. REAL interfaces LLMs with the mission planning and control framework of an autonomous robot. The LLM employed by REAL provides (i) a source of prior knowledge to increase resilience for challenging scenarios that the system has not been explicitly designed for; (ii) a way to interpret natural language and other log/diagnostic information available in the autonomy stack, for mission planning; (iii) a way to adapt the control inputs using minimal user-provided prior knowledge about the robot. We integrate REAL in the autonomy stack of a real multirotor, querying onboard an offboard LLM at about 1.0-0.1 {\textbackslash}mathrm Hz as part of the robot’s mission planning and control feedback loops. We provide a demonstration of capabilities by showcasing in real-world experiments the ability of the LLM to reduce the position tracking errors of a multirotor, and decision-making to avoid potentially dangerous scenarios (e.g., robot oscillates) that are not explicitly accounted for in the initial prompt design.},
	urldate = {2025-11-24},
	booktitle = {2024 {IEEE} 63rd {Conference} on {Decision} and {Control} ({CDC})},
	author = {Tagliabue, Andrea and Kondo, Kota and Zhao, Tong and Peterson, Mason and Tewari, Claudius T. and How, Jonathan P.},
	month = dec,
	year = {2024},
	note = {ISSN: 2576-2370},
	keywords = {Autonomous aerial vehicles, Autonomous robots, Cognition, Large language models, Mobile robots, Natural language processing, Planning, Resilience, Symbols, System analysis and design},
	pages = {1539--1546},
	file = {已提交版本:/Users/zhangyunshi/Zotero/storage/WIFTJHCB/Tagliabue 等 - 2024 - REAL Resilience and Adaptation using Large Language Models on Autonomous Aerial Robots.pdf:application/pdf},
}

@misc{bonucchi_bridging_2024,
	title = {Bridging {Intelligence}: {The} {Next} {Evolution} in {AI} with {Hybrid} {LLM} and {Rule}-{Based} {Systems}},
	shorttitle = {Bridging {Intelligence}},
	url = {https://medium.com/@ceciliabonucchi/bridging-intelligence-the-next-evolution-in-ai-with-hybrid-llm-and-rule-based-systems-db0d89998c6d},
	abstract = {Artificial intelligence is rapidly evolving, yet few developments hold as much transformative potential as hybrid models combining…},
	language = {en},
	urldate = {2025-11-24},
	journal = {Medium},
	author = {Bonucchi, Cecilia},
	month = oct,
	year = {2024},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/YMVC89W5/bridging-intelligence-the-next-evolution-in-ai-with-hybrid-llm-and-rule-based-systems-db0d89998.html:text/html},
}

@incollection{liu_llmstp_2025,
	address = {Singapore},
	title = {{LLMSTP}: {Empowering} {Swarm} {Task} {Planning} with {Large} {Language} {Models}},
	volume = {1374},
	isbn = {978-981-96-3551-1 978-981-96-3552-8},
	shorttitle = {{LLMSTP}},
	url = {https://link.springer.com/10.1007/978-981-96-3552-8_47},
	language = {en},
	urldate = {2025-11-24},
	booktitle = {Proceedings of 4th 2024 {International} {Conference} on {Autonomous} {Unmanned} {Systems} (4th {ICAUS} 2024)},
	publisher = {Springer Nature Singapore},
	author = {Yu, Hongbo and Wang, Chang and Wu, Lizhen and Liu, Yuhao and Niu, Yifeng},
	editor = {Liu, Lianqing and Niu, Yifeng and Fu, Wenxing and Qu, Yi},
	year = {2025},
	doi = {10.1007/978-981-96-3552-8_47},
	note = {Series Title: Lecture Notes in Electrical Engineering},
	pages = {500--510},
}

@article{tu_multi-modal_2025,
	title = {Multi-modal {Traffic} {Scenario} {Generation} for {Autonomous} {Driving} {System} {Testing}},
	volume = {2},
	issn = {2994-970X},
	url = {https://dl.acm.org/doi/10.1145/3729348},
	doi = {10.1145/3729348},
	abstract = {Autonomous driving systems (ADS) require extensive testing and validation before deployment. However, it is tedious and time-consuming to construct traffic scenarios for ADS testing. In this paper, we propose TrafficComposer, a multi-modal traffic scenario construction approach for ADS testing. TrafficComposer takes as input a natural language (NL) description of a desired traffic scenario and a complementary traffic scene image. Then, it generates the corresponding traffic scenario in a simulator, such as CARLA and LGSVL. Specifically, TrafficComposer integrates high-level dynamic information about the traffic scenario from the NL description and intricate details about the surrounding vehicles, pedestrians, and the road network from the image. The information from the two modalities is complementary to each other and helps generate high-quality traffic scenarios for ADS testing. On a benchmark of 120 traffic scenarios, TrafficComposer achieves 97.0\% accuracy, outperforming the best-performing baseline by 7.3\%. Both direct testing and fuzz testing experiments on six ADSs prove the bug detection capabilities of the traffic scenarios generated by TrafficComposer. These scenarios can directly discover 37 bugs and help two fuzzing methods find 33\%–124\% more bugs serving as initial seeds.},
	language = {en},
	number = {FSE},
	urldate = {2025-11-25},
	journal = {Proc. ACM Softw. Eng.},
	author = {Tu, Zhi and Niu, Liangkun and Fan, Wei and Zhang, Tianyi},
	month = jun,
	year = {2025},
	pages = {1733--1756},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/6Q5DDWU8/Tu 等 - 2025 - Multi-modal Traffic Scenario Generation for Autonomous Driving System Testing.pdf:application/pdf},
}

@misc{qiu_locobench-agent_2025,
	title = {{LoCoBench}-{Agent}: {An} {Interactive} {Benchmark} for {LLM} {Agents} in {Long}-{Context} {Software} {Engineering}},
	shorttitle = {{LoCoBench}-{Agent}},
	url = {http://arxiv.org/abs/2511.13998},
	doi = {10.48550/arXiv.2511.13998},
	abstract = {As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench{\textasciitilde}{\textbackslash}cite\{qiu2025locobench\} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce {\textbackslash}textbf\{LoCoBench-Agent\}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Qiu, Jielin and Liu, Zuxin and Liu, Zhiwei and Murthy, Rithesh and Zhang, Jianguo and Chen, Haolin and Wang, Shiyu and Zhu, Ming and Yang, Liangwei and Tan, Juntao and Ram, Roshan and Prabhakar, Akshara and Awalgaonkar, Tulika and Chen, Zixiang and Cen, Zhepeng and Qian, Cheng and Heinecke, Shelby and Yao, Weiran and Savarese, Silvio and Xiong, Caiming and Wang, Huan},
	month = nov,
	year = {2025},
	note = {arXiv:2511.13998 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{cai_text2scenario_2025,
	title = {{Text2Scenario}: {Text}-{Driven} {Scenario} {Generation} for {Autonomous} {Driving} {Test}},
	shorttitle = {{Text2Scenario}},
	url = {http://arxiv.org/abs/2503.02911},
	doi = {10.48550/arXiv.2503.02911},
	abstract = {Autonomous driving (AD) testing constitutes a critical methodology for assessing performance benchmarks prior to product deployment. The creation of segmented scenarios within a simulated environment is acknowledged as a robust and effective strategy; however, the process of tailoring these scenarios often necessitates laborious and time-consuming manual efforts, thereby hindering the development and implementation of AD technologies. In response to this challenge, we introduce Text2Scenario, a framework that leverages a Large Language Model (LLM) to autonomously generate simulation test scenarios that closely align with user specifications, derived from their natural language inputs. Specifically, an LLM, equipped with a meticulously engineered input prompt scheme functions as a text parser for test scenario descriptions, extracting from a hierarchically organized scenario repository the components that most accurately reflect the user's preferences. Subsequently, by exploiting the precedence of scenario components, the process involves sequentially matching and linking scenario representations within a Domain Specific Language corpus, ultimately fabricating executable test scenarios. The experimental results demonstrate that such prompt engineering can meticulously extract the nuanced details of scenario elements embedded within various descriptive formats, with the majority of generated scenarios aligning closely with the user's initial expectations, allowing for the efficient and precise evaluation of diverse AD stacks void of the labor-intensive need for manual scenario configuration. Project page: https://caixxuan.github.io/Text2Scenario.GitHub.io.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Cai, Xuan and Bai, Xuesong and Cui, Zhiyong and Xie, Danmu and Fu, Daocheng and Yu, Haiyang and Ren, Yilong},
	month = mar,
	year = {2025},
	note = {arXiv:2503.02911 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@article{finkeldei_scenario_2025,
	title = {Scenario {Factory} 2.0: {Scenario}-{Based} {Testing} of {Automated} {Vehicles} with {CommonRoad}},
	volume = {8},
	issn = {2096-4250, 2522-8765},
	shorttitle = {Scenario {Factory} 2.0},
	url = {https://link.springer.com/10.1007/s42154-025-00360-0},
	doi = {10.1007/s42154-025-00360-0},
	abstract = {Scenario-based testing plays a pivotal role in the development and validation of automated vehicles. Its main challenge is to efficiently generate realistic and relevant test scenarios to identify and analyze shortcomings of automated driving systems. The Scenario Factory 2.0 unifies several scenario generation techniques from the open-source CommonRoad framework and introduces simulation modes for coupling with the traffic simulators OpenTrafficSim and SUMO. The simulation modes enable generating scenarios with a tunable similarity to existing ones. As existing approaches, the Scenario Factory 2.0 integrates scenario generation from formal specifications and falsification techniques. Scenario Factory 2.0 has a modular structure and the modules can be easily rearranged for creating required scenarios. We evaluate the effectiveness of the novel simulation modes for various traffic scenarios and demonstrate the scenario generation with Scenario Factory 2.0 in a use case. The open-source code is provided at https://commonroad.in.tum.de/tools/scenario-factory.},
	language = {en},
	number = {2},
	urldate = {2025-11-25},
	journal = {Automot. Innov.},
	author = {Finkeldei, Florian and Thees, Christoph and Weghorn, Jan-Niklas and Althoff, Matthias},
	month = may,
	year = {2025},
	pages = {207--220},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/93Y3VJJM/Finkeldei 等 - 2025 - Scenario Factory 2.0 Scenario-Based Testing of Automated Vehicles with CommonRoad.pdf:application/pdf},
}

@article{song_critical_2023,
	title = {Critical scenario identification for realistic testing of autonomous driving systems},
	volume = {31},
	issn = {0963-9314, 1573-1367},
	url = {https://link.springer.com/10.1007/s11219-022-09604-2},
	doi = {10.1007/s11219-022-09604-2},
	abstract = {Abstract
            Autonomous driving has become an important research area for road traffic, whereas testing of autonomous driving systems to ensure a safe and reliable operation remains an open challenge. Substantial real-world testing or massive driving data collection does not scale since the potential test scenarios in real-world traffic are infinite, and covering large shares of them in the test is impractical. Thus, critical ones have to be prioritized. We have developed an approach for critical test scenario identification and in this study, we implement the approach and validate it on two real autonomous driving systems from industry by integrating it into their tool-chain. Our main contribution in this work is the demonstration and validation of our approach for critical scenario identification for testing real autonomous driving systems.},
	language = {en},
	number = {2},
	urldate = {2025-11-25},
	journal = {Software Qual J},
	author = {Song, Qunying and Tan, Kaige and Runeson, Per and Persson, Stefan},
	month = jun,
	year = {2023},
	pages = {441--469},
	file = {全文:/Users/zhangyunshi/Zotero/storage/RXQ5IYMJ/Song 等 - 2023 - Critical scenario identification for realistic testing of autonomous driving systems.pdf:application/pdf},
}

@article{lee_adaptive_2020,
	title = {Adaptive {Stress} {Testing}: {Finding} {Likely} {Failure} {Events} with {Reinforcement} {Learning}},
	volume = {69},
	issn = {1076-9757},
	shorttitle = {Adaptive {Stress} {Testing}},
	url = {https://jair.org/index.php/jair/article/view/12190},
	doi = {10.1613/jair.1.12190},
	abstract = {Finding the most likely path to a set of failure states is important to the analysis of safetycritical systems that operate over a sequence of time steps, such as aircraft collision avoidance systems and autonomous cars. In many applications such as autonomous driving, failures cannot be completely eliminated due to the complex stochastic environment in which the system operates. As a result, safety validation is not only concerned about whether a failure can occur, but also discovering which failures are most likely to occur. This article presents adaptive stress testing (AST), a framework for ﬁnding the most likely path to a failure event in simulation. We consider a general black box setting for partially observable and continuous-valued systems operating in an environment with stochastic disturbances. We formulate the problem as a Markov decision process and use reinforcement learning to optimize it. The approach is simulation-based and does not require internal knowledge of the system, making it suitable for black-box testing of large systems. We present diﬀerent formulations depending on whether the state is fully observable or partially observable. In the latter case, we present a modiﬁed Monte Carlo tree search algorithm that only requires access to the pseudorandom number generator of the simulator to overcome partial observability. We also present an extension of the framework, called diﬀerential adaptive stress testing (DAST), that can ﬁnd failures that occur in one system but not in another. This type of diﬀerential analysis is useful in applications such as regression testing, where we are concerned with ﬁnding areas of relative weakness compared to a baseline. We demonstrate the eﬀectiveness of the approach on an aircraft collision avoidance application, where a prototype aircraft collision avoidance system is stress tested to ﬁnd the most likely scenarios of near mid-air collision.},
	language = {en},
	urldate = {2025-11-25},
	journal = {jair},
	author = {Lee, Ritchie and Mengshoel, Ole J. and Saksena, Anshu and Gardner, Ryan W. and Genin, Daniel and Silbermann, Joshua and Owen, Michael and Kochenderfer, Mykel J.},
	month = dec,
	year = {2020},
	pages = {1165--1201},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/JRL8AHCA/Lee 等 - 2020 - Adaptive Stress Testing Finding Likely Failure Events with Reinforcement Learning.pdf:application/pdf},
}

@misc{trinh_novel_2024,
	title = {A novel framework for adaptive stress testing of autonomous vehicles in multi-lane roads},
	url = {http://arxiv.org/abs/2402.11813},
	doi = {10.48550/arXiv.2402.11813},
	abstract = {Stress testing is an approach for evaluating the reliability of systems under extreme conditions which help reveal vulnerable scenarios that standard testing may overlook. Identifying such scenarios is of great importance in autonomous vehicles (AV) and other safety-critical systems. Since failure events are rare, naive random search approaches require a large number of vehicle operation hours to identify potential system failures. Adaptive Stress Testing (AST) is a method addressing this constraint by effectively exploring the failure trajectories of AV using a Markov decision process and employs reinforcement learning techniques to identify driving scenarios with high probability of failures. However, existing AST frameworks are able to handle only simple scenarios, such as one vehicle moving longitudinally on a single lane road which is not realistic and has a limited applicability. In this paper, we propose a novel AST framework to systematically explore corner cases of intelligent driving models that can result in safety concerns involving both longitudinal and lateral vehicle's movements. Specially, we develop a new reward function for Deep Reinforcement Learning to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and the trajectory of other vehicles on the multi-lane roads. To demonstrate the effectiveness of our framework, we tested it with a complex driving model vehicle that can be controlled in both longitudinal and lateral directions. Quantitative and qualitative analyses of our experimental results demonstrate that our framework outperforms the state-of-the-art AST scheme in identifying corner cases with complex driving maneuvers.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Trinh, Linh and Luu, Quang-Hung and Nguyen, Thai M. and Vu, Hai L.},
	month = sep,
	year = {2024},
	note = {arXiv:2402.11813 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/93AIWVX5/Trinh 等 - 2024 - A novel framework for adaptive stress testing of autonomous vehicles in multi-lane roads.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/4482G3SZ/2402.html:text/html},
}

@misc{noauthor_stress_nodate,
	title = {Stress {Testing} for {Autonomous} {Systems}},
	url = {https://www.nrec.ri.cmu.edu/solutions/defense/stress-testing-for-autonomous-systems/},
	abstract = {Solutions {\textgreater} Defense {\textgreater} Stress Testing for Autonomous Systems Stress Testing for Autonomous Systems NREC developed the Stress Testing for Autonomy Architectures (ASTAA) tool to find safety problems in autonomy systems that are unlikely to be discovered by other types of tests or extended field testing. The ASTAA tool can be used to test both entire systems and individual components at […]},
	language = {en-US},
	urldate = {2025-11-25},
	journal = {NREC},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/IR2WWT6I/stress-testing-for-autonomous-systems.html:text/html},
}

@article{laurent_parameter_2023,
	title = {Parameter {Coverage} for {Testing} of {Autonomous} {Driving} {Systems} under {Uncertainty}},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3550270},
	doi = {10.1145/3550270},
	abstract = {Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS’s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS’s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.},
	language = {en},
	number = {3},
	urldate = {2025-11-25},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Laurent, Thomas and Klikovits, Stefan and Arcaini, Paolo and Ishikawa, Fuyuki and Ventresque, Anthony},
	month = jul,
	year = {2023},
	pages = {1--31},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/JIUQE9YU/Laurent 等 - 2023 - Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty.pdf:application/pdf},
}

@article{mullins_adaptive_2018,
	title = {Adaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles},
	volume = {137},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121217302546},
	doi = {10.1016/j.jss.2017.10.031},
	language = {en},
	urldate = {2025-11-25},
	journal = {Journal of Systems and Software},
	author = {Mullins, Galen E. and Stankiewicz, Paul G. and Hawthorne, R. Chad and Gupta, Satyandra K.},
	month = mar,
	year = {2018},
	pages = {197--215},
}

@inproceedings{fu_vulnerabilities_2024,
	address = {Bangkok, Thailand},
	title = {Vulnerabilities of {Large} {Language} {Models} to {Adversarial} {Attacks}},
	url = {https://aclanthology.org/2024.acl-tutorials.5/},
	doi = {10.18653/v1/2024.acl-tutorials.5},
	abstract = {This tutorial serves as a comprehensive guide on the vulnerabilities of Large Language Models (LLMs) to adversarial attacks, an interdisciplinary field that blends perspectives from Natural Language Processing (NLP) and Cybersecurity. As LLMs become more complex and integrated into various systems, understanding their security attributes is crucial. However, current research indicates that even safety-aligned models are not impervious to adversarial attacks that can result in incorrect or harmful outputs. The tutorial first lays the foundation by explaining safety-aligned LLMs and concepts in cybersecurity. It then categorizes existing research based on different types of learning architectures and attack methods. We highlight the existing vulnerabilities of unimodal LLMs, multi-modal LLMs, and systems that integrate LLMs, focusing on adversarial attacks designed to exploit weaknesses and mislead AI systems. Finally, the tutorial delves into the potential causes of these vulnerabilities and discusses potential defense mechanisms.},
	urldate = {2025-11-25},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 5: {Tutorial} {Abstracts})},
	publisher = {Association for Computational Linguistics},
	author = {Fu, Yu and Shayegan, Erfan and Abdullah, Md. Mamun Al and Zaree, Pedram and Abu-Ghazaleh, Nael and Dong, Yue},
	editor = {Chiruzzo, Luis and Lee, Hung-yi and Ribeiro, Leonardo},
	month = aug,
	year = {2024},
	pages = {8--9},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/GW2XWI9D/Fu 等 - 2024 - Vulnerabilities of Large Language Models to Adversarial Attacks.pdf:application/pdf},
}

@misc{dubrovsky_selective_2025,
	title = {Selective {Adversarial} {Attacks} on {LLM} {Benchmarks}},
	url = {http://arxiv.org/abs/2510.13570},
	doi = {10.48550/arXiv.2510.13570},
	abstract = {Benchmarking outcomes increasingly govern trust, selection, and deployment of LLMs, yet these evaluations remain vulnerable to semantically equivalent adversarial perturbations. Prior work on adversarial robustness in NLP has emphasized text attacks that affect many models equally, leaving open the question of whether it is possible to selectively degrade or enhance performance while minimally affecting other models. We formalize this problem and study selective adversarial attacks on MMLU - a widely used benchmark designed to measure a language model's broad general knowledge and reasoning ability across different subjects. Using canonical attacks integrated into TextAttack framework, we introduce a protocol for selectivity assessment, develop a custom constraint to increase selectivity of attacks and propose a surrogate-LLM pipeline that generates selective perturbations. Empirically, we find that selective adversarial attacks exist and can materially alter relative rankings, challenging the fairness, reproducibility, and transparency of leaderboard-driven evaluation. Our results motivate perturbation-aware reporting and robustness diagnostics for LLM evaluation and demonstrate that even subtle edits can shift comparative judgments.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Dubrovsky, Ivan and Orlova, Anastasia and Iov, Illarion and Gubina, Nina and Gureeva, Irena and Zaytsev, Alexey},
	month = oct,
	year = {2025},
	note = {arXiv:2510.13570 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/XWEWFI97/Dubrovsky 等 - 2025 - Selective Adversarial Attacks on LLM Benchmarks.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/DLIHDXRE/2510.html:text/html},
}

@misc{yang_exploiting_2025,
	title = {Exploiting {Synergistic} {Cognitive} {Biases} to {Bypass} {Safety} in {LLMs}},
	url = {http://arxiv.org/abs/2507.22564},
	doi = {10.48550/arXiv.2507.22564},
	abstract = {Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1\% vs. 31.6\%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Yang, Xikang and Zhou, Biyu and Tang, Xuehai and Han, Jizhong and Hu, Songlin},
	month = nov,
	year = {2025},
	note = {arXiv:2507.22564 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{banerjee_vulnerability_2024,
	title = {The {Vulnerability} of {Language} {Model} {Benchmarks}: {Do} {They} {Accurately} {Reflect} {True} {LLM} {Performance}?},
	shorttitle = {The {Vulnerability} of {Language} {Model} {Benchmarks}},
	url = {http://arxiv.org/abs/2412.03597},
	doi = {10.48550/arXiv.2412.03597},
	abstract = {The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and MMLU. These vulnerabilities manifest through benchmark exploitation, dataset contamination, and evaluation bias, creating a false perception of progress in language understanding capabilities. Through extensive review of contemporary evaluation approaches, we identify significant limitations in static benchmark designs, human evaluation protocols, and LLM-as-judge frameworks, all of which compromise the reliability of current performance assessments. As LLM capabilities evolve and existing benchmarks become redundant, we lay the groundwork for new evaluation methods that resist manipulation, minimize data contamination, and assess domain-specific tasks. This requires frameworks that are adapted dynamically, addressing current limitations and providing a more accurate reflection of LLM performance.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Banerjee, Sourav and Agarwal, Ayushi and Singh, Eishkaran},
	month = dec,
	year = {2024},
	note = {arXiv:2412.03597 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{he_g-retriever_2024,
	title = {G-{Retriever}: {Retrieval}-{Augmented} {Generation} for {Textual} {Graph} {Understanding} and {Question} {Answering}},
	shorttitle = {G-{Retriever}},
	url = {http://arxiv.org/abs/2402.07630},
	doi = {10.48550/arXiv.2402.07630},
	abstract = {Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.{\textasciitilde}{\textbackslash}footnote\{Our codes and datasets are available at: {\textbackslash}url\{https://github.com/XiaoxinHe/G-Retriever\}\}},
	language = {en-US},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V. and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
	month = may,
	year = {2024},
	note = {arXiv:2402.07630 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/RQESSA7U/2402.html:text/html},
}

@misc{chung_graphcompliance_2025,
	title = {{GraphCompliance}: {Aligning} {Policy} and {Context} {Graphs} for {LLM}-{Based} {Regulatory} {Compliance}},
	shorttitle = {{GraphCompliance}},
	url = {http://arxiv.org/abs/2510.26309},
	doi = {10.48550/arXiv.2510.26309},
	abstract = {Compliance at web scale poses practical challenges: each request may require a regulatory assessment. Regulatory texts (e.g., the General Data Protection Regulation, GDPR) are cross-referential and normative, while runtime contexts are expressed in unstructured natural language. This setting motivates us to align semantic information in unstructured text with the structured, normative elements of regulations. To this end, we introduce GraphCompliance, a framework that represents regulatory texts as a Policy Graph and runtime contexts as a Context Graph, and aligns them. In this formulation, the policy graph encodes normative structure and cross-references, whereas the context graph formalizes events as subject-action-object (SAO) and entity-relation triples. This alignment anchors the reasoning of a judge large language model (LLM) in structured information and helps reduce the burden of regulatory interpretation and event parsing, enabling a focus on the core reasoning step. In experiments on 300 GDPR-derived real-world scenarios spanning five evaluation tasks, GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than LLM-only and RAG baselines, with fewer under- and over-predictions, resulting in higher recall and lower false positive rates. Ablation studies indicate contributions from each graph component, suggesting that structured representations and a judge LLM are complementary for normative reasoning.},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Chung, Jiseong and Ko, Ronny and Yoo, Wonchul and Onizuka, Makoto and Kim, Sungmok and Kim, Tae-Wan and Shin, Won-Yong},
	month = oct,
	year = {2025},
	note = {arXiv:2510.26309 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/282LFL6P/2510.html:text/html},
}

@misc{luo_gfm-rag_2025,
	title = {{GFM}-{RAG}: {Graph} {Foundation} {Model} for {Retrieval} {Augmented} {Generation}},
	shorttitle = {{GFM}-{RAG}},
	url = {http://arxiv.org/abs/2502.01113},
	doi = {10.48550/arXiv.2502.01113},
	abstract = {Retrieval-augmented generation (RAG) has proven effective in integrating knowledge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their performance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effective and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented generation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement.},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Luo, Linhao and Zhao, Zicheng and Haffari, Gholamreza and Phung, Dinh and Gong, Chen and Pan, Shirui},
	month = oct,
	year = {2025},
	note = {arXiv:2502.01113 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/FYJKYEQ3/2502.html:text/html},
}

@article{yu_rankrag_nodate,
	title = {{RankRAG}: {Unifying} {Context} {Ranking} with {Retrieval}-{Augmented} {Generation} in {LLMs}},
	abstract = {Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.},
	language = {en},
	author = {Yu, Yue and Ping, Wei and Liu, Zihan and Wang, Boxin and You, Jiaxuan},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/MECSPLRN/Yu 等 - RankRAG Unifying Context Ranking with Retrieval-Augmented Generation in LLMs.pdf:application/pdf},
}

@misc{sharma_retrieval-augmented_2025,
	title = {Retrieval-{Augmented} {Generation}: {A} {Comprehensive} {Survey} of {Architectures}, {Enhancements}, and {Robustness} {Frontiers}},
	shorttitle = {Retrieval-{Augmented} {Generation}},
	url = {http://arxiv.org/abs/2506.00054},
	doi = {10.48550/arXiv.2506.00054},
	abstract = {Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance large language models (LLMs) by conditioning generation on external evidence retrieved at inference time. While RAG addresses critical limitations of parametric knowledge storage-such as factual inconsistency and domain inflexibility-it introduces new challenges in retrieval quality, grounding fidelity, pipeline efficiency, and robustness against noisy or adversarial inputs. This survey provides a comprehensive synthesis of recent advances in RAG systems, offering a taxonomy that categorizes architectures into retriever-centric, generator-centric, hybrid, and robustness-oriented designs. We systematically analyze enhancements across retrieval optimization, context filtering, decoding control, and efficiency improvements, supported by comparative performance analyses on short-form and multi-hop question answering tasks. Furthermore, we review state-of-the-art evaluation frameworks and benchmarks, highlighting trends in retrieval-aware evaluation, robustness testing, and federated retrieval settings. Our analysis reveals recurring trade-offs between retrieval precision and generation flexibility, efficiency and faithfulness, and modularity and coordination. We conclude by identifying open challenges and future research directions, including adaptive retrieval architectures, real-time retrieval integration, structured reasoning over multi-hop evidence, and privacy-preserving retrieval mechanisms. This survey aims to consolidate current knowledge in RAG research and serve as a foundation for the next generation of retrieval-augmented language modeling systems.},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Sharma, Chaitanya},
	month = may,
	year = {2025},
	note = {arXiv:2506.00054 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/8L68JLLL/Sharma - 2025 - Retrieval-Augmented Generation A Comprehensive Survey of Architectures, Enhancements, and Robustnes.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/DMLEBJTL/2506.html:text/html},
}

@article{asai_self-rag_2024,
	title = {{SELF}-{RAG}: {LEARNING} {TO} {RETRIEVE}, {GENERATE}, {AND} {CRITIQUE} {THROUGH} {SELF}-{REFLECTION}},
	language = {en},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	year = {2024},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/KEZTYWE6/Asai 等 - 2024 - SELF-RAG LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION.pdf:application/pdf},
}

@article{zheng_retrieval-augmented_2025,
	title = {A {Retrieval}-{Augmented} {Generation} {Method} for {Question} {Answering} on {Airworthiness} {Regulations}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/14/16/3314},
	doi = {10.3390/electronics14163314},
	abstract = {Civil aviation airworthiness regulations are the fundamental basis for the design and operational safety of aircraft. Their provisions exhibit a high degree of specialization, cross-disciplinary complexity, and hierarchical structure. Moreover, the regulations are frequently updated, posing unique challenges for automated question-answering systems. While large language models (LLMs) have demonstrated remarkable capabilities in dialog and reasoning; however, they still face challenges such as difficulties in knowledge updating and a scarcity of high-quality domain-specific datasets when tackling knowledge-intensive tasks in the field of civil aviation regulations. This study introduces a retrieval-augmented generation (RAG) approach that integrates retrieval modules with generative models to enable more efficient knowledge acquisition and updating, encompassing data processing and retrieval-based reasoning. The data processing stage comprises document conversion, information extraction, and document parsing modules. Additionally, a high-quality airworthiness regulation QA dataset was specifically constructed, covering multiple-choice, true/false, and fill-in-the-blank questions, with a total of 4688 entries. The retrieval-based reasoning stage employs vector search and re-ranking strategies, combined with prompt optimization, to enhance the model’s reasoning capabilities in specific airworthiness certification regulation comprehension tasks. A series of experiments demonstrate the effectiveness of the retrieval-augmented generation approach in this domain, significantly improving answer accuracy and retrieval hit rates.},
	language = {en},
	number = {16},
	urldate = {2025-11-27},
	journal = {Electronics},
	author = {Zheng, Tao and Shen, Shiyu and Zeng, Changchang},
	month = jan,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {civil aviation regulations, large language model, question answering, retrieval augment generation},
	pages = {3314},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/A99TZE66/Zheng 等 - 2025 - A Retrieval-Augmented Generation Method for Question Answering on Airworthiness Regulations.pdf:application/pdf},
}

@misc{masoudifard_leveraging_2024,
	title = {Leveraging {Graph}-{RAG} and {Prompt} {Engineering} to {Enhance} {LLM}-{Based} {Automated} {Requirement} {Traceability} and {Compliance} {Checks}},
	url = {http://arxiv.org/abs/2412.08593},
	doi = {10.48550/arXiv.2412.08593},
	abstract = {Ensuring that Software Requirements Specifications (SRS) align with higher-level organizational or national requirements is vital, particularly in regulated environments such as finance and aerospace. In these domains, maintaining consistency, adhering to regulatory frameworks, minimizing errors, and meeting critical expectations are essential for the reliable functioning of systems. The widespread adoption of large language models (LLMs) highlights their immense potential, yet there remains considerable scope for improvement in retrieving relevant information and enhancing reasoning capabilities. This study demonstrates that integrating a robust Graph-RAG framework with advanced prompt engineering techniques, such as Chain of Thought and Tree of Thought, can significantly enhance performance. Compared to baseline RAG methods and simple prompting strategies, this approach delivers more accurate and context-aware results. While this method demonstrates significant improvements in performance, it comes with challenges. It is both costly and more complex to implement across diverse contexts, requiring careful adaptation to specific scenarios. Additionally, its effectiveness heavily relies on having complete and accurate input data, which may not always be readily available, posing further limitations to its scalability and practicality.},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Masoudifard, Arsalan and Sorond, Mohammad Mowlavi and Madadi, Moein and Sabokrou, Mohammad and Habibi, Elahe},
	month = dec,
	year = {2024},
	note = {arXiv:2412.08593 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/ZDQ4R5JN/Masoudifard 等 - 2024 - Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based Automated Requirement Traceability.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/RRGRLLFL/2412.html:text/html},
}

@misc{agarwal_ragulating_2025,
	title = {{RAGulating} {Compliance}: {A} {Multi}-{Agent} {Knowledge} {Graph} for {Regulatory} {QA}},
	shorttitle = {{RAGulating} {Compliance}},
	url = {http://arxiv.org/abs/2508.09893},
	doi = {10.48550/arXiv.2508.09893},
	abstract = {Regulatory compliance question answering (QA) requires precise, verifiable information, and domain-specific expertise, posing challenges for Large Language Models (LLMs). In this work, we present a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with Retrieval-Augmented Generation (RAG) to address these demands. First, agents build and maintain an ontology-free KG by extracting subject--predicate--object (SPO) triplets from regulatory documents and systematically cleaning, normalizing, deduplicating, and updating them. Second, these triplets are embedded and stored along with their corresponding textual sections and metadata in a single enriched vector database, allowing for both graph-based reasoning and efficient information retrieval. Third, an orchestrated agent pipeline leverages triplet-level retrieval for question answering, ensuring high semantic alignment between user queries and the factual "who-did-what-to-whom" core captured by the graph. Our hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader audit-focused applications.},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Agarwal, Bhavik and Jomraj, Hemant Sunil and Kaplunov, Simone and Krolick, Jack and Rojkova, Viktoria},
	month = aug,
	year = {2025},
	note = {arXiv:2508.09893 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/N6RRI97I/Agarwal 等 - 2025 - RAGulating Compliance A Multi-Agent Knowledge Graph for Regulatory QA.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/7CDWKF4Q/2508.html:text/html},
}

@misc{schlichting_leraat_2025,
	title = {{LeRAAT}: {LLM}-{Enabled} {Real}-{Time} {Aviation} {Advisory} {Tool}},
	shorttitle = {{LeRAAT}},
	url = {http://arxiv.org/abs/2503.16477},
	doi = {10.48550/arXiv.2503.16477},
	abstract = {In aviation emergencies, high-stakes decisions must be made in an instant. Pilots rely on quick access to precise, context-specific information -- an area where emerging tools like large language models (LLMs) show promise in providing critical support. This paper introduces LeRAAT, a framework that integrates LLMs with the X-Plane flight simulator to deliver real-time, context-aware pilot assistance. The system uses live flight data, weather conditions, and aircraft documentation to generate recommendations aligned with aviation best practices and tailored to the particular situation. It employs a Retrieval-Augmented Generation (RAG) pipeline that extracts and synthesizes information from aircraft type-specific manuals, including performance specifications and emergency procedures, as well as aviation regulatory materials, such as FAA directives and standard operating procedures. We showcase the framework in both a virtual reality and traditional on-screen simulation, supporting a wide range of research applications such as pilot training, human factors research, and operational decision support.},
	language = {en-US},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Schlichting, Marc R. and Rasmussen, Vale and Alazzeh, Heba and Liu, Houjun and Jafari, Kiana and Hardy, Amelia F. and Asmar, Dylan M. and Kochenderfer, Mykel J.},
	month = mar,
	year = {2025},
	note = {arXiv:2503.16477 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Emerging Technologies, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/UGURRWN4/Schlichting 等 - 2025 - LeRAAT LLM-Enabled Real-Time Aviation Advisory Tool.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/G9DWH74B/2503.html:text/html},
}

@misc{hsia_ragged_2025,
	title = {{RAGGED}: {Towards} {Informed} {Design} of {Scalable} and {Stable} {RAG} {Systems}},
	shorttitle = {{RAGGED}},
	url = {http://arxiv.org/abs/2403.09040},
	doi = {10.48550/arXiv.2403.09040},
	abstract = {Retrieval-augmented generation (RAG) enhances language models by integrating external knowledge, but its effectiveness is highly dependent on system configuration. Improper retrieval settings can degrade performance, making RAG less reliable than closed-book generation. In this work, we introduce RAGGED, a framework for systematically evaluating RAG systems across diverse retriever-reader configurations, retrieval depths, and datasets. Our analysis reveals that reader robustness to noise is the key determinant of RAG stability and scalability. Some readers benefit from increased retrieval depth, while others degrade due to their sensitivity to distracting content. Through large-scale experiments on open-domain, multi-hop, and specialized-domain datasets, we show that retrievers, rerankers, and prompts influence performance but do not fundamentally alter these reader-driven trends. By providing a principled framework and new metrics to assess RAG stability and scalability, RAGGED enables systematic evaluation of retrieval-augmented generation systems, guiding future research on optimizing retrieval depth and model robustness.},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Hsia, Jennifer and Shaikh, Afreen and Wang, Zhiruo and Neubig, Graham},
	month = jul,
	year = {2025},
	note = {arXiv:2403.09040 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/Q9IQV9VW/Hsia 等 - 2025 - RAGGED Towards Informed Design of Scalable and Stable RAG Systems.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/KEB7XQQR/2403.html:text/html},
}

@article{ho_decentralized_2022,
	title = {Decentralized {Multi}-{Agent} {Path} {Finding} for {UAV} {Traffic} {Management}},
	volume = {23},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1524-9050, 1558-0016},
	url = {https://ieeexplore.ieee.org/document/9187227/},
	doi = {10.1109/TITS.2020.3019397},
	number = {2},
	urldate = {2025-12-04},
	journal = {IEEE Trans. Intell. Transport. Syst.},
	author = {Ho, Florence and Geraldes, Ruben and Goncalves, Artur and Rigault, Bastien and Sportich, Benjamin and Kubo, Daisuke and Cavazza, Marc and Prendinger, Helmut},
	month = feb,
	year = {2022},
	pages = {997--1008},
	file = {已接受版本:/Users/zhangyunshi/Zotero/storage/KQL3Z2X5/Ho 等 - 2022 - Decentralized Multi-Agent Path Finding for UAV Traffic Management.pdf:application/pdf},
}

@article{merchan_2021_2024,
	title = {2021 {Amazon} {Last} {Mile} {Routing} {Research} {Challenge}: {Data} {Set}},
	volume = {58},
	issn = {0041-1655, 1526-5447},
	shorttitle = {2021 {Amazon} {Last} {Mile} {Routing} {Research} {Challenge}},
	url = {https://pubsonline.informs.org/doi/10.1287/trsc.2022.1173},
	doi = {10.1287/trsc.2022.1173},
	abstract = {The 2021 Amazon Last Mile Routing Research Challenge, hosted by Amazon. com’s Last Mile Research team, and scientiﬁcally supported by the Massachusetts Institute of Technology’s Center for Transportation and Logistics, prompted participants to leverage real operational data to ﬁnd new and better ways to solve a real-world routing problem. In this article, we describe the data set released for the research challenge, which includes route-, stop-, and package-level features for 9,184 historical routes performed by Amazon drivers in 2018 in ﬁve metropolitan areas in the United States. This real-world data set excludes any personally identiﬁable information: all route and package identiﬁers have been randomly regenerated and related location data have been obfuscated to ensure anonymity. Although multiple synthetic benchmark data sets are available in the literature, the data set of the 2021 Amazon Last Mile Routing Research Challenge is the ﬁrst large and publicly available data set to include instances based on real-world operational routing data.},
	language = {en},
	number = {1},
	urldate = {2025-12-04},
	journal = {Transportation Science},
	author = {Merchán, Daniel and Arora, Jatin and Pachon, Julian and Konduri, Karthik and Winkenbach, Matthias and Parks, Steven and Noszek, Joseph},
	month = jan,
	year = {2024},
	pages = {8--11},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/LFSEYF5Z/Merchán 等 - 2024 - 2021 Amazon Last Mile Routing Research Challenge Data Set.pdf:application/pdf},
}

@misc{ferrag_uavbench_2025,
	title = {{UAVBench}: {An} {Open} {Benchmark} {Dataset} for {Autonomous} and {Agentic} {AI} {UAV} {Systems} via {LLM}-{Generated} {Flight} {Scenarios}},
	shorttitle = {{UAVBench}},
	url = {http://arxiv.org/abs/2511.11252},
	doi = {10.48550/arXiv.2511.11252},
	abstract = {Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench\_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench\_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench},
	urldate = {2025-12-04},
	publisher = {arXiv},
	author = {Ferrag, Mohamed Amine and Lakas, Abderrahmane and Debbah, Merouane},
	month = nov,
	year = {2025},
	note = {arXiv:2511.11252 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/7UPEUB2J/Ferrag 等 - 2025 - UAVBench An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flig.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/LPYY46JB/2511.html:text/html},
}

@misc{yuan_next-generation_2025,
	title = {Next-{Generation} {LLM} for {UAV}: {From} {Natural} {Language} to {Autonomous} {Flight}},
	shorttitle = {Next-{Generation} {LLM} for {UAV}},
	url = {http://arxiv.org/abs/2510.21739},
	doi = {10.48550/arXiv.2510.21739},
	abstract = {With the rapid advancement of Large Language Models (LLMs), their capabilities in various automation domains, particularly Unmanned Aerial Vehicle (UAV) operations, have garnered increasing attention. Current research remains predominantly constrained to small-scale UAV applications, with most studies focusing on isolated components such as path planning for toy drones, while lacking comprehensive investigation of medium- and long-range UAV systems in real-world operational contexts. Larger UAV platforms introduce distinct challenges, including stringent requirements for airport-based take-off and landing procedures, adherence to complex regulatory frameworks, and specialized operational capabilities with elevated mission expectations. This position paper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive demonstration and automation roadmap for integrating LLMs into multi-scale UAV operations. The NeLV system processes natural language instructions to orchestrate short-, medium-, and long-range UAV missions through five key technical components: (i) LLM-as-Parser for instruction interpretation, (ii) Route Planner for Points of Interest (POI) determination, (iii) Path Planner for waypoint generation, (iv) Control Platform for executable trajectory implementation, and (v) UAV monitoring. We demonstrate the system's feasibility through three representative use cases spanning different operational scales: multi-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the current implementation, we establish a five-level automation taxonomy that charts the evolution from current LLM-as-Parser capabilities (Level 1) to fully autonomous LLM-as-Autopilot systems (Level 5), identifying technical prerequisites and research challenges at each stage.},
	urldate = {2025-12-04},
	publisher = {arXiv},
	author = {Yuan, Liangqi and Deng, Chuhao and Han, Dong-Jun and Hwang, Inseok and Brunswicker, Sabine and Brinton, Christopher G.},
	month = oct,
	year = {2025},
	note = {arXiv:2510.21739 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/E333CKHK/Yuan 等 - 2025 - Next-Generation LLM for UAV From Natural Language to Autonomous Flight.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/JCFWIE2K/2510.html:text/html},
}

@article{liu_large_2024,
	title = {Large language models for air transportation: {A} critical review},
	volume = {2},
	issn = {2941198X},
	shorttitle = {Large language models for air transportation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2941198X24000356},
	doi = {10.1016/j.jatrs.2024.100024},
	abstract = {In the past decade, Artiﬁcial Intelligence (AI) has contributed to the improvement of various aviation aspects, including ﬂight plan optimization, the development of autonomous systems, performing predictive analytics, as well as in passenger / crew assistance systems. The latest AI technology to potentially revolutionize air transportation are so-called Large Language Models (LLMs), which have an outstanding ability to process and generate human-like text. The application areas for LLMs cover nearly all aspects of air transportation, through language processing, content generation, and problem solving. In this study, we discuss the potential of this impact with two major contributions. First, we have performed an experimental evaluation of twelve commonly-used LLMs concerning their performance of air transportation related subjects, covering fact retrieval, complex reasoning abilities, and explanation tasks. Second, we have performed a survey among graduate students at Beihang University, a leading aviation university in China, to explore the experiences and uses of LLMs. We believe that our study makes a signiﬁcant contribution towards the dissemination and application of LLMs in the air transportation domain.},
	language = {en},
	urldate = {2025-12-04},
	journal = {Journal of the Air Transport Research Society},
	author = {Liu, Yucheng},
	month = jun,
	year = {2024},
	pages = {100024},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/YN9SEUQT/Liu - 2024 - Large language models for air transportation A critical review.pdf:application/pdf},
}

@article{sood_paradigm_2025,
	title = {The paradigm of hallucinations in {AI}-driven cybersecurity systems: {Understanding} taxonomy, classification outcomes, and mitigations},
	volume = {124},
	issn = {00457906},
	shorttitle = {The paradigm of hallucinations in {AI}-driven cybersecurity systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045790625002502},
	doi = {10.1016/j.compeleceng.2025.110307},
	abstract = {The adoption of AI to solve cybersecurity problems is occurring exponentially. However, AIdriven cybersecurity systems face significant challenges due to the impact of hallucinations in Large Language Models (LLMs). In AI-driven cybersecurity systems, hallucinations refer to instances when an AI model generates fabricated, inaccurate, and misleading information that impacts the security posture of organizations. This failure to recognize and misreport security threats identifies benign activities as malicious, invents insights not grounded to actual cyber threats, and causes real threats to go undetected due to erroneous interpretations. Hallucinations are a critical problem in AI-driven cybersecurity because they can lead to severe vulnerabilities, erode trust in automated systems, and divert resources to address non-existent threats. In cybersecurity, where real-time, accurate insights are vital, hallucinated outputs—such as mistakenly generated alerts, can cause a misallocation of time and resources. It is crucial to address hallucinations by improving LLM accuracy, grounding outputs in real-time data, and implementing human oversight mechanisms to ensure that AI-based cybersecurity systems remain trustworthy, reliable, and capable of defending against sophisticated threats. We present a taxonomy of hallucinations in LLMs for cybersecurity, including mapping LLM responses to classification outcomes (confusion matrix components). Finally, we discuss mitigation strategies to combat hallucinations.},
	language = {en},
	urldate = {2025-12-04},
	journal = {Computers and Electrical Engineering},
	author = {Sood, Aditya K and Zeadally, Sherali and Hong, EenKee},
	month = may,
	year = {2025},
	pages = {110307},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/LRIZFGNR/Sood 等 - 2025 - The paradigm of hallucinations in AI-driven cybersecurity systems Understanding taxonomy, classific.pdf:application/pdf},
}

@misc{noauthor_when_nodate,
	title = {When {LLMs} day dream: {Hallucinations} and how to prevent them},
	shorttitle = {When {LLMs} day dream},
	url = {https://www.redhat.com/en/blog/when-llms-day-dream-hallucinations-how-prevent-them},
	abstract = {Most general purpose large language models (LLM) are trained with a wide range of generic data on the internet.},
	language = {en},
	urldate = {2025-12-04},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/UR66INBD/when-llms-day-dream-hallucinations-how-prevent-them.html:text/html},
}

@article{balazs_decentralized_2025,
	title = {Decentralized traffic management of autonomous drones},
	volume = {19},
	issn = {1935-3820},
	url = {https://doi.org/10.1007/s11721-024-00241-y},
	doi = {10.1007/s11721-024-00241-y},
	abstract = {Coordination of local and global aerial traffic has become a legal and technological bottleneck as the number of unmanned vehicles in the common airspace continues to grow. To meet this challenge, automation and decentralization of control is an unavoidable requirement. In this paper, we present a solution that enables self-organization of cooperating autonomous agents into an effective traffic flow state in which the common aerial coordination task—filled with conflicts—is resolved. Using realistic simulations, we show that our algorithm is safe, efficient, and scalable regarding the number of drones and their speed range, while it can also handle heterogeneous agents and even pairwise priorities between them. The algorithm works in any sparse or dense traffic scenario in two dimensions and can be made increasingly efficient by a layered flight space structure in three dimensions. To support the feasibility of our solution, we show stable traffic simulations with up to 5000 agents, and experimentally demonstrate coordinated aerial traffic of 100 autonomous drones within a 250 m wide circular area.},
	language = {en},
	number = {1},
	urldate = {2025-12-06},
	journal = {Swarm Intell},
	author = {Balázs, Boldizsár and Vicsek, Tamás and Somorjai, Gergő and Nepusz, Tamás and Vásárhelyi, Gábor},
	month = mar,
	year = {2025},
	keywords = {Collective AI, Collective intelligence, Decentralized air traffic, Drone swarm, Self-organization, Unmanned traffic management},
	pages = {29--53},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/DQ5IGX3P/Balázs 等 - 2025 - Decentralized traffic management of autonomous drones.pdf:application/pdf},
}

@article{li_traffic_2022,
	title = {Traffic management and resource allocation for {UAV}-based parcel delivery in low-altitude urban space},
	volume = {143},
	issn = {0968090X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0968090X22002339},
	doi = {10.1016/j.trc.2022.103808},
	abstract = {This research proposes a framework of Unmanned Aircraft Vehicles (UAV) system traffic management in the context of parcel delivery in low-altitude urban airspace, including clusteringbased UAV path planning, Unmanned Aircraft System Traffic Management (UTM) with conflict detection and resolution (CD\&R), and mechanism design for airspace resource allocation. For UAV path planning, we develop a procedure by first clustering a large variety of obstacles that arise from building heights and terrain topology and can impede UAV flying. Based on the clustered obstacles, Saturated Fast-Marching Square (Saturated FM2) algorithm is then employed to generate optimal and alternative paths for each UAV mission. While identifying the optimal and alternative paths does not consider UAV traffic interactions, several traffic management models are proposed to efficiently allocate spatial and temporal airspace resources to UAV missions. The UTM models determine the departure time and the path to take for each UAV flight while resolving path conflicts from different perspectives. Specifically, four UTM models are proposed: Sequential Delay (SD) Model, Sequential Delay/Reroute (SDR) Model, Full Optimization (FO) Model, and Batch Optimization (BO) Model. Among the four models, the BO model is of particular interest as it strikes a balance between seeking a system optimum solution and maintaining computational tractability. Given that traffic management requires private information from UAV operators, the Vickrey-Clarke-Groves (VCG) mechanism is further adapted to the UTM context, in which airspace resource allocation is performed in conjunction with a payment scheme to incentivize truthful private information reporting by UAV operators. Extensive numerical analysis is conducted with San Francisco as the case study area. The results show the effectiveness of the proposed framework, particularly the scalability of the BO model. We also find that payment by a UAV flight under the adapted VCG mechanism depends critically on traffic density and the extent of interaction the UAV flight has with other flights.},
	language = {en},
	urldate = {2025-12-06},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Li, Ang and Hansen, Mark and Zou, Bo},
	month = oct,
	year = {2022},
	pages = {103808},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/TFS2LDCE/Li 等 - 2022 - Traffic management and resource allocation for UAV-based parcel delivery in low-altitude urban space.pdf:application/pdf},
}

@article{outay_applications_2020,
	title = {Applications of unmanned aerial vehicle ({UAV}) in road safety, traffic and highway infrastructure management: {Recent} advances and challenges},
	volume = {141},
	issn = {09658564},
	shorttitle = {Applications of unmanned aerial vehicle ({UAV}) in road safety, traffic and highway infrastructure management},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S096585642030728X},
	doi = {10.1016/j.tra.2020.09.018},
	abstract = {For next-generation smart cities, small UAVs (also known as drones) are vital to incorporate in airspace for advancing the transportation systems. This paper presents a review of recent developments in relation to the application of UAVs in three major domains of transportation, namely; road safety, traffic monitoring and highway infrastructure management. Advances in computer vision algorithms to extract key features from UAV acquired videos and images are discussed along with the discussion on improvements made in traffic flow analysis methods, risk assessment and assistance in accident investigation and damage assessments for bridges and pavements. Additionally, barriers associated with the wide-scale deployment of UAVs technology are identified and countermeasures to overcome these barriers are discussed, along with their implications.},
	language = {en},
	urldate = {2025-12-06},
	journal = {Transportation Research Part A: Policy and Practice},
	author = {Outay, Fatma and Mengash, Hanan Abdullah and Adnan, Muhammad},
	month = nov,
	year = {2020},
	pages = {116--129},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/FL4PH3B4/Outay 等 - 2020 - Applications of unmanned aerial vehicle (UAV) in road safety, traffic and highway infrastructure man.pdf:application/pdf},
}

@misc{andriuskevicius_automatic_2024,
	title = {Automatic {Control} {With} {Human}-{Like} {Reasoning}: {Exploring} {Language} {Model} {Embodied} {Air} {Traffic} {Agents}},
	shorttitle = {Automatic {Control} {With} {Human}-{Like} {Reasoning}},
	url = {http://arxiv.org/abs/2409.09717},
	doi = {10.48550/arXiv.2409.09717},
	abstract = {Recent developments in language models have created new opportunities in air traffic control studies. The current focus is primarily on text and language-based use cases. However, these language models may offer a higher potential impact in the air traffic control domain, thanks to their ability to interact with air traffic environments in an embodied agent form. They also provide a language-like reasoning capability to explain their decisions, which has been a significant roadblock for the implementation of automatic air traffic control. This paper investigates the application of a language model-based agent with function-calling and learning capabilities to resolve air traffic conflicts without human intervention. The main components of this research are foundational large language models, tools that allow the agent to interact with the simulator, and a new concept, the experience library. An innovative part of this research, the experience library, is a vector database that stores synthesized knowledge that agents have learned from interactions with the simulations and language models. To evaluate the performance of our language model-based agent, both open-source and closed-source models were tested. The results of our study reveal significant differences in performance across various configurations of the language model-based agents. The best-performing configuration was able to solve almost all 120 but one imminent conflict scenarios, including up to four aircraft at the same time. Most importantly, the agents are able to provide human-level text explanations on traffic situations and conflict resolution strategies.},
	urldate = {2025-12-06},
	publisher = {arXiv},
	author = {Andriuškevičius, Justas and Sun, Junzi},
	month = sep,
	year = {2024},
	note = {arXiv:2409.09717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/BXHK2CNT/Andriuškevičius和Sun - 2024 - Automatic Control With Human-Like Reasoning Exploring Language Model Embodied Air Traffic Agents.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/VPJLL24Y/2409.html:text/html},
}

@misc{aguilera_agentic_2025,
	title = {Agentic {AI} in {Aviation}: {A} {Proof} of {Concept} for {Autonomous} {Flight} {Plan} {Generation}},
	shorttitle = {Agentic {AI} in {Aviation}},
	url = {https://medium.com/thedeephub/agentic-ai-in-aviation-a-proof-of-concept-for-autonomous-flight-plan-generation-3f93498d50e5},
	abstract = {Frank Morales Aguilera, BEng, MEng, SMIEEE},
	language = {en},
	urldate = {2025-12-06},
	journal = {The Deep Hub},
	author = {Aguilera, Frank Morales},
	month = jan,
	year = {2025},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/WEQFQHCF/agentic-ai-in-aviation-a-proof-of-concept-for-autonomous-flight-plan-generation-3f93498d50e5.html:text/html},
}

@article{javaid_large_2024,
	title = {Large {Language} {Models} for {UAVs}: {Current} {State} and {Pathways} to the {Future}},
	volume = {5},
	issn = {2644-1330},
	shorttitle = {Large {Language} {Models} for {UAVs}},
	url = {https://ieeexplore.ieee.org/document/10643253/},
	doi = {10.1109/OJVT.2024.3446799},
	abstract = {Unmanned Aerial Vehicles (UAVs) have emerged as a transformative technology across diverse sectors, offering adaptable solutions to complex challenges in both military and civilian domains. Their expanding capabilities present a platform for further advancement by integrating cutting-edge computational tools like Artificial Intelligence (AI) and Machine Learning (ML) algorithms. These advancements have significantly impacted various facets of human life, fostering an era of unparalleled efficiency and convenience. Large Language Models (LLMs), a key component of AI, exhibit remarkable learning and adaptation capabilities within deployed environments, demonstrating an evolving form of intelligence with the potential to approach human-level proficiency. This work explores the significant potential of integrating UAVs and LLMs to propel the development of autonomous systems. We comprehensively review LLM architectures, evaluating their suitability for UAV integration. Additionally, we summarize the state-of-the-art LLM-based UAV architectures and identify novel opportunities for LLM embedding within UAV frameworks. Notably, we focus on leveraging LLMs to refine data analysis and decision-making processes, specifically for enhanced spectral sensing and sharing in UAV applications. Furthermore, we investigate how LLM integration expands the scope of existing UAV applications, enabling autonomous data processing, improved decision-making, and faster response times in emergency scenarios like disaster response and network restoration. Finally, we highlight crucial areas for future research that are critical for facilitating the effective integration of LLMs and UAVs.},
	urldate = {2025-12-06},
	journal = {IEEE Open Journal of Vehicular Technology},
	author = {Javaid, Shumaila and Fahim, Hamza and He, Bin and Saeed, Nasir},
	year = {2024},
	keywords = {Artificial intelligence, Autonomous aerial vehicles, autonomous systems, Decision making, decision-making, large language models, Large language models, Real-time systems, Sensors, Spectral analysis, spectral sensing, Training, UAVs, Wireless networks},
	pages = {1166--1192},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/CIATLI7V/Javaid 等 - 2024 - Large Language Models for UAVs Current State and Pathways to the Future.pdf:application/pdf},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome - {GraphRAG}},
	url = {https://microsoft.github.io/graphrag/},
	urldate = {2025-12-06},
	file = {Welcome - GraphRAG:/Users/zhangyunshi/Zotero/storage/E9J4C3CB/graphrag.html:text/html},
}

@misc{noauthor_microsoftgraphrag_2025,
	title = {microsoft/graphrag},
	copyright = {MIT},
	url = {https://github.com/microsoft/graphrag},
	abstract = {A modular graph-based Retrieval-Augmented Generation (RAG) system},
	urldate = {2025-12-06},
	publisher = {Microsoft},
	month = dec,
	year = {2025},
	note = {original-date: 2024-03-27T17:57:52Z},
	keywords = {gpt, gpt-4, gpt4, graphrag, llm, llms, rag},
}

@misc{jourdain_graph_2025,
	title = {Graph {RAG} \& {Elasticsearch}: {Implementing} {RAG} on a {Knowledge} {Graph}},
	shorttitle = {Graph {RAG} \& {Elasticsearch}},
	url = {https://www.elastic.co/search-labs/blog/rag-graph-traversal},
	abstract = {Learn about Graph RAG and discover how to use Knowledge Graphs to enhance RAG results while storing the graph in Elasticsearch.},
	language = {en-US},
	urldate = {2025-12-06},
	journal = {Elasticsearch Labs},
	author = {Jourdain, Louis and Monnier, Ivan},
	month = jan,
	year = {2025},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/MU4V779N/rag-graph-traversal.html:text/html},
}

@misc{yuan_mkg-rag_2025,
	title = {{mKG}-{RAG}: {Multimodal} {Knowledge} {Graph}-{Enhanced} {RAG} for {Visual} {Question} {Answering}},
	shorttitle = {{mKG}-{RAG}},
	url = {http://arxiv.org/abs/2508.05318},
	doi = {10.48550/arXiv.2508.05318},
	abstract = {Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating external knowledge databases into the generation process, which is widely used for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive advancements, vanilla RAG-based VQA methods that rely on unstructured documents and overlook the structural relationships among knowledge elements frequently introduce irrelevant or misleading content, reducing answer accuracy and reliability. To overcome these challenges, a promising solution is to integrate multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the generation by introducing structured multimodal knowledge. Therefore, in this paper, we propose a novel multimodal knowledge-augmented generation framework (mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks. Specifically, our approach leverages MLLM-powered keyword extraction and vision-text matching to distill semantically consistent and modality-aligned entities/relationships from multimodal documents, constructing high-quality multimodal KGs as structured knowledge representations. In addition, a dual-stage retrieval strategy equipped with a question-aware multimodal retriever is introduced to improve retrieval efficiency while refining precision. Comprehensive experiments demonstrate that our approach significantly outperforms existing methods, setting a new state-of-the-art for knowledge-based VQA.},
	urldate = {2025-12-06},
	publisher = {arXiv},
	author = {Yuan, Xu and Ning, Liangbo and Fan, Wenqi and Li, Qing},
	month = aug,
	year = {2025},
	note = {arXiv:2508.05318 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/8NTTNFFZ/Yuan 等 - 2025 - mKG-RAG Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/DLXQX4XF/2508.html:text/html},
}

@inproceedings{zhao_medrag_2025,
	address = {Sydney NSW Australia},
	title = {{MedRAG}: {Enhancing} {Retrieval}-augmented {Generation} with {Knowledge} {Graph}-{Elicited} {Reasoning} for {Healthcare} {Copilot}},
	isbn = {979-8-4007-1274-6},
	shorttitle = {{MedRAG}},
	url = {https://dl.acm.org/doi/10.1145/3696410.3714782},
	doi = {10.1145/3696410.3714782},
	language = {en},
	urldate = {2025-12-06},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {ACM},
	author = {Zhao, Xuejiao and Liu, Siyan and Yang, Su-Yin and Miao, Chunyan},
	month = apr,
	year = {2025},
	pages = {4442--4457},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/BDZJ82BJ/Zhao 等 - 2025 - MedRAG Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthc.pdf:application/pdf},
}

@inproceedings{xu_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} with {Knowledge} {Graphs} for {Customer} {Service} {Question} {Answering}},
	url = {http://arxiv.org/abs/2404.17723},
	doi = {10.1145/3626772.3661370},
	abstract = {In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6\% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6\%.},
	urldate = {2025-12-06},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Xu, Zhentao and Cruz, Mark Jerome and Guevara, Matthew and Wang, Tie and Deshpande, Manasi and Wang, Xiaofeng and Li, Zheng},
	month = jul,
	year = {2024},
	note = {arXiv:2404.17723 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {2905--2909},
	file = {Preprint PDF:/Users/zhangyunshi/Zotero/storage/LQ88C3EM/Xu 等 - 2024 - Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/EVSQMLSI/2404.html:text/html},
}

@inproceedings{guo_lightrag_2025,
	address = {Suzhou, China},
	title = {{LightRAG}: {Simple} and {Fast} {Retrieval}-{Augmented} {Generation}},
	isbn = {979-8-89176-335-7},
	shorttitle = {{LightRAG}},
	url = {https://aclanthology.org/2025.findings-emnlp.568/},
	doi = {10.18653/v1/2025.findings-emnlp.568},
	abstract = {Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex interdependencies. To address these challenges, we propose LightRAG, a novel framework that incorporates graph structures into text indexing and retrieval processes. This innovative approach employs a dual-level retrieval system that enhances comprehensive information retrieval from both low- and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG framework open source and anonymously available at the link: https://anonymous.4open.science/r/LightRAG-2BEE.},
	urldate = {2025-12-06},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
	editor = {Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet},
	month = nov,
	year = {2025},
	pages = {10746--10761},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/84UM7ARL/Guo 等 - 2025 - LightRAG Simple and Fast Retrieval-Augmented Generation.pdf:application/pdf},
}

@article{song_industry_2024,
	title = {Industry {Practices} for {Challenging} {Autonomous} {Driving} {Systems} with {Critical} {Scenarios}},
	volume = {33},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3640334},
	doi = {10.1145/3640334},
	abstract = {Testing autonomous driving systems for safety and reliability is essential, yet complex. A primary challenge is identifying relevant test scenarios, especially the critical ones that may expose hazards or harm to autonomous vehicles and other road users. Although numerous approaches and tools for critical scenario identification are proposed, the industry practices for selection, implementation, and evaluation of approaches, are not well understood. Therefore, we aim at exploring practical aspects of how autonomous driving systems are tested, particularly the identification and use of critical scenarios. We interviewed 13 practitioners from 7 companies in autonomous driving in Sweden. We used thematic modeling to analyse and synthesize the interview data. As a result, we present 9 themes of practices and 4 themes of challenges related to critical scenarios. Our analysis indicates there is little joint effort in the industry, despite every approach has its own limitations, and tools and platforms are lacking. To that end, we recommend the industry and academia combine different approaches, collaborate among different stakeholders, and continuously learn the field. The contributions of our study are exploration and synthesis of industry practices and related challenges for critical scenario identification and testing, and potential increase of industry relevance for future studies.},
	language = {en},
	number = {4},
	urldate = {2025-12-06},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Song, Qunying and Engström, Emelie and Runeson, Per},
	month = may,
	year = {2024},
	pages = {1--35},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/9VYZWWA8/Song 等 - 2024 - Industry Practices for Challenging Autonomous Driving Systems with Critical Scenarios.pdf:application/pdf},
}

@inproceedings{raina_is_2024,
	address = {Miami, Florida, USA},
	title = {Is {LLM}-as-a-{Judge} {Robust}? {Investigating} {Universal} {Adversarial} {Attacks} on {Zero}-shot {LLM} {Assessment}},
	shorttitle = {Is {LLM}-as-a-{Judge} {Robust}?},
	url = {https://aclanthology.org/2024.emnlp-main.427},
	doi = {10.18653/v1/2024.emnlp-main.427},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Raina, Vyas and Liusie, Adian and Gales, Mark},
	year = {2024},
	pages = {7499--7517},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/U84MJUDL/Raina 等 - 2024 - Is LLM-as-a-Judge Robust Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment.pdf:application/pdf},
}

@misc{ye_benchmarking_2024,
	title = {Benchmarking {LLMs} via {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2401.12794},
	doi = {10.48550/arXiv.2401.12794},
	abstract = {The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs.},
	urldate = {2025-12-10},
	publisher = {arXiv},
	author = {Ye, Fanghua and Yang, Mingming and Pang, Jianhui and Wang, Longyue and Wong, Derek F. and Yilmaz, Emine and Shi, Shuming and Tu, Zhaopeng},
	month = oct,
	year = {2024},
	note = {arXiv:2401.12794 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/NXLHSAZU/Ye 等 - 2024 - Benchmarking LLMs via Uncertainty Quantification.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/J5HD8P5J/2401.html:text/html},
}

@inproceedings{li_evaluating_2024,
	address = {Miami, Florida, USA},
	title = {Evaluating the {Instruction}-{Following} {Robustness} of {Large} {Language} {Models} to {Prompt} {Injection}},
	url = {https://aclanthology.org/2024.emnlp-main.33/},
	doi = {10.18653/v1/2024.emnlp-main.33},
	abstract = {Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs' instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future.},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Zekun and Peng, Baolin and He, Pengcheng and Yan, Xifeng},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {557--568},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/U39IHVDR/Li 等 - 2024 - Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection.pdf:application/pdf},
}

@misc{yadkori_believe_2024,
	title = {To {Believe} or {Not} to {Believe} {Your} {LLM}},
	url = {http://arxiv.org/abs/2406.02543},
	doi = {10.48550/arXiv.2406.02543},
	abstract = {We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.},
	urldate = {2025-12-10},
	publisher = {arXiv},
	author = {Yadkori, Yasin Abbasi and Kuzborskij, Ilja and György, András and Szepesvári, Csaba},
	month = jul,
	year = {2024},
	note = {arXiv:2406.02543 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/9HUS4JUI/Yadkori 等 - 2024 - To Believe or Not to Believe Your LLM.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/XHW49CMF/2406.html:text/html},
}

@inproceedings{puerto_code_2024,
	address = {Miami, Florida, USA},
	title = {Code {Prompting} {Elicits} {Conditional} {Reasoning} {Abilities} in {Text}+{Code} {LLMs}},
	url = {https://aclanthology.org/2024.emnlp-main.629/},
	doi = {10.18653/v1/2024.emnlp-main.629},
	abstract = {Reasoning is a fundamental component of language understanding. Recent prompting techniques, such as chain of thought, have consistently improved LLMs' performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage. In this paper, we investigate the effect of the input representation on the reasoning abilities of LLMs. We hypothesize that representing natural language tasks as code can enhance specific reasoning abilities such as entity tracking or logical reasoning. To study this, we propose code prompting, a methodology we operationalize as a chain of prompts that transforms a natural language problem into code and directly prompts the LLM using the generated code without resorting to external code execution. We find that code prompting exhibits a high-performance boost for multiple LLMs (up to 22.52 percentage points on GPT 3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional reasoning datasets. We then conduct comprehensive experiments to understand how the code representation triggers reasoning abilities and which capabilities are elicited in the underlying models. Our analysis on GPT 3.5 reveals that the code formatting of the input problem is essential for performance improvement. Furthermore, the code representation improves sample efficiency of in-context learning and facilitates state tracking of entities.},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Puerto, Haritz and Tutek, Martin and Aditya, Somak and Zhu, Xiaodan and Gurevych, Iryna},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {11234--11258},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/25WV7BFX/Puerto 等 - 2024 - Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs.pdf:application/pdf},
}

@inproceedings{xu_knowledge_2024,
	address = {Miami, Florida, USA},
	title = {Knowledge {Conflicts} for {LLMs}: {A} {Survey}},
	shorttitle = {Knowledge {Conflicts} for {LLMs}},
	url = {https://aclanthology.org/2024.emnlp-main.486/},
	doi = {10.18653/v1/2024.emnlp-main.486},
	abstract = {This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Rongwu and Qi, Zehan and Guo, Zhijiang and Wang, Cunxiang and Wang, Hongru and Zhang, Yue and Xu, Wei},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {8541--8565},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/8ENZT5BJ/Xu 等 - 2024 - Knowledge Conflicts for LLMs A Survey.pdf:application/pdf},
}

@article{falanga_dynamic_2020,
	title = {Dynamic obstacle avoidance for quadrotors with event cameras},
	volume = {5},
	url = {https://www.science.org/doi/10.1126/scirobotics.aaz9712},
	doi = {10.1126/scirobotics.aaz9712},
	abstract = {Today’s autonomous drones have reaction times of tens of milliseconds, which is not enough for navigating fast in complex dynamic environments. To safely avoid fast moving objects, drones need low-latency sensors and algorithms. We departed from state-of-the-art approaches by using event cameras, which are bioinspired sensors with reaction times of microseconds. Our approach exploits the temporal information contained in the event stream to distinguish between static and dynamic objects and leverages a fast strategy to generate the motor commands necessary to avoid the approaching obstacles. Standard vision algorithms cannot be applied to event cameras because the output of these sensors is not images but a stream of asynchronous events that encode per-pixel intensity changes. Our resulting algorithm has an overall latency of only 3.5 milliseconds, which is sufficient for reliable detection and avoidance of fast-moving obstacles. We demonstrate the effectiveness of our approach on an autonomous quadrotor using only onboard sensing and computation. Our drone was capable of avoiding multiple obstacles of different sizes and shapes, at relative speeds up to 10 meters/second, both indoors and outdoors.},
	number = {40},
	urldate = {2025-12-10},
	journal = {Science Robotics},
	author = {Falanga, Davide and Kleber, Kevin and Scaramuzza, Davide},
	month = mar,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaaz9712},
}

@inproceedings{jian_dynamic_2023,
	title = {Dynamic {Control} {Barrier} {Function}-based {Model} {Predictive} {Control} to {Safety}-{Critical} {Obstacle}-{Avoidance} of {Mobile} {Robot}},
	url = {https://ieeexplore.ieee.org/document/10160857/},
	doi = {10.1109/ICRA48891.2023.10160857},
	abstract = {This paper presents an efficient and safe method to avoid static and dynamic obstacles based on LiDAR. First, point cloud is used to generate a real-time local grid map for obstacle detection. Then, obstacles are clustered by DBSCAN algorithm and enclosed with minimum bounding ellipses (MBEs). In addition, data association is conducted to match each MBE with the obstacle in the current frame. Considering MBE as an observation, Kalman filter (KF) is used to estimate and predict the motion state of the obstacle. In this way, the trajectory of each obstacle in the forward time domain can be parameterized as a set of ellipses. Due to the uncertainty of the MBE, the semi-major and semi-minor axes of the parameterized ellipse are extended to ensure safety. We extend the traditional Control Barrier Function (CBF) and propose Dynamic Control Barrier Function (D-CBF). We combine D-CBF with Model Predictive Control (MPC) to implement safety-critical dynamic obstacle avoidance. Experiments in simulated and real scenarios are conducted to verify the effectiveness of our algorithm. The source code is released for the reference of the community11Code: https://github.com/jianzhuozhuTHU/MPC-D-CBF..},
	urldate = {2025-12-10},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Jian, Zhuozhu and Yan, Zihong and Lei, Xuanang and Lu, Zihong and Lan, Bin and Wang, Xueqian and Liang, Bin},
	month = may,
	year = {2023},
	keywords = {Heuristic algorithms, Kalman filters, Laser radar, Prediction algorithms, Source coding, Trajectory, Uncertainty},
	pages = {3679--3685},
	file = {已提交版本:/Users/zhangyunshi/Zotero/storage/T2EI5SAL/Jian 等 - 2023 - Dynamic Control Barrier Function-based Model Predictive Control to Safety-Critical Obstacle-Avoidanc.pdf:application/pdf},
}

@inproceedings{sun_fast_2020,
	title = {Fast {UAV} {Trajectory} {Optimization} using {Bilevel} {Optimization} with {Analytical} {Gradients}},
	url = {https://ieeexplore.ieee.org/document/9147300/},
	doi = {10.23919/ACC45564.2020.9147300},
	abstract = {We present an efficient optimization framework that solves trajectory optimization problems by decoupling state variables from timing variables, thereby decomposing a challenging nonlinear programming (NLP) problem into two easier subproblems. With timing fixed, the state variables can be optimized efficiently using convex optimization, and the timing variables can be optimized in a separate NLP, which forms a bilevel optimization problem. The challenge is to obtain the gradient of the objective function which itself needs an optimization to compute. Whereas finite differences must solve many optimization problems to compute the gradient, our method is based on sensitivity analysis of parametric programming: the dual solution (Lagrange multipliers) of the lower-level optimization is used to compute analytical gradients. Since the dual solution is a by-product of the optimization, the exact gradients can be obtained "for free". The framework is demonstrated on generating trajectories in safe corridors for an unmanned aerial vehicle. Experiments demonstrate that bilevel optimization converges significantly more reliably than a standard NLP solver, and analytical gradients outperform finite differences in terms of computation speed and accuracy. With a 25ms cutoff time, our approach achieves over 8 times better suboptimality than the current state-of-the-art.},
	urldate = {2025-12-10},
	booktitle = {2020 {American} {Control} {Conference} ({ACC})},
	author = {Sun, Weidong and Tang, Gao and Hauser, Kris},
	month = jul,
	year = {2020},
	note = {ISSN: 2378-5861},
	keywords = {Resource management, Robots, Sensitivity analysis, Timing, Trajectory optimization},
	pages = {82--87},
	file = {已提交版本:/Users/zhangyunshi/Zotero/storage/H5F3RZUL/Sun 等 - 2020 - Fast UAV Trajectory Optimization using Bilevel Optimization with Analytical Gradients.pdf:application/pdf},
}

@inproceedings{wang_autonomous_2021,
	title = {Autonomous {Flights} in {Dynamic} {Environments} with {Onboard} {Vision}},
	url = {https://ieeexplore.ieee.org/document/9636117/},
	doi = {10.1109/IROS51168.2021.9636117},
	abstract = {In this paper, we introduce a complete system for autonomous flight of quadrotors in dynamic environments with onboard sensing. Extended from existing work, we develop an occlusion-aware dynamic perception method based on depth images, which classifies obstacles as dynamic and static. For representing generic dynamic environment, we model dynamic objects with moving ellipsoids and fuse static ones into an occupancy grid map. To achieve dynamic avoidance, we design a planning method composed of modified kinodynamic path searching and gradient-based optimization. The method leverages manually constructed gradients without maintaining a signed distance field (SDF), making the planning procedure finished in milliseconds. We integrate the above methods into a customized quadrotor system and thoroughly test it in real-world experiments, verifying its effective collision avoidance in dynamic environments.},
	urldate = {2025-12-10},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Wang, Yingjian and Ji, Jialin and Wang, Qianhao and Xu, Chao and Gao, Fei},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Design methodology, Fuses, Navigation, Planning, Robot sensing systems, Sensors, Trajectory},
	pages = {1966--1973},
	file = {已提交版本:/Users/zhangyunshi/Zotero/storage/LVR6IZ3R/Wang 等 - 2021 - Autonomous Flights in Dynamic Environments with Onboard Vision.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	urldate = {2025-12-10},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@inproceedings{wang_glue_2018,
	address = {Brussels, Belgium},
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {https://aclanthology.org/W18-5446/},
	doi = {10.18653/v1/W18-5446},
	abstract = {Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	editor = {Linzen, Tal and Chrupała, Grzegorz and Alishahi, Afra},
	month = nov,
	year = {2018},
	pages = {353--355},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/7K2MFKZR/Wang 等 - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.pdf:application/pdf},
}

@inproceedings{rajpurkar_squad_2016,
	address = {Austin, Texas},
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
	shorttitle = {{SQuAD}},
	url = {https://aclanthology.org/D16-1264/},
	doi = {10.18653/v1/D16-1264},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
	month = nov,
	year = {2016},
	pages = {2383--2392},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/M2FX27H7/Rajpurkar 等 - 2016 - SQuAD 100,000+ Questions for Machine Comprehension of Text.pdf:application/pdf},
}

@inproceedings{geiger_are_2012,
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	shorttitle = {Are we ready for autonomous driving?},
	url = {https://ieeexplore.ieee.org/document/6248074/},
	doi = {10.1109/CVPR.2012.6248074},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti},
	urldate = {2025-12-10},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {Benchmark testing, Cameras, Measurement, Optical imaging, Optical sensors, Visualization},
	pages = {3354--3361},
}

@inproceedings{zellers_hellaswag_2019,
	address = {Florence, Italy},
	title = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
	shorttitle = {{HellaSwag}},
	url = {https://aclanthology.org/P19-1472/},
	doi = {10.18653/v1/P19-1472},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textbackslash}ensuremath{\textgreater}95\% accuracy), state-of-the-art models struggle ({\textbackslash}ensuremath{\textless}48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical `Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {4791--4800},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/P75RKMMH/Zellers 等 - 2019 - HellaSwag Can a Machine Really Finish Your Sentence.pdf:application/pdf},
}

@inproceedings{suzgun_challenging_2023,
	address = {Toronto, Canada},
	title = {Challenging {BIG}-{Bench} {Tasks} and {Whether} {Chain}-of-{Thought} {Can} {Solve} {Them}},
	url = {https://aclanthology.org/2023.findings-acl.824/},
	doi = {10.18653/v1/2023.findings-acl.824},
	abstract = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65\% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.},
	urldate = {2025-12-10},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and Wei, Jason},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {13003--13051},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/BEWYNA6S/Suzgun 等 - 2023 - Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.pdf:application/pdf},
}

@misc{noauthor_researchers_2025,
	title = {Researchers discover a shortcoming that makes {LLMs} less reliable},
	url = {https://news.mit.edu/2025/shortcoming-makes-llms-less-reliable-1126},
	abstract = {MIT researchers find large language models sometimes mistakenly link grammatical sequences to specific topics, then rely on these learned patterns when answering queries. This can cause LLMs to fail on new tasks and could be exploited by adversarial agents to trick an LLM into generating harmful content.},
	language = {en},
	urldate = {2025-12-12},
	journal = {MIT News {\textbar} Massachusetts Institute of Technology},
	month = nov,
	year = {2025},
	file = {Snapshot:/Users/zhangyunshi/Zotero/storage/4VMUCUHF/shortcoming-makes-llms-less-reliable-1126.html:text/html},
}

@misc{shi_large_2023,
	title = {Large {Language} {Models} {Can} {Be} {Easily} {Distracted} by {Irrelevant} {Context}},
	url = {http://arxiv.org/abs/2302.00093},
	doi = {10.48550/arXiv.2302.00093},
	abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Schärli, Nathanael and Zhou, Denny},
	month = jun,
	year = {2023},
	note = {arXiv:2302.00093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/IVNDUBCF/Shi 等 - 2023 - Large Language Models Can Be Easily Distracted by Irrelevant Context.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/SED4KZ52/2302.html:text/html},
}

@misc{chen_premise_2024,
	title = {Premise {Order} {Matters} in {Reasoning} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.08939},
	doi = {10.48550/arXiv.2402.08939},
	abstract = {Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30\%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Chen, Xinyun and Chi, Ryan A. and Wang, Xuezhi and Zhou, Denny},
	month = may,
	year = {2024},
	note = {arXiv:2402.08939 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/TMRQTP5B/Chen 等 - 2024 - Premise Order Matters in Reasoning with Large Language Models.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/4PHW84HY/2402.html:text/html},
}

@inproceedings{wang_agentvigil_2025,
	address = {Suzhou, China},
	title = {{AGENTVIGIL}: {Automatic} {Black}-{Box} {Red}-teaming for {Indirect} {Prompt} {Injection} against {LLM} {Agents}},
	isbn = {979-8-89176-335-7},
	shorttitle = {{AGENTVIGIL}},
	url = {https://aclanthology.org/2025.findings-emnlp.1258/},
	doi = {10.18653/v1/2025.findings-emnlp.1258},
	abstract = {There emerges a critical security risk of LLM agents: indirect prompt injection, a sophisticated attack vector that compromises thecore of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box optimization framework, AGENTVIGIL, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selectionalgorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, therebymaximizing the likelihood of uncovering agent weaknesses. We evaluate AGENTVIGIL on twopublic benchmarks, AgentDojo and VWA-adv, where it achieves 71\% and 70\% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of handcrafted baseline attacks. Moreover, AGENTVIGIL exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyondbenchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs,including malicious sites.},
	urldate = {2025-12-12},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Zhun and Siu, Vincent and Ye, Zhe and Shi, Tianneng and Nie, Yuzhou and Zhao, Xuandong and Wang, Chenguang and Guo, Wenbo and Song, Dawn},
	editor = {Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet},
	month = nov,
	year = {2025},
	pages = {23159--23172},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/XLX34WI8/Wang 等 - 2025 - AGENTVIGIL Automatic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents.pdf:application/pdf},
}

@inproceedings{conneau_cross-lingual_2019,
	title = {Cross-lingual {Language} {Model} {Pretraining}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html},
	abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
	urldate = {2025-12-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {CONNEAU, Alexis and Lample, Guillaume},
	year = {2019},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/PTQAHH6D/CONNEAU和Lample - 2019 - Cross-lingual Language Model Pretraining.pdf:application/pdf},
}

@misc{beyer_are_2020,
	title = {Are we done with {ImageNet}?},
	url = {http://arxiv.org/abs/2006.07159},
	doi = {10.48550/arXiv.2006.07159},
	abstract = {Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Beyer, Lucas and Hénaff, Olivier J. and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, Aäron van den},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07159 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/26B6U2FZ/Beyer 等 - 2020 - Are we done with ImageNet.pdf:application/pdf;Snapshot:/Users/zhangyunshi/Zotero/storage/V49HTPIB/2006.html:text/html},
}

@inproceedings{han_evaluation_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {An {Evaluation} {Dataset} and {Strategy} for {Building} {Robust} {Multi}-turn {Response} {Selection} {Model}},
	url = {https://aclanthology.org/2021.emnlp-main.180/},
	doi = {10.18653/v1/2021.emnlp-main.180},
	abstract = {Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this study, we analyze the weaknesses of the open-domain Korean Multi-turn response selection models and publish an adversarial dataset to evaluate these weaknesses. We also suggest a strategy to build a robust model in this adversarial environment.},
	urldate = {2025-12-12},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Han, Kijong and Lee, Seojin and Lee, Dong-hun},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {2338--2344},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/AAGEQBM7/Han 等 - 2021 - An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model.pdf:application/pdf},
}

@inproceedings{tanwar_multilingual_2023,
	address = {Toronto, Canada},
	title = {Multilingual {LLMs} are {Better} {Cross}-lingual {In}-context {Learners} with {Alignment}},
	url = {https://aclanthology.org/2023.acl-long.346/},
	doi = {10.18653/v1/2023.acl-long.346},
	abstract = {In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy — Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.},
	urldate = {2025-12-12},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tanwar, Eshaan and Dutta, Subhabrata and Borthakur, Manish and Chakraborty, Tanmoy},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {6292--6307},
	file = {Full Text PDF:/Users/zhangyunshi/Zotero/storage/9UU4N8ZX/Tanwar 等 - 2023 - Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment.pdf:application/pdf},
}

@inproceedings{campbell_benchmark_2024,
	address = {Orlando, FL},
	title = {Benchmark {Problem} for {Autonomous} {Urban} {Air} {Mobility}},
	isbn = {978-1-62410-711-5},
	url = {https://arc.aiaa.org/doi/10.2514/6.2024-0718},
	doi = {10.2514/6.2024-0718},
	language = {en},
	urldate = {2025-12-15},
	booktitle = {{AIAA} {SCITECH} 2024 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Campbell, Newton H. and Gregory, Irene M. and Acheson, Michael J. and Ilangovan, Hari S. and Ranganathan, Shivakumar},
	month = jan,
	year = {2024},
	file = {PDF:/Users/zhangyunshi/Zotero/storage/WJLSQT4D/Campbell 等 - 2024 - Benchmark Problem for Autonomous Urban Air Mobility.pdf:application/pdf},
}
